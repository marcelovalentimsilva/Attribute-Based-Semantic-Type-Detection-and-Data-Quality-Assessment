{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "01 Read forty datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>name</th>\n",
       "      <th>area</th>\n",
       "      <th>url</th>\n",
       "      <th>instances</th>\n",
       "      <th>attributes</th>\n",
       "      <th>year</th>\n",
       "      <th>#webhits</th>\n",
       "      <th>Order</th>\n",
       "      <th>New_area</th>\n",
       "      <th>...</th>\n",
       "      <th>names_file_format</th>\n",
       "      <th>attribute_info</th>\n",
       "      <th>source</th>\n",
       "      <th>data_set_information</th>\n",
       "      <th>relevant_papers</th>\n",
       "      <th>papers_that_cite_this_data_set</th>\n",
       "      <th>num_papers</th>\n",
       "      <th>#numpapers</th>\n",
       "      <th>2#</th>\n",
       "      <th>FinalRank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>52</td>\n",
       "      <td>Iris</td>\n",
       "      <td>Life</td>\n",
       "      <td>https://archive.ics.uci.edu/ml/datasets/Iris</td>\n",
       "      <td>150.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>107</td>\n",
       "      <td>Wine</td>\n",
       "      <td>Physical</td>\n",
       "      <td>https://archive.ics.uci.edu/ml/datasets/Wine</td>\n",
       "      <td>178.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40</td>\n",
       "      <td>13.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92</td>\n",
       "      <td>Spambase</td>\n",
       "      <td>Computer</td>\n",
       "      <td>https://archive.ics.uci.edu/ml/datasets/Spambase</td>\n",
       "      <td>4601.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>75.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "      <td>Heart Disease</td>\n",
       "      <td>Life</td>\n",
       "      <td>https://archive.ics.uci.edu/ml/datasets/Heart+...</td>\n",
       "      <td>303.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Adult</td>\n",
       "      <td>Social</td>\n",
       "      <td>https://archive.ics.uci.edu/ml/datasets/Adult</td>\n",
       "      <td>48842.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index           name      area  \\\n",
       "0     52           Iris      Life   \n",
       "1    107           Wine  Physical   \n",
       "2     92       Spambase  Computer   \n",
       "3     45  Heart Disease      Life   \n",
       "4      2          Adult    Social   \n",
       "\n",
       "                                                 url  instances  attributes  \\\n",
       "0       https://archive.ics.uci.edu/ml/datasets/Iris      150.0         4.0   \n",
       "1       https://archive.ics.uci.edu/ml/datasets/Wine      178.0        13.0   \n",
       "2   https://archive.ics.uci.edu/ml/datasets/Spambase     4601.0        57.0   \n",
       "3  https://archive.ics.uci.edu/ml/datasets/Heart+...      303.0        75.0   \n",
       "4      https://archive.ics.uci.edu/ml/datasets/Adult    48842.0        14.0   \n",
       "\n",
       "     year  #webhits  Order New_area  ...  names_file_format attribute_info  \\\n",
       "0  1988.0         1    NaN      NaN  ...                NaN            NaN   \n",
       "1  1991.0         5    NaN      NaN  ...                NaN            NaN   \n",
       "2  1999.0        25    NaN      NaN  ...                NaN            NaN   \n",
       "3  1988.0         4    NaN      NaN  ...                NaN            NaN   \n",
       "4  1996.0         2    NaN      NaN  ...                NaN            NaN   \n",
       "\n",
       "   source data_set_information relevant_papers papers_that_cite_this_data_set  \\\n",
       "0     NaN                  NaN             NaN                            NaN   \n",
       "1     NaN                  NaN             NaN                            NaN   \n",
       "2     NaN                  NaN             NaN                            NaN   \n",
       "3     NaN                  NaN             NaN                            NaN   \n",
       "4     NaN                  NaN             NaN                            NaN   \n",
       "\n",
       "  num_papers #numpapers     2# FinalRank  \n",
       "0        100        1.0    2.0       1.0  \n",
       "1         40       13.0   18.0       4.0  \n",
       "2          4       75.0  100.0      24.0  \n",
       "3         58        3.0    7.0       2.0  \n",
       "4         51        8.0   10.0       3.0  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from typing import Union, Tuple, List, Dict\n",
    "import logging\n",
    "import re\n",
    "import numpy as np\n",
    "import string\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Load the correct file to get the FORTY dataset details\n",
    "forty_datasets_file_path = 'Fortydatasets.xlsx'\n",
    "\n",
    "# Load the sheet to obtain the details of all datasets\n",
    "forty_datasets_df = pd.read_excel(forty_datasets_file_path)\n",
    "\n",
    "# Display the first few rows to verify the loaded data\n",
    "forty_datasets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "02 Read Analysed Columns and define dataset index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "desired_dataset_index: 275 Last run on: 2024-04-07 21:40:09\n"
     ]
    }
   ],
   "source": [
    "# Load the 'AnalysedColumns' sheet to identify the columns \n",
    "analysed_columns_file_path = 'AnalysedColumns.xlsx' # for 50 datasets\n",
    "#analysed_columns_file_path = 'AnalysedColumns1411.xlsx' # for 10 datasets\n",
    "#analysed_columns_file_path = 'AnalysedColumns2111.xlsx' # for 92 fake data\n",
    "\n",
    "analysed_columns_df = pd.read_excel(analysed_columns_file_path)\n",
    "\n",
    "# Define the dataset index for which you want the dataset file URL and name\n",
    "desired_dataset_index = 275 # Replace with the actual index you are interested in \n",
    "\n",
    "#changed_dataset_local_path = 'iris_CHANGED.txt' # 52\n",
    "#changed_dataset_local_path = 'glass_CHANGED.txt' # 42\n",
    "#changed_dataset_local_path = 'letter-recognition_CHANGED.txt' #  58\n",
    "#changed_dataset_local_path = 'Rice_Cammeo_Osmancik.txt' #545 - Obtained from @DATA part \n",
    "#changed_dataset_local_path = 'chronic_kidney_disease_full.txt' #336 - Obtained from @DATA part\n",
    "\n",
    "from datetime import datetime\n",
    "print(f\"desired_dataset_index: {desired_dataset_index} Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "03 Get dataset file URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset file URL for dataset Bike Sharing Dataset (index 275) is: https://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip\n",
      "Last run on: 2024-04-07 21:40:09\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from typing import Union, Tuple\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def get_dataset_file_url(excel_file_path: str, dataset_index: int) -> Union[Tuple[str, str], Exception]:\n",
    "    \"\"\"\n",
    "    Load dataset details from an Excel file and return the dataset file URL and name for a specific dataset index.\n",
    "\n",
    "    Parameters:\n",
    "    - excel_file_path (str): The path to the Excel file containing dataset details.\n",
    "    - dataset_index (int): The index number of the dataset for which to get the URL.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple or Exception: A tuple containing the dataset file URL and name, or an exception if something goes wrong.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the Excel sheet into a DataFrame\n",
    "        datasets_df = pd.read_excel(excel_file_path)\n",
    "        \n",
    "        # Check if 'index', 'dataset_file_url', and 'name' columns exist in the DataFrame\n",
    "        required_columns = ['index', 'dataset_file_url', 'name']\n",
    "        for col in required_columns:\n",
    "            if col not in datasets_df.columns:\n",
    "                return f\"Required column '{col}' does not exist in the DataFrame\"\n",
    "        \n",
    "        # Extract the dataset file URL and name for the given dataset index\n",
    "        dataset_details = datasets_df.loc[datasets_df['index'] == dataset_index, ['dataset_file_url', 'name']]\n",
    "        if dataset_details.empty:\n",
    "            return f\"No dataset found with index {dataset_index}\"\n",
    "\n",
    "        dataset_file_url = dataset_details['dataset_file_url'].values[0]\n",
    "        dataset_name = dataset_details['name'].values[0]\n",
    "\n",
    "        #logging.info(f\"Dataset name: {dataset_name}, File URL: {dataset_file_url}\")\n",
    "        return (dataset_file_url, dataset_name)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred: {e}\")\n",
    "        return e\n",
    "\n",
    "# Define the Excel file path where the dataset details are stored\n",
    "excel_file_path = forty_datasets_file_path\n",
    "\n",
    "# Call the function to get the dataset details\n",
    "dataset_details = get_dataset_file_url(excel_file_path, desired_dataset_index)\n",
    "\n",
    "# Check the type of the return value to see if it's a tuple (indicating success)\n",
    "if isinstance(dataset_details, tuple):\n",
    "    dataset_file_url, dataset_name = dataset_details\n",
    "    print(f\"The dataset file URL for dataset {dataset_name} (index {desired_dataset_index}) is: {dataset_file_url}\")\n",
    "else:\n",
    "    print(f\"An error occurred or the dataset was not found: {dataset_details}\")\n",
    "\n",
    "from datetime import datetime\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "04 Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset file url: https://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: day.csv\n",
      "2: hour.csv\n",
      "3: Readme.txt\n",
      "Selected File: hour.csv\n",
      "First row for inspection: instant,dteday,season,yr,mnth,hr,holiday,weekday,workingday,weathersit,temp,atemp,hum,windspeed,casual,registered,cnt\n",
      "\n",
      "Is first row header ? True\n",
      "Dataset loaded successfully.\n",
      "   instant      dteday  season  yr  mnth  hr  holiday  weekday  workingday  \\\n",
      "0        1  2011-01-01       1   0     1   0        0        6           0   \n",
      "1        2  2011-01-01       1   0     1   1        0        6           0   \n",
      "2        3  2011-01-01       1   0     1   2        0        6           0   \n",
      "3        4  2011-01-01       1   0     1   3        0        6           0   \n",
      "4        5  2011-01-01       1   0     1   4        0        6           0   \n",
      "\n",
      "   weathersit  temp   atemp   hum  windspeed  casual  registered  cnt  \n",
      "0           1  0.24  0.2879  0.81        0.0       3          13   16  \n",
      "1           1  0.22  0.2727  0.80        0.0       8          32   40  \n",
      "2           1  0.22  0.2727  0.80        0.0       5          27   32  \n",
      "3           1  0.24  0.2879  0.75        0.0       3          10   13  \n",
      "4           1  0.24  0.2879  0.75        0.0       0           1    1  \n",
      "Last run on: 2024-04-07 21:40:26\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import io\n",
    "from io import BytesIO\n",
    "import zipfile\n",
    "import tarfile\n",
    "import gzip\n",
    "import logging\n",
    "import csv\n",
    "from typing import Optional, Union\n",
    "import os\n",
    "import tempfile\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def is_header_for_csv(line, delimiter=' '):\n",
    "    \"\"\"\n",
    "    Determine if a line is likely a header by checking if there are no numeric values.\n",
    "    If at least one numeric value is found, the line is considered not a header (i.e., a data line).\n",
    "    \"\"\"\n",
    "    # Regex to match quoted strings or non-whitespace sequences\n",
    "    pattern = re.compile(r'\\\".*?\\\"|\\S+')\n",
    "\n",
    "    elements = pattern.findall(line.replace(delimiter, ' '))  # Replace delimiter with space for easier parsing\n",
    "\n",
    "    # Check if any element is a number\n",
    "    is_numeric_present = any(element.replace('.', '', 1).lstrip('-').isdigit() for element in elements)\n",
    "\n",
    "    # Check for replicated elements\n",
    "    unique_elements = set(elements)\n",
    "    replicated_elements = len(elements) - len(unique_elements)\n",
    "\n",
    "    # If no numeric value is found and there are replicated elements, consider this a header line\n",
    "    is_header = not is_numeric_present and replicated_elements == 0\n",
    "\n",
    "    return is_header\n",
    "\n",
    "\n",
    "def load_csv(file_content: Union[str, bytes], header: Optional[int], na_values: Optional[Union[str, list]]) -> pd.DataFrame:\n",
    "    # Adjust to decode bytes if necessary\n",
    "    if isinstance(file_content, bytes):\n",
    "        try:\n",
    "            file_content_decoded = file_content.decode('utf-8')\n",
    "        except UnicodeDecodeError:\n",
    "            file_content_decoded = file_content.decode('ISO-8859-1')\n",
    "    else:\n",
    "        file_content_decoded = file_content\n",
    "    \n",
    "    # Preprocess to replace multiple tabs with a single tab\n",
    "    #file_content_processed = re.sub('\\t+', '\\t', file_content_decoded)\n",
    "    file_content_processed = re.sub('[ \\t]+', ' ', file_content_decoded)\n",
    "\n",
    "    file_stream = io.StringIO(file_content_processed)\n",
    "        \n",
    "    try:\n",
    "        dialect = csv.Sniffer().sniff(file_stream.readline())\n",
    "        delimiter = dialect.delimiter\n",
    "        file_stream.seek(0)\n",
    "    except csv.Error:\n",
    "        delimiter = ','\n",
    "        logging.info(\"Falling back to default delimiter ',' due to detection failure.\")\n",
    "  \n",
    "    # Use the heuristic to decide if the first line is likely a header\n",
    "    first_line = file_stream.readline()\n",
    "    # Print the first row for inspection\n",
    "    print(\"First row for inspection:\", first_line)\n",
    "    is_header_row = is_header_for_csv(first_line, delimiter)\n",
    "    print('Is first row header ?',is_header_row )\n",
    "    file_stream.seek(0)  # Reset to start of file after reading the first line\n",
    "    \n",
    "    header_decision = 0 if is_header_for_csv(first_line, delimiter) else None\n",
    "    #header_decision = None if is_data_row else 0\n",
    "    # Regex pattern to match quoted strings or non-whitespace sequences\n",
    "    pattern = re.compile(r'\\\".*?\\\"|\\S+')\n",
    "\n",
    "    df = pd.read_csv(file_stream, header=header_decision, delimiter=delimiter, na_values=na_values, keep_default_na=False)\n",
    "   \n",
    "  # Check if the DataFrame needs re-parsing with regex pattern\n",
    "    if len(df.columns) < 2:\n",
    "        file_stream.seek(0)\n",
    "        lines = file_stream.readlines()\n",
    "        parsed_data = [pattern.findall(line) for line in lines]\n",
    "\n",
    "        if header is None and header_decision == 0:\n",
    "            header_row = parsed_data.pop(0)  # Use the first row as header\n",
    "        else:\n",
    "            header_row = None  # Let pandas create default headers or use provided header index\n",
    "\n",
    "        df = pd.DataFrame(parsed_data, columns=header_row)\n",
    "        df = df.apply(pd.to_numeric, errors='ignore')  # Attempt to correct data types\n",
    "    return df\n",
    "\n",
    "def is_header_for_excel(first_row: pd.DataFrame) -> bool:\n",
    "    \"\"\"\n",
    "    Determines if the first row of a DataFrame is likely to be a header by checking the entire row's content.\n",
    "\n",
    "    Args:\n",
    "        first_row (pd.DataFrame): DataFrame containing at least one row of data to inspect.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the first row is likely a header (i.e., contains mostly non-numeric data), False if it's likely data.\n",
    "    \"\"\"\n",
    "    # Check if the majority of the items in the first row are non-numeric\n",
    "    non_numeric_count = first_row.applymap(lambda x: not isinstance(x, (int, float))).iloc[0].sum()\n",
    "    \n",
    "    # Determine if the first row is likely a header based on the proportion of non-numeric items\n",
    "    is_header = non_numeric_count > len(first_row.columns) / 2\n",
    "    return is_header\n",
    "\n",
    "def load_excel(file_content: Union[str, bytes, io.BytesIO], na_values: Optional[Union[str, list]] = None, skip_rows: Union[int, list] = 0, parse_dates: bool = False) -> pd.DataFrame:\n",
    "    # Ajuste para o file_content ser um objeto BytesIO, se for bytes\n",
    "    if isinstance(file_content, bytes):\n",
    "        file_content = BytesIO(file_content)  # Envolve bytes em BytesIO\n",
    "    try:\n",
    "        # Load the first row to check if it's likely to be a header\n",
    "        first_row = pd.read_excel(file_content, nrows=1, header=None)\n",
    "        \n",
    "        # Print the first row for inspection\n",
    "        print(\"First row for inspection:\", first_row.iloc[0].values)\n",
    "\n",
    "        # Execute the heuristic\n",
    "        likely_header = is_header_for_excel(first_row)\n",
    "\n",
    "        # Decide on using the first row as header based on heuristic\n",
    "        #header_decision = None if likely_header else 0  \n",
    "        header_decision = 0 if likely_header else None\n",
    "\n",
    "        # Reset file_content to read from the beginning if it's a stream\n",
    "        if isinstance(file_content, io.BytesIO):\n",
    "            print(\"Resetting stream position\")  # Diagnostic print\n",
    "            file_content.seek(0)\n",
    "\n",
    "        # Load the full Excel file with determined header option\n",
    "        df = pd.read_excel(file_content, na_values=na_values, skiprows=skip_rows, header=header_decision, parse_dates=parse_dates)\n",
    "        return df\n",
    "    except ValueError as e:\n",
    "        logging.error(f\"Excel loading error: {e}\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "def download_and_extract(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Downloads an archive from the given URL and extracts it into a temporary directory.\n",
    "    Handles .zip, .tar.gz, and .gz files. Returns the path to the directory or file.\n",
    "    \"\"\"\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    response = requests.get(url, stream=True)\n",
    "    file_name = os.path.basename(url)\n",
    "    temp_file_path = os.path.join(temp_dir, file_name)\n",
    "    \n",
    "    with open(temp_file_path, 'wb') as file:\n",
    "        shutil.copyfileobj(response.raw, file)\n",
    "    \n",
    "    if file_name.endswith('.zip'):\n",
    "        with zipfile.ZipFile(temp_file_path, 'r') as archive:\n",
    "            archive.extractall(temp_dir)\n",
    "    elif file_name.endswith(('.tar.gz', '.tgz')):\n",
    "        with tarfile.open(temp_file_path, 'r:gz') as archive:\n",
    "            archive.extractall(temp_dir)\n",
    "    elif file_name.endswith('.gz'):\n",
    "        # Handle single .gz files by extracting to the same directory\n",
    "        extracted_file_path = temp_file_path[:-3]  # Remove .gz extension\n",
    "        with gzip.open(temp_file_path, 'rb') as f_in, open(extracted_file_path, 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "        os.remove(temp_file_path)  # Remove the original .gz file\n",
    "        return extracted_file_path  # Direct path to the extracted file for .gz\n",
    "    \n",
    "    os.remove(temp_file_path)  # Clean up archive file after extraction\n",
    "    return temp_dir  # Path to directory with extracted files\n",
    "\n",
    "def select_file_from_extracted(directory: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Recursively lists all files in the extracted directory and prompts the user to select one.\n",
    "    \"\"\"\n",
    "    files_list = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            files_list.append(os.path.join(root, file))\n",
    "\n",
    "    if not files_list:\n",
    "        logging.error(\"No files found in the directory.\")\n",
    "        return None\n",
    "\n",
    "    for index, file in enumerate(files_list, start=1):\n",
    "        relative_path = os.path.relpath(file, directory)  # Show relative path for clarity\n",
    "        print(f\"{index}: {relative_path}\", flush=True)\n",
    "\n",
    "    try:\n",
    "        file_index = int(input(\"Enter the number of the file you want to load: \")) - 1\n",
    "        if 0 <= file_index < len(files_list):\n",
    "            selected_relative_path = os.path.relpath(files_list[file_index], directory)  # Get relative path\n",
    "            print(f\"Selected File: {selected_relative_path}\")  # Print relative path\n",
    "            return files_list[file_index]  # Return full path for further processing\n",
    "        else:\n",
    "            logging.error(\"Invalid selection.\")\n",
    "    except ValueError:\n",
    "        logging.error(\"Please enter a valid number.\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "def fetch_file_content(url: str) -> str:\n",
    "    response = requests.get(url, stream=True)\n",
    "    try:\n",
    "        content = response.content.decode('utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        content = response.content.decode('ISO-8859-1')\n",
    "    return content\n",
    "\n",
    "def load_dataset(file_path_or_url: str, na_values: Optional[Union[str, list]] = None, skip_rows: Union[int, list] = 0, parse_dates: bool = False) -> Optional[pd.DataFrame]:\n",
    "    if file_path_or_url.startswith(('http://', 'https://')):\n",
    "        if file_path_or_url.endswith(('.csv', '.txt', '.data')):\n",
    "            response = requests.get(file_path_or_url)\n",
    "            file_content = response.content\n",
    "            return load_csv(file_content, header=None, na_values=na_values)\n",
    "        elif file_path_or_url.endswith(('.xlsx', '.xls')):\n",
    "            response = requests.get(file_path_or_url)\n",
    "            return load_excel(response.content, na_values=na_values, skip_rows=skip_rows, parse_dates=parse_dates)\n",
    "    else:\n",
    "        # Handle local file path\n",
    "        if os.path.exists(file_path_or_url):\n",
    "            if file_path_or_url.endswith(('.csv', '.txt', '.data')):\n",
    "                with open(file_path_or_url, 'r', encoding='utf-8') as f:\n",
    "                    file_content = f.read()\n",
    "                return load_csv(file_content, header=None, na_values=na_values)\n",
    "            elif file_path_or_url.endswith(('.xlsx', '.xls')):\n",
    "                with open(file_path_or_url, 'rb') as f:\n",
    "                    file_content = f.read()\n",
    "                return load_excel(file_content, na_values=na_values, skip_rows=skip_rows, parse_dates=parse_dates)\n",
    "        else:\n",
    "            logging.error(f\"File does not exist: {file_path_or_url}\")\n",
    "            return None\n",
    "    # Handling archives\n",
    "    extracted_path = download_and_extract(file_path_or_url)\n",
    "    if os.path.isdir(extracted_path):\n",
    "        selected_file = select_file_from_extracted(extracted_path)\n",
    "        if selected_file and selected_file.endswith(('.csv', '.txt', '.data')):\n",
    "            with open(selected_file, 'rb') as f:\n",
    "                file_content = f.read()\n",
    "            return load_csv(file_content, header=None, na_values=na_values)\n",
    "        elif selected_file and selected_file.endswith(('.xlsx', '.xls')):\n",
    "            with open(selected_file, 'rb') as f:\n",
    "                file_content = f.read()\n",
    "            return load_excel(file_content, na_values=na_values, skip_rows=skip_rows, parse_dates=parse_dates)\n",
    "    elif os.path.isfile(extracted_path):\n",
    "        if extracted_path.endswith(('.csv', '.txt', '.data')):\n",
    "            with open(extracted_path, 'rb') as f:\n",
    "                file_content = f.read()\n",
    "            return load_csv(file_content, header=None, na_values=na_values)\n",
    "        elif extracted_path.endswith(('.xlsx', '.xls')):\n",
    "            with open(extracted_path, 'rb') as f:\n",
    "                file_content = f.read()\n",
    "            return load_excel(file_content, na_values=na_values, skip_rows=skip_rows, parse_dates=parse_dates)\n",
    "        else:\n",
    "            # Attempt to load the file without assuming an extension, particularly useful for .gz extracted files\n",
    "            with open(extracted_path, 'rb') as f:\n",
    "                file_content = f.read()\n",
    "            try:\n",
    "                # Attempt to load as CSV first; this part assumes CSV if no extension is found\n",
    "                return load_csv(file_content, header=None, na_values=na_values)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to automatically determine file type for {extracted_path}: {e}\")\n",
    "                return None\n",
    "    return None\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Dataset file url: {dataset_file_url}\")\n",
    "    dataset_df = load_dataset(dataset_file_url)\n",
    "\n",
    "    if dataset_df is not None:\n",
    "        print(\"Dataset loaded successfully.\")\n",
    "        print(dataset_df.head())\n",
    "    else:\n",
    "        print(\"Failed to load dataset.\")\n",
    "\n",
    "from datetime import datetime\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "05 Assign Column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Successfully assigned column names to the dataset 'Bike Sharing Dataset' for index 275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last run on: 2024-04-07 21:40:26\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instant</th>\n",
       "      <th>dteday</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>hr</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17374</th>\n",
       "      <td>17375</td>\n",
       "      <td>2012-12-31</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.2576</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.1642</td>\n",
       "      <td>11</td>\n",
       "      <td>108</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17375</th>\n",
       "      <td>17376</td>\n",
       "      <td>2012-12-31</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.2576</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.1642</td>\n",
       "      <td>8</td>\n",
       "      <td>81</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17376</th>\n",
       "      <td>17377</td>\n",
       "      <td>2012-12-31</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.2576</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.1642</td>\n",
       "      <td>7</td>\n",
       "      <td>83</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17377</th>\n",
       "      <td>17378</td>\n",
       "      <td>2012-12-31</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.1343</td>\n",
       "      <td>13</td>\n",
       "      <td>48</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17378</th>\n",
       "      <td>17379</td>\n",
       "      <td>2012-12-31</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.1343</td>\n",
       "      <td>12</td>\n",
       "      <td>37</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17379 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       instant      dteday  season  yr  mnth  hr  holiday  weekday  \\\n",
       "0            1  2011-01-01       1   0     1   0        0        6   \n",
       "1            2  2011-01-01       1   0     1   1        0        6   \n",
       "2            3  2011-01-01       1   0     1   2        0        6   \n",
       "3            4  2011-01-01       1   0     1   3        0        6   \n",
       "4            5  2011-01-01       1   0     1   4        0        6   \n",
       "...        ...         ...     ...  ..   ...  ..      ...      ...   \n",
       "17374    17375  2012-12-31       1   1    12  19        0        1   \n",
       "17375    17376  2012-12-31       1   1    12  20        0        1   \n",
       "17376    17377  2012-12-31       1   1    12  21        0        1   \n",
       "17377    17378  2012-12-31       1   1    12  22        0        1   \n",
       "17378    17379  2012-12-31       1   1    12  23        0        1   \n",
       "\n",
       "       workingday  weathersit  temp   atemp   hum  windspeed  casual  \\\n",
       "0               0           1  0.24  0.2879  0.81     0.0000       3   \n",
       "1               0           1  0.22  0.2727  0.80     0.0000       8   \n",
       "2               0           1  0.22  0.2727  0.80     0.0000       5   \n",
       "3               0           1  0.24  0.2879  0.75     0.0000       3   \n",
       "4               0           1  0.24  0.2879  0.75     0.0000       0   \n",
       "...           ...         ...   ...     ...   ...        ...     ...   \n",
       "17374           1           2  0.26  0.2576  0.60     0.1642      11   \n",
       "17375           1           2  0.26  0.2576  0.60     0.1642       8   \n",
       "17376           1           1  0.26  0.2576  0.60     0.1642       7   \n",
       "17377           1           1  0.26  0.2727  0.56     0.1343      13   \n",
       "17378           1           1  0.26  0.2727  0.65     0.1343      12   \n",
       "\n",
       "       registered  cnt  \n",
       "0              13   16  \n",
       "1              32   40  \n",
       "2              27   32  \n",
       "3              10   13  \n",
       "4               1    1  \n",
       "...           ...  ...  \n",
       "17374         108  119  \n",
       "17375          81   89  \n",
       "17376          83   90  \n",
       "17377          48   61  \n",
       "17378          37   49  \n",
       "\n",
       "[17379 rows x 17 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from typing import Union\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def assign_column_names(analysed_columns_df: pd.DataFrame, dataset_index: int, dataset_df: pd.DataFrame, dataset_name: str = \"Unknown\") -> Union[pd.DataFrame, Exception]:\n",
    "    \"\"\"\n",
    "    Assign column names to a DataFrame based on a given dataset index from an \"AnalysedColumns\" DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - analysed_columns_df (pd.DataFrame): The DataFrame containing analysed columns information.\n",
    "    - dataset_index (int): The index number of the dataset for which to get the column names.\n",
    "    - target_df (pd.DataFrame): The DataFrame to which the column names will be assigned.\n",
    "    - dataset_name (str): The name of the dataset. Default is \"Unknown\".\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame or Exception: The DataFrame with assigned column names or an exception if something goes wrong.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if 'index' and 'Column' columns exist in the DataFrame\n",
    "        if 'index' not in analysed_columns_df.columns or 'Column' not in analysed_columns_df.columns:\n",
    "            return f\"Required columns 'index' or 'Column' do not exist in the DataFrame\"\n",
    "\n",
    "        # Extract the column names for the given dataset index\n",
    "        column_names = analysed_columns_df.loc[analysed_columns_df['index'] == dataset_index, 'Column'].tolist()\n",
    "        if not column_names:\n",
    "            return f\"No column names found for dataset index {dataset_index}\"\n",
    "\n",
    "        # Assign the extracted column names to the target DataFrame\n",
    "        dataset_df.columns = column_names\n",
    "\n",
    "        logging.info(f\"Successfully assigned column names to the dataset '{dataset_name}' for index {dataset_index}\")\n",
    "        return dataset_df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred while assigning column names to the dataset '{dataset_name}': {e}\")\n",
    "        return e\n",
    "\n",
    "# Assume analysed_columns_df contains the analysed columns information\n",
    "# Assume target_df is the DataFrame loaded previously\n",
    "# Assume dataset_name contains the name of the dataset\n",
    "\n",
    "# Call the function to assign the column names\n",
    "dataset_df = assign_column_names(analysed_columns_df, desired_dataset_index, dataset_df, dataset_name)\n",
    "\n",
    "# Display the first few rows to verify the column names have been correctly assigned\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "dataset_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of functions in the DataQualityIssues class along with their associated DQI (Data Quality Issue) numbers:\n",
    "\n",
    "is_blank - Not directly associated with a DQI number\n",
    "\n",
    "handle_blank_empty_null_nan - DQI #1 (Missing Data - Completeness)\n",
    "\n",
    "handle_predefined_unacceptable_values - DQI #4 (Ambiguous Data - Accuracy, Consistency)\n",
    "\n",
    "handle_extraneous_data - DQI #5 (Extraneous Data - Consistency, Uniqueness)\n",
    "\n",
    "handle_street_extraneous_data - DQI #5 (Extraneous Data - Consistency, Uniqueness)\n",
    "\n",
    "standardize_date - Not directly associated with a DQI number\n",
    "\n",
    "standardize_date_time - Not directly associated with a DQI number\n",
    "\n",
    "is_valid_time - Not directly associated with a DQI number\n",
    "\n",
    "handle_outdated_temporal_data - DQI #6 (Outdated Temporal Data - Timeliness)\n",
    "\n",
    "handle_outdated_temporal_data_datetime - DQI #6 (Outdated Temporal Data - Timeliness)\n",
    "\n",
    "handle_duplicates - DQI #9 (Duplicates - Uniqueness)\n",
    "\n",
    "handle_excessive_distinct_values - DQI #10 (Structural Conflicts - Consistency, Uniqueness)\n",
    "\n",
    "handle_dates_format - DQI #14 (Different units/representations - Consistency)\n",
    "\n",
    "handle_datetimes_format - DQI #14 (Different units/representations - Consistency)\n",
    "\n",
    "handle_negative_values - DQI #15 (Domain Violation - Accuracy)\n",
    "\n",
    "handle_values_outside_range - DQI #15 (Domain Violation - Accuracy)\n",
    "\n",
    "handle_floating_point_values - DQI #15 (Domain Violation - Accuracy)\n",
    "\n",
    "handle_capitalization_format - DQI #15 (Domain Violation - Accuracy)\n",
    "\n",
    "handle_short_length_values - DQI #15 (Domain Violation - Accuracy)\n",
    "\n",
    "handle_invalid_months - DQI #15 (Domain Violation - Accuracy)\n",
    "\n",
    "handle_invalid_weekdays - DQI #15 (Domain Violation - Accuracy)\n",
    "\n",
    "handle_street_format - DQI #15 (Domain Violation - Accuracy)\n",
    "\n",
    "is_date_valid - Not directly associated with a DQI number\n",
    "\n",
    "handle_invalid_dates - DQI #15 (Domain Violation - Accuracy)\n",
    "\n",
    "is_structurally_valid_date - Not directly associated with a DQI number\n",
    "\n",
    "handle_invalid_datetimes - DQI #15 (Domain Violation - Accuracy)\n",
    "\n",
    "standardize_phone_number - Not directly associated with a DQI number\n",
    "\n",
    "handle_phone_number_format - DQI #15 (Domain Violation - Accuracy)\n",
    "\n",
    "handle_ip_format - DQI #15 (Domain Violation - Accuracy)\n",
    "\n",
    "handle_url_format - DQI #15 (Domain Violation - Accuracy)\n",
    "\n",
    "is_valid_email - Not directly associated with a DQI number\n",
    "\n",
    "handle_email_format - DQI #15 (Domain Violation - Accuracy)\n",
    "\n",
    "handle_binary_values - DQI #15 (Domain Violation - Accuracy)\n",
    "\n",
    "handle_non_numeric_values - DQI #17 (Wrong Data Type - Consistency)\n",
    "\n",
    "handle_non_alphanumeric_values - DQI #17 (Wrong Data Type - Consistency)\n",
    "\n",
    "handle_non_string_values - DQI #17 (Non-String Data Type - Consistency)\n",
    "\n",
    "handle_alphanumeric_consistency - DQI #17 (Wrong Data Type - Consistency)\n",
    "\n",
    "handle_uniqueness_violation - DQI #19 (Uniqueness Violation - Uniqueness)\n",
    "\n",
    "handle_special_characters - DQI #21 (Use of Special Characters - Consistency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "06 Data Quality Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last run on: 2024-04-07 21:40:26\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from datetime import datetime, time  \n",
    "\n",
    "min_valid_year = 1800\n",
    "max_valid_year = 2100\n",
    "\n",
    "class DataQualityIssues:\n",
    "\n",
    "    @staticmethod    \n",
    "    def is_blank(x):\n",
    "        # Function to determine if a value is blank\n",
    "        x_str = str(x).strip()\n",
    "        return pd.isnull(x) or x_str == '' or x_str.lower() == 'null' or x_str in ['\"\"', \"''\", '\" \"', \"' '\"]\n",
    "\n",
    "    @staticmethod\n",
    "    def handle_blank_empty_null_nan(df, column):          \n",
    "        # Get indices for blank/empty/null/NaN values\n",
    "        blank_indices = df[df[column].apply(DataQualityIssues.is_blank)].index\n",
    "\n",
    "        # Convert 'nan' to a string representation for display\n",
    "        issue_data = []\n",
    "        for index in blank_indices:\n",
    "            val = df.at[index, column]\n",
    "            if isinstance(val, float) and np.isnan(val):\n",
    "                # Convert NaN floats to a string for display\n",
    "                issue_data.append((index, ''))  # Represent empty values as empty strings\n",
    "            else:\n",
    "                # Use the original value\n",
    "                issue_data.append((index, val))\n",
    "\n",
    "        total_issues = len(issue_data)\n",
    "\n",
    "        if issue_data:\n",
    "            if total_issues > 20:\n",
    "                # Keep only the first 10 and last 10 items\n",
    "                first_10 = issue_data[:10]\n",
    "                last_10 = issue_data[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                message = f\"{total_issues} Blank/Empty/Null/NaN value(s) at index(es): {display_list} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                message = f\"{total_issues} Blank/Empty/Null/NaN value(s) at index(es): {issue_data}\"\n",
    "\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\": \"DQI #1 (Missing Data - Completeness)\"\n",
    "            }\n",
    "\n",
    "        return {\"issue\": False}\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def handle_predefined_unacceptable_values(df, column):\n",
    "        predefined_unacceptable_values = ['?']\n",
    "        unacceptable_indices_and_values = []\n",
    "\n",
    "        for idx, val in df[column].items():\n",
    "            val_str = str(val).strip()\n",
    "            if val_str in predefined_unacceptable_values:\n",
    "                unacceptable_indices_and_values.append((idx, val))\n",
    "\n",
    "        total_issues = len(unacceptable_indices_and_values)\n",
    "        if unacceptable_indices_and_values:\n",
    "            if total_issues > 20:\n",
    "                first_10 = unacceptable_indices_and_values[:10]\n",
    "                last_10 = unacceptable_indices_and_values[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                message = f\"{total_issues} Unacceptable value(s) at index(es): {display_list} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                message = f\"{total_issues} Unacceptable value(s) at index(es): {unacceptable_indices_and_values}\"\n",
    "\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\": \"DQI #4 (Ambiguous Data - Accuracy, Consistency)\"\n",
    "            }\n",
    "        return {\"issue\": False}\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def handle_extraneous_data(df, column):\n",
    "        # Define the function to check for extraneous data\n",
    "        def has_extraneous_data(x):\n",
    "            return any(char.isdigit() or char in ['!', '?'] for char in str(x))\n",
    "\n",
    "        # Get indices and values for extraneous data\n",
    "        extraneous_data_indices = df[df[column].apply(has_extraneous_data)].index\n",
    "        extraneous_data_values = df.loc[extraneous_data_indices, column].tolist()\n",
    "        issue_data = list(zip(extraneous_data_indices, extraneous_data_values))\n",
    "        total_issues = len(issue_data)\n",
    "\n",
    "        if issue_data:\n",
    "            if total_issues > 20:\n",
    "                # Keep only the first 10 and last 10 items\n",
    "                first_10 = issue_data[:10]\n",
    "                last_10 = issue_data[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                message = f\"{total_issues} Extraneous data value(s) at index(es): {display_list} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                message = f\"{total_issues} Extraneous data value(s) at index(es): {issue_data}\"\n",
    "\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"indices\": extraneous_data_indices,  # Include all indices directly\n",
    "                \"dq_issue\": \"DQI #5 (Extraneous Data - Consistency, Uniqueness)\"\n",
    "            }\n",
    "\n",
    "        return {\"issue\": False}\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def handle_street_extraneous_data(df, column):\n",
    "        \"\"\"\n",
    "        Check for extraneous data in street names, allowing common patterns including numbers,\n",
    "        hyphens, periods, slashes, and commas which are typical in street addresses, but flagging street names\n",
    "        composed solely of numbers as errors.\n",
    "        \"\"\"\n",
    "        def has_extraneous_street_data(x):\n",
    "            # Allow numbers, letters, spaces, hyphens, periods, slashes, and commas\n",
    "            allowed_chars = string.ascii_letters + string.digits + ' -./,'\n",
    "            # Flag if the string is solely numeric\n",
    "            if x.isdigit():\n",
    "                return True\n",
    "            return any(char not in allowed_chars for char in x)\n",
    "\n",
    "        extraneous_data_indices = df[df[column].apply(lambda x: isinstance(x, str) and has_extraneous_street_data(x))].index\n",
    "        extraneous_data_values = df.loc[extraneous_data_indices, column].tolist()\n",
    "        issue_data = list(zip(extraneous_data_indices, extraneous_data_values))\n",
    "        total_issues = len(issue_data)\n",
    "\n",
    "        if issue_data:\n",
    "            if total_issues > 20:\n",
    "                # Keep only the first 10 and last 10 items\n",
    "                first_10 = issue_data[:10]\n",
    "                last_10 = issue_data[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                message = f\"{total_issues} Extraneous street data value(s) at index(es): {display_list} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                message = f\"{total_issues} Extraneous street data value(s) at index(es): {issue_data}\"\n",
    "\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"indices\": extraneous_data_indices,  # Include all indices directly\n",
    "                \"dq_issue\": \"DQI #5 (Extraneous Data - Consistency, Uniqueness)\"\n",
    "            }\n",
    "\n",
    "        return {\"issue\": False}\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_to_strftime_format(deduced_format):\n",
    "        format_mappings = {\n",
    "            \"DDMMYYYY\": \"%d/%m/%Y\",\n",
    "            \"MMDDYYYY\": \"%m/%d/%Y\",\n",
    "            \"YYYYMMDD\": \"%Y/%m/%d\"\n",
    "        }\n",
    "        return format_mappings.get(deduced_format, \"%Y-%m-%d\")  # Default format\n",
    "\n",
    "        \n",
    "    @staticmethod\n",
    "    def standardize_date(date_str, deduced_format):\n",
    "        month_mapping = {\n",
    "            'January': '01', 'February': '02', 'March': '03', 'April': '04', 'May': '05', 'June': '06',\n",
    "            'July': '07', 'August': '08', 'September': '09', 'October': '10', 'November': '11', 'December': '12',\n",
    "            'Jan': '01', 'Feb': '02', 'Mar': '03', 'Apr': '04', 'May': '05', 'Jun': '06',\n",
    "            'Jul': '07', 'Aug': '08', 'Sep': '09', 'Oct': '10', 'Nov': '11', 'Dec': '12'\n",
    "        }\n",
    "        # Remove ordinal suffixes and commas\n",
    "        date_str = re.sub(r'(\\d+)(st|nd|rd|th)', r'\\1', date_str.replace(',', ''))\n",
    "\n",
    "        for month, num in month_mapping.items():\n",
    "            date_str = re.sub(r'\\b' + month + r'\\b', num, date_str, flags=re.IGNORECASE)\n",
    "\n",
    "        # Split the date string into components\n",
    "        date_parts = re.split(r'[-/. ]', date_str)\n",
    "\n",
    "        if len(date_parts) == 3:\n",
    "            if deduced_format == 'YYYYMMDD':\n",
    "                standardized_date = date_parts[0][:4] + date_parts[1].zfill(2) + date_parts[2].zfill(2)\n",
    "            elif deduced_format == 'DDMMYYYY':\n",
    "                standardized_date = date_parts[0].zfill(2) + date_parts[1].zfill(2) + date_parts[2][:4]\n",
    "            elif deduced_format == 'MMDDYYYY':\n",
    "                standardized_date = date_parts[0].zfill(2) + date_parts[1].zfill(2) + date_parts[2][:4]\n",
    "            else:\n",
    "                return None  # Format mismatch\n",
    "        else:\n",
    "            return None  # Format not recognized    \n",
    "        \n",
    "        return standardized_date\n",
    "\n",
    "    @staticmethod\n",
    "    def standardize_date_time(date_str, deduced_format):\n",
    "        \n",
    "       # Convert to string in case the input is not a string (e.g., float, int)\n",
    "        date_str = str(date_str)\n",
    "        \n",
    "        # Handle dates with ordinal suffixes and commas\n",
    "        date_str = re.sub(r'(\\d+)(st|nd|rd|th)', r'\\1', date_str.replace(',', ''))\n",
    "\n",
    "        parts = date_str.split(' ')\n",
    "        \n",
    "        if len(parts) > 1 and parts[-1] in [\"AM\", \"PM\"]:\n",
    "            time_part = ' '.join(parts[-2:])\n",
    "            date_part = ' '.join(parts[:-2])\n",
    "        elif len(parts) > 1 and ':' in parts[-1]:\n",
    "            time_part = parts[-1]\n",
    "            date_part = ' '.join(parts[:-1])\n",
    "        else:\n",
    "            date_part = date_str\n",
    "            time_part = '00:00:00'  # Default time for date-only entries\n",
    "\n",
    "        # Convert textual months to numbers\n",
    "        month_mapping = {\n",
    "            'January': '01', 'February': '02', 'March': '03', 'April': '04', 'May': '05', 'June': '06',\n",
    "            'July': '07', 'August': '08', 'September': '09', 'October': '10', 'November': '11', 'December': '12',\n",
    "            'Jan': '01', 'Feb': '02', 'Mar': '03', 'Apr': '04', 'May': '05', 'Jun': '06',\n",
    "            'Jul': '07', 'Aug': '08', 'Sep': '09', 'Oct': '10', 'Nov': '11', 'Dec': '12'\n",
    "        }\n",
    "\n",
    "        # Only apply month conversion if there's a textual month\n",
    "        needs_conversion = any(month in date_part for month in month_mapping)\n",
    "\n",
    "        if needs_conversion:\n",
    "            for month, num in month_mapping.items():\n",
    "                date_part = re.sub(r'\\b' + month + r'\\b', num, date_part, flags=re.IGNORECASE)\n",
    "\n",
    "        # Process the date part\n",
    "        date_parts = re.split(r'[-/. ]', date_part)\n",
    "\n",
    "        if len(date_parts) == 3:\n",
    "            if deduced_format == 'YYYYMMDD':\n",
    "                standardized_date = date_parts[0][:4] + date_parts[1].zfill(2) + date_parts[2].zfill(2)\n",
    "            elif deduced_format == 'DDMMYYYY':\n",
    "                standardized_date = date_parts[0].zfill(2) + date_parts[1].zfill(2) + date_parts[2][:4]\n",
    "            elif deduced_format == 'MMDDYYYY':\n",
    "                standardized_date = date_parts[0].zfill(2) + date_parts[1].zfill(2) + date_parts[2][:4]\n",
    "            else:\n",
    "                return None  # Format mismatch\n",
    "        else:\n",
    "            return None  # Format not recognized\n",
    "\n",
    "        return standardized_date + ' ' + time_part if time_part and standardized_date else None\n",
    "\n",
    "        \n",
    "    @staticmethod\n",
    "    def is_valid_time(time_str):\n",
    "        if isinstance(time_str, time):  \n",
    "            time_str = time_str.strftime(\"%H:%M:%S\")\n",
    "\n",
    "        if not time_str:\n",
    "            return False\n",
    "\n",
    "        time_formats = ['%H:%M', '%I:%M %p', '%H:%M:%S', '%I:%M:%S %p']\n",
    "\n",
    "        for fmt in time_formats:\n",
    "            try:\n",
    "                datetime.strptime(time_str, fmt)\n",
    "                return True\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "        return False\n",
    "\n",
    "    @staticmethod\n",
    "    def handle_outdated_temporal_data(df, column, min_year, max_year):\n",
    "        outdated_entries = []\n",
    "        # Assume a default format for date standardization\n",
    "        default_format = 'DDMMYYYY'  \n",
    "\n",
    "        for idx, val in df[column].items():\n",
    "            standardized_date = DataQualityIssues.standardize_date(str(val), default_format)\n",
    "            year = None\n",
    "\n",
    "            # Skip the processing if standardized_date is None\n",
    "            if standardized_date is None:\n",
    "                continue\n",
    "\n",
    "            if len(standardized_date) == 8 and standardized_date.isdigit():\n",
    "                # Check for YYYYMMDD format\n",
    "                if int(standardized_date[4:6]) <= 12 and int(standardized_date[6:]) <= 31:\n",
    "                    year = int(standardized_date[:4])\n",
    "                # Check for DDMMYYYY format\n",
    "                elif int(standardized_date[:2]) <= 31 and int(standardized_date[2:4]) <= 12:\n",
    "                    year = int(standardized_date[4:])\n",
    "\n",
    "            # Check if the year is within the valid range\n",
    "            if year and not (min_year <= year <= max_year):\n",
    "                outdated_entries.append((idx, val))\n",
    "\n",
    "        total_issues = len(outdated_entries)\n",
    "        if outdated_entries:\n",
    "            if total_issues > 20:\n",
    "                # Keep only the first 10 and last 10 items\n",
    "                first_10 = outdated_entries[:10]\n",
    "                last_10 = outdated_entries[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                message = f\"{total_issues} Date value(s) not in [{min_year}-{max_year}] period at index(es): {display_list} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                message = f\"{total_issues} Date value(s) not in [{min_year}-{max_year}] period at index(es): {outdated_entries}\"\n",
    "\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\": \"DQI #6 (Outdated Temporal Data - Timeliness)\"\n",
    "            }\n",
    "\n",
    "        return {\"issue\": False}\n",
    "    \n",
    "    @staticmethod\n",
    "    def handle_outdated_temporal_data_datetime(df, column, min_year, max_year):\n",
    "        outdated_entries = []\n",
    "        # Assume a default format for date standardization\n",
    "        default_format = 'DDMMYYYY'  \n",
    "\n",
    "        for idx, val in df[column].items():\n",
    "            # Split the datetime into date and time parts using standardize_date_time\n",
    "            standardized_datetime = DataQualityIssues.standardize_date_time(str(val), default_format)\n",
    "            if standardized_datetime is None:\n",
    "                continue\n",
    "\n",
    "            parts = standardized_datetime.split(' ')\n",
    "            date_part = parts[0]  # The date part is always the first part\n",
    "\n",
    "            year = None\n",
    "\n",
    "            # Extract the year from the standardized date\n",
    "            if len(date_part) == 8 and date_part.isdigit():\n",
    "                # Check for YYYYMMDD format\n",
    "                if int(date_part[4:6]) <= 12 and int(date_part[6:]) <= 31:\n",
    "                    year = int(date_part[:4])\n",
    "                # Check for DDMMYYYY format\n",
    "                elif int(date_part[:2]) <= 31 and int(date_part[2:4]) <= 12:\n",
    "                    year = int(date_part[4:])\n",
    "\n",
    "            # Check if the year is within the valid range\n",
    "            if year and not (min_year <= year <= max_year):\n",
    "                outdated_entries.append((idx, val))\n",
    "\n",
    "        total_issues = len(outdated_entries)\n",
    "        if outdated_entries:\n",
    "            if total_issues > 20:\n",
    "                # Keep only the first 10 and last 10 items\n",
    "                first_10 = outdated_entries[:10]\n",
    "                last_10 = outdated_entries[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                message = f\"{total_issues} Datetime value(s) not in [{min_year}-{max_year}] period at index(es): {display_list} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                message = f\"{total_issues} Datetime value(s) not in [{min_year}-{max_year}] period at index(es): {outdated_entries}\"\n",
    "\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\": \"DQI #6 (Outdated Temporal Data - Timeliness)\"\n",
    "            }\n",
    "\n",
    "        return {\"issue\": False}\n",
    "    \n",
    "    @staticmethod\n",
    "    def handle_duplicates(df, column):\n",
    "        duplicate_values = df[column].duplicated(keep=False)  # Mark all duplicates\n",
    "        issue_data = df[duplicate_values][column].reset_index().values.tolist()\n",
    "        total_issues = len(issue_data)\n",
    "\n",
    "        if issue_data:\n",
    "            if total_issues > 20:\n",
    "                first_10 = issue_data[:10]\n",
    "                last_10 = issue_data[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                message = f\"{total_issues} Duplicate value(s) at index(es): {display_list} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                message = f\"{total_issues} Duplicate value(s) at index(es): {issue_data}\"\n",
    "\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\": \"DQI #9 (Duplicates - Uniqueness)\"\n",
    "            }\n",
    "        return {\"issue\": False}\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def handle_excessive_distinct_values(df, column, threshold=100):\n",
    "        unique_values_count = df[column].nunique(dropna=False)\n",
    "\n",
    "        if unique_values_count > threshold:\n",
    "            sample_values = df[column].dropna().unique()[:10]  # Sample of up to 10 unique values\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": f\"Data seems not categorical or has too many categories (> {threshold}). Sample values: {list(sample_values)}\",\n",
    "                \"dq_issue\": \"DQI #10 (Structural Conflicts - Consistency, Uniqueness)\"\n",
    "            }\n",
    "        return {\"issue\": False}\n",
    "\n",
    "                \n",
    "    @staticmethod\n",
    "    def is_date_valid(date_str, fmt):\n",
    "        try:\n",
    "            # Parse the date string based on the provided format\n",
    "            if fmt == 'YYYYMMDD':\n",
    "                year, month, day = int(date_str[:4]), int(date_str[4:6]), int(date_str[6:8])\n",
    "            elif fmt == 'DDMMYYYY':\n",
    "                day, month, year = int(date_str[:2]), int(date_str[2:4]), int(date_str[4:8])\n",
    "            elif fmt == 'MMDDYYYY':\n",
    "                month, day, year = int(date_str[:2]), int(date_str[2:4]), int(date_str[4:8])\n",
    "            else:\n",
    "                return False\n",
    "\n",
    "            # Debug print to check extracted date components\n",
    "            # print(f\"Extracted components for {fmt}: Year: {year}, Month: {month}, Day: {day}\")\n",
    "\n",
    "            # Construct datetime object to validate the date\n",
    "            datetime(year, month, day)\n",
    "\n",
    "            return min_valid_year <= year <= max_valid_year\n",
    "        except ValueError as e:\n",
    "            # print(f\"Date parsing resulted in an error: {e}\")\n",
    "            return False\n",
    "\n",
    "    @staticmethod\n",
    "    def is_valid_in_any_format(date_str):\n",
    "        formats = ['%Y%m%d', '%d%m%Y', '%m%d%Y']  # Add other potential formats\n",
    "        for fmt in formats:\n",
    "            try:\n",
    "                datetime.strptime(date_str, fmt)\n",
    "                return True\n",
    "            except ValueError:\n",
    "                continue\n",
    "        return False\n",
    "        \n",
    "    @staticmethod\n",
    "    def handle_invalid_dates(df, column):\n",
    "        invalid_entries = []\n",
    "        # Assume a default format for date standardization\n",
    "        default_format = 'DDMMYYYY'  \n",
    "\n",
    "        for idx, val in df[column].items():\n",
    "            standardized_date = DataQualityIssues.standardize_date(str(val),default_format)\n",
    "\n",
    "            # Skip the validation if standardized_date is None\n",
    "            if standardized_date is None:\n",
    "                continue\n",
    "\n",
    "            # Check if date is structurally valid in any format\n",
    "            if not (DataQualityIssues.is_date_valid(standardized_date, 'DDMMYYYY') or\n",
    "                    DataQualityIssues.is_date_valid(standardized_date, 'MMDDYYYY') or\n",
    "                    DataQualityIssues.is_date_valid(standardized_date, 'YYYYMMDD')):\n",
    "                invalid_entries.append((idx, val))    \n",
    "\n",
    "        total_issues = len(invalid_entries)\n",
    "        if invalid_entries:\n",
    "            if total_issues > 20:\n",
    "                # Keep only the first 10 and last 10 items\n",
    "                first_10 = invalid_entries[:10]\n",
    "                last_10 = invalid_entries[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                # Convert the list to a string while keeping the square brackets\n",
    "                issue_data = str(display_list)\n",
    "                message = f\"{total_issues} Invalid date value(s) at index(es): {issue_data} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                # Convert the list to a string while keeping the square brackets\n",
    "                issue_data = str(invalid_entries)\n",
    "                message = f\"{total_issues} Invalid date value(s) at index(es): {issue_data}\"\n",
    "\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\":  \"DQI #13 (Temporal mismatch - Accuracy, Timeliness)\"\n",
    "            }\n",
    "\n",
    "        return {\"issue\": False}\n",
    "\n",
    "    @staticmethod\n",
    "    def is_structurally_valid_date(date_str, expected_format):\n",
    "        # Check the date in any format for structural validity\n",
    "        return DataQualityIssues.is_date_valid(date_str, expected_format)\n",
    "\n",
    "    @staticmethod\n",
    "    def handle_invalid_datetimes(df, column, deduced_format):\n",
    "        #print(f\"Handling invalid datetimes. Deduced format: {deduced_format}\")\n",
    "        invalid_datetime_errors = []\n",
    "\n",
    "        for idx, val in df[column].items():\n",
    "            #print(f\"Processing index {idx}, value: {val}\")\n",
    "            standardized_datetime = DataQualityIssues.standardize_date_time(val, deduced_format)\n",
    "\n",
    "            if not standardized_datetime:\n",
    "                #print(f\"Standardization failed for: {val}\")\n",
    "                invalid_datetime_errors.append((idx, val))\n",
    "                continue\n",
    "\n",
    "            parts = standardized_datetime.split(' ')\n",
    "            date_str = parts[0]\n",
    "            time_str = parts[1] if len(parts) > 1 else '00:00:00'\n",
    "\n",
    "            #print(f\"Standardized datetime: {standardized_datetime}, Date: {date_str}, Time: {time_str}\")\n",
    "\n",
    "            if not DataQualityIssues.is_valid_time(time_str):\n",
    "                #print(f\"Invalid time for: {val}\")\n",
    "                invalid_datetime_errors.append((idx, val))\n",
    "            elif not DataQualityIssues.is_structurally_valid_date(date_str, deduced_format):\n",
    "                #print(f\"Date structure invalid for: {val}\")\n",
    "                invalid_datetime_errors.append((idx, val))\n",
    "\n",
    "        total_issues = len(invalid_datetime_errors)\n",
    "        if invalid_datetime_errors:\n",
    "            #print(f\"Total invalid datetime values: {total_issues}\")\n",
    "            if total_issues > 20:\n",
    "                # Keep only the first 10 and last 10 items\n",
    "                first_10 = invalid_datetime_errors[:10]\n",
    "                last_10 = invalid_datetime_errors[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                issue_data = str(display_list)\n",
    "                message = f\"{total_issues} Invalid datetime value(s) at index(es): {issue_data} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                issue_data = str(invalid_datetime_errors)\n",
    "                message = f\"{total_issues} Invalid datetime value(s) at index(es): {issue_data}\"\n",
    "\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\":  \"DQI #13 (Temporal mismatch - Accuracy, Timeliness)\"\n",
    "            }\n",
    "\n",
    "        #print(\"No invalid datetimes found.\")\n",
    "        return {\"issue\": False}\n",
    "\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def handle_invalid_times(df, column):\n",
    "        invalid_time_errors = []\n",
    "        for idx, time_str in df[column].items():\n",
    "            if not DataQualityIssues.is_valid_time(time_str):\n",
    "                invalid_time_errors.append((idx, time_str))\n",
    "\n",
    "        total_issues = len(invalid_time_errors)\n",
    "        if invalid_time_errors:\n",
    "            if total_issues > 20:\n",
    "                # Keep only the first 10 and last 10 items\n",
    "                first_10 = invalid_time_errors[:10]\n",
    "                last_10 = invalid_time_errors[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                # Convert the list to a string while keeping the square brackets\n",
    "                issue_data = str(display_list)\n",
    "                message = f\"{total_issues} Invalid time value(s) at index(es): {issue_data} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                # Convert the list to a string while keeping the square brackets\n",
    "                issue_data = str(invalid_time_errors)\n",
    "                message = f\"{total_issues} Invalid time value(s) at index(es): {issue_data}\"\n",
    "\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\": \"DQI #13 (Temporal mismatch - Accuracy, Timeliness)\"\n",
    "            }\n",
    "\n",
    "        return {\"issue\": False}\n",
    "\n",
    "    @staticmethod\n",
    "    def deduce_regional_format(date_samples):\n",
    "        month_mapping = {\n",
    "            'January': '01', 'February': '02', 'March': '03', 'April': '04', 'May': '05', 'June': '06',\n",
    "            'July': '07', 'August': '08', 'September': '09', 'October': '10', 'November': '11', 'December': '12',\n",
    "            'Jan': '01', 'Feb': '02', 'Mar': '03', 'Apr': '04', 'May': '05', 'Jun': '06',\n",
    "            'Jul': '07', 'Aug': '08', 'Sep': '09', 'Oct': '10', 'Nov': '11', 'Dec': '12'\n",
    "        }\n",
    "        standardized_dates = []\n",
    "        format_counts = {'DDMMYYYY': 0, 'MMDDYYYY': 0, 'YYYYMMDD': 0}\n",
    "\n",
    "        for date in date_samples:\n",
    "            # Convert textual months to numbers\n",
    "            for month, num in month_mapping.items():\n",
    "                date = re.sub(r'\\b' + month + r'\\b', num, date, flags=re.IGNORECASE)\n",
    "\n",
    "            # Handle continuous string dates (without separators)\n",
    "            if len(date) == 8 and date.isdigit():\n",
    "                # Check if the first four characters represent a plausible year\n",
    "                if 1800 <= int(date[:4]) <= 2100:\n",
    "                    format_counts['YYYYMMDD'] += 1\n",
    "                # Check if the first two characters represent a plausible day and next two a month\n",
    "                elif 1 <= int(date[:2]) <= 31 and 1 <= int(date[2:4]) <= 12:\n",
    "                    format_counts['DDMMYYYY'] += 1\n",
    "                # Check if the first two characters represent a plausible month and next two a day\n",
    "                elif 1 <= int(date[:2]) <= 12 and 1 <= int(date[2:4]) <= 31:\n",
    "                    format_counts['MMDDYYYY'] += 1\n",
    "            else:\n",
    "                date_parts = re.split(r'[-/. ]', date)\n",
    "                if len(date_parts) == 3:\n",
    "                    # Check if the year is the first or last component\n",
    "                    if len(date_parts[0]) == 4:\n",
    "                        format_counts['YYYYMMDD'] += 1\n",
    "                    elif len(date_parts[2]) == 4:\n",
    "                        format_counts['DDMMYYYY'] += 1\n",
    "                    else:\n",
    "                        # Default to MMDDYYYY if year is not first or last\n",
    "                        format_counts['MMDDYYYY'] += 1\n",
    "\n",
    "                    standardized_date = '-'.join(date_parts)  # Rejoin the date parts for standardized dates\n",
    "                    standardized_dates.append(standardized_date)\n",
    "\n",
    "        preferred_format = max(format_counts, key=format_counts.get)\n",
    "        #print(f\"Deduced format: {preferred_format}\")  # Diagnostic print\n",
    "\n",
    "        return preferred_format, standardized_dates\n",
    "    \n",
    "    @staticmethod\n",
    "    def handle_dates_format(df, column, expected_format):\n",
    "        format_errors = []\n",
    "        for idx, val in df[column].items():\n",
    "            standardized_date = DataQualityIssues.standardize_date(str(val), expected_format)\n",
    "\n",
    "            # Check if standardized_date is None\n",
    "            if standardized_date is None:\n",
    "                # If the date can't be standardized, skip it (it will be caught in other checks)\n",
    "                continue\n",
    "            \n",
    "            # Check if the date is valid according to the expected format\n",
    "            if not DataQualityIssues.is_date_valid(standardized_date, expected_format):\n",
    "                format_errors.append((idx, val))\n",
    "\n",
    "        total_issues = len(format_errors)\n",
    "        if format_errors:\n",
    "            if total_issues > 20:\n",
    "                # Keep only the first 10 and last 10 items\n",
    "                first_10 = format_errors[:10]\n",
    "                last_10 = format_errors[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                message = f\"{total_issues} Date value(s) without format '{expected_format}' in [{min_valid_year}-{max_valid_year}] period at index(es): {display_list} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                message = f\"{total_issues} Date value(s) without format '{expected_format}' in [{min_valid_year}-{max_valid_year}] period at index(es): {format_errors}\"\n",
    "           \n",
    "            return{\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\": \"DQI #14 (Different units/representations - Consistency)\"\n",
    "            }\n",
    "\n",
    "        return {\"issue\": False}\n",
    "\n",
    "    @staticmethod\n",
    "    def handle_datetimes_format(df, column, expected_format):\n",
    "        format_errors = []\n",
    " \n",
    "        for idx, val in df[column].items():\n",
    "            standardized_datetime = DataQualityIssues.standardize_date_time(str(val), expected_format)\n",
    "\n",
    "            if not standardized_datetime:\n",
    "                print(f\"Index {idx}: Standardization failed.\")\n",
    "                continue\n",
    "\n",
    "            date_str = standardized_datetime.split(' ')[0]\n",
    "            \n",
    "             # Check if the date is valid according to the expected format\n",
    "            if DataQualityIssues.is_valid_in_any_format(date_str) and not DataQualityIssues.is_structurally_valid_date(date_str, expected_format):\n",
    "                format_errors.append((idx, val))\n",
    "       \n",
    "        total_issues = len(format_errors)\n",
    "        if format_errors:\n",
    "            if total_issues > 20:\n",
    "                # Keep only the first 10 and last 10 items\n",
    "                first_10 = format_errors[:10]\n",
    "                last_10 = format_errors[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                message = f\"{total_issues} Datetime value(s) without format '{expected_format}' in [{min_valid_year}-{max_valid_year}] period at index(es): {display_list} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                message = f\"{total_issues} Datetime value(s) without format '{expected_format}' in [{min_valid_year}-{max_valid_year}] period at index(es): {format_errors}\"\n",
    "           \n",
    "            return{\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\": \"DQI #14 (Different units/representations - Consistency)\"\n",
    "            }\n",
    "\n",
    "        return {\"issue\": False}\n",
    "\n",
    "    @staticmethod\n",
    "    def handle_negative_values(df, column):\n",
    "        # Function to check if a value is negative\n",
    "        def is_negative(x):\n",
    "            try:\n",
    "                num_val = float(x)\n",
    "                return num_val < 0  # Check if the value is negative\n",
    "            except (ValueError, TypeError):\n",
    "                return False  # Non-numeric values are not considered here\n",
    "\n",
    "        # Get indices and values for negative values\n",
    "        negative_indices = df[df[column].apply(is_negative)].index\n",
    "        negative_values = df.loc[negative_indices, column].tolist()\n",
    "        issue_data = list(zip(negative_indices, negative_values))\n",
    "        total_issues = len(issue_data)\n",
    "\n",
    "        if issue_data:\n",
    "            if total_issues > 20:\n",
    "                # Keep only the first 10 and last 10 items\n",
    "                first_10 = issue_data[:10]\n",
    "                last_10 = issue_data[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                message = f\"{total_issues} Negative value(s) at index(es): {display_list} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                message = f\"{total_issues} Negative value(s) at index(es): {issue_data}\"\n",
    "\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\": \"DQI #15 (Domain Violation - Accuracy)\"\n",
    "            }\n",
    "\n",
    "        return {\"issue\": False}\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def handle_values_outside_range(df, column, min_value, max_value):\n",
    "        # Function to check if a value is within the specified range\n",
    "        def is_outside_range(x):\n",
    "            try:\n",
    "                num_val = float(x)\n",
    "                return num_val < min_value or num_val > max_value\n",
    "            except (ValueError, TypeError):\n",
    "                return False  # Non-numeric values are not considered here\n",
    "\n",
    "        # Get indices and values for values outside the specified range\n",
    "        outside_range_indices = df[df[column].apply(is_outside_range)].index\n",
    "        outside_range_values = df.loc[outside_range_indices, column].tolist()\n",
    "        issue_data = list(zip(outside_range_indices, outside_range_values))\n",
    "        total_issues = len(issue_data)\n",
    "\n",
    "        if issue_data:\n",
    "            if total_issues > 20:\n",
    "                # Keep only the first 10 and last 10 items\n",
    "                first_10 = issue_data[:10]\n",
    "                last_10 = issue_data[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                message = f\"{total_issues} Value(s) outside range [{min_value}, {max_value}] at index(es): {display_list} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                message = f\"{total_issues} Value(s) outside range [{min_value}, {max_value}] at index(es): {issue_data}\"\n",
    "\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\": \"DQI #15 (Domain Violation - Accuracy)\"\n",
    "            }\n",
    "\n",
    "        return {\"issue\": False}\n",
    "       \n",
    "           \n",
    "    @staticmethod\n",
    "    def handle_floating_point_values(df, column):\n",
    "        \"\"\"\n",
    "        Flags floating-point numbers in a column.\n",
    "\n",
    "        Parameters:\n",
    "        - df (pd.DataFrame): The pandas DataFrame to analyze.\n",
    "        - column (str): The name of the column to check.\n",
    "\n",
    "        Returns:\n",
    "        - dict: A dictionary indicating if there are floating-point values, with their indices and values, and the associated DQI.\n",
    "        \"\"\"\n",
    "        floating_point_indices = []\n",
    "        for idx, value in df[column].items():\n",
    "            try:\n",
    "                # If it's a floating point and not an integer\n",
    "                if float(value) and not float(value).is_integer():\n",
    "                    floating_point_indices.append((idx, value))\n",
    "            except ValueError:\n",
    "                # Ignore non-numeric values, handle them in other checks\n",
    "                continue\n",
    "\n",
    "        total_issues = len(floating_point_indices)\n",
    "        if floating_point_indices:\n",
    "            if total_issues > 20:\n",
    "                # Keep only the first 10 and last 10 items\n",
    "                first_10 = floating_point_indices[:10]\n",
    "                last_10 = floating_point_indices[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                message = f\"{total_issues} Floating-point number(s) at index(es): {display_list} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                message = f\"{total_issues} Floating-point number(s) at index(es): {floating_point_indices}\"\n",
    "\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\": \"DQI #15 (Domain Violation - Accuracy)\"\n",
    "            }\n",
    "\n",
    "        return {\"issue\": False}\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def handle_capitalization_format(df, column, linking_words):\n",
    "        \"\"\"\n",
    "        Check if the values in the column adhere to capitalization and format standards.\n",
    "        \"\"\"\n",
    "        def is_capitalization_issue(word):\n",
    "            # Allow all-uppercase words, ordinal numbers like '3rd', '5th', etc., and linking words\n",
    "            if word.isupper() or re.match(r\"^\\d+(st|nd|rd|th)$\", word.lower()) or word.lower() in linking_words:\n",
    "                return False\n",
    "            return word != word.title()\n",
    "\n",
    "        format_issues = []\n",
    "        for idx, val in df[column].items():\n",
    "            words = str(val).split()\n",
    "            if any(is_capitalization_issue(word) for word in words):\n",
    "                format_issues.append((idx, val))\n",
    "\n",
    "        total_issues = len(format_issues)\n",
    "        if format_issues:\n",
    "            if total_issues > 20:\n",
    "                # Keep only the first 10 and last 10 items\n",
    "                first_10 = format_issues[:10]\n",
    "                last_10 = format_issues[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                # Convert the list to a string while keeping the square brackets\n",
    "                issue_data = str(display_list)\n",
    "                message = f\"{total_issues} Capitalization/Format issue(s) at index(es): \" + issue_data + \" (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                # Convert the list to a string while keeping the square brackets\n",
    "                issue_data = str(format_issues)\n",
    "                message = f\"{total_issues} Capitalization/Format issue(s) at index(es): \" + issue_data\n",
    "\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\": \"DQI #15 (Domain Violation - Accuracy)\"\n",
    "            }\n",
    "\n",
    "        return {\"issue\": False}\n",
    "\n",
    "    @staticmethod\n",
    "    def handle_capitalization_format_country(df, column, linking_words):\n",
    "        \"\"\"\n",
    "        Check if the values in the column adhere to capitalization and format standards,\n",
    "        with exceptions for certain patterns typical in country names.\n",
    "        \"\"\"\n",
    "        def is_exceptional_case(word, prev_word=None):\n",
    "            # Directly return False (no issue) for all-uppercase words or ordinal numbers\n",
    "            if word.isupper() or re.match(r\"^\\d+(st|nd|rd|th)$\", word.lower()):\n",
    "                return False\n",
    "            # Check if the word is a linking word or part of an exception pattern (e.g., within parentheses or after a hyphen)\n",
    "            if word.lower() in linking_words or (prev_word and prev_word.endswith('-')):\n",
    "                return True\n",
    "            return False\n",
    "        \n",
    "        def is_capitalization_issue(words):\n",
    "            # Check for capitalization issue while considering exceptions\n",
    "            for i, word in enumerate(words):\n",
    "                prev_word = words[i-1] if i > 0 else None\n",
    "                # Skip words that are part of an exceptional case\n",
    "                if is_exceptional_case(word, prev_word):\n",
    "                    continue\n",
    "                # Identify words that fail the capitalization check (excluding exceptions)\n",
    "                if not word.istitle() and not word.isupper():\n",
    "                    return True\n",
    "            return False\n",
    "\n",
    "        format_issues = []\n",
    "        for idx, val in df[column].items():\n",
    "            original_val = str(val)\n",
    "            # Split the value considering spaces and treating content in parentheses as a single block\n",
    "            words = re.findall(r'\\b\\w+\\b', original_val)\n",
    "            if is_capitalization_issue(words):\n",
    "                format_issues.append((idx, val))\n",
    "\n",
    "        total_issues = len(format_issues)\n",
    "        if format_issues:\n",
    "            # Prepare the message with a simplified display if there are many issues\n",
    "            display_list = format_issues[:10] + [('...', '...')] if total_issues > 20 else format_issues\n",
    "            issue_data = str(display_list)\n",
    "            message = f\"{total_issues} Capitalization/Format issue(s) at index(es): \" + issue_data\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\": \"DQI #15 (Domain Violation - Accuracy)\"\n",
    "            }\n",
    "\n",
    "        return {\"issue\": False}\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def handle_short_length_values(df, column, min_length):\n",
    "        \"\"\"\n",
    "        Flags values in a column that are shorter than the specified minimum length, excluding non-alphanumeric characters.\n",
    "\n",
    "        Parameters:\n",
    "        - df (pd.DataFrame): The pandas DataFrame to analyze.\n",
    "        - column (str): The name of the column to check.\n",
    "        - min_length (int): The minimum acceptable length for values.\n",
    "\n",
    "        Returns:\n",
    "        - dict: A dictionary indicating if there are short values, with their indices and values, and the associated DQI.\n",
    "        \"\"\"\n",
    "        def is_short_and_alphanumeric(x):\n",
    "            return isinstance(x, str) and len(x) < min_length and x.isalnum()\n",
    "\n",
    "        short_values = df[df[column].apply(is_short_and_alphanumeric)]\n",
    "        issue_data = list(zip(short_values.index, short_values[column]))\n",
    "        total_issues = len(issue_data)\n",
    "\n",
    "        if issue_data:\n",
    "            if total_issues > 20:\n",
    "                # Keep only the first 10 and last 10 items\n",
    "                first_10 = issue_data[:10]\n",
    "                last_10 = issue_data[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                message = f\"{total_issues} Short length alphanumeric value(s) at index(es): {display_list} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                message = f\"{total_issues} Short length alphanumeric value(s) at index(es): {issue_data}\"\n",
    "\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\": \"DQI #15 (Domain Violation - Accuracy)\"\n",
    "            }\n",
    "\n",
    "        return {\"issue\": False}\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_month(month_str):\n",
    "        \"\"\"\n",
    "        Normalize the given month string to its full month name if valid.\n",
    "\n",
    "        Parameters:\n",
    "        - month_str (str): The month string to normalize.\n",
    "\n",
    "        Returns:\n",
    "        - str/None: Normalized month name if valid, None otherwise.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if month_str.isdigit():\n",
    "                month_val = int(month_str)\n",
    "                if 1 <= month_val <= 12:\n",
    "                    return datetime(2000, month_val, 1).strftime('%B')\n",
    "            elif len(month_str) == 3:\n",
    "                return datetime.strptime(month_str.title(), '%b').strftime('%B')\n",
    "            else:\n",
    "                datetime.strptime(month_str.title(), '%B')\n",
    "                return month_str.title()\n",
    "        except ValueError:\n",
    "            return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def handle_invalid_months(df, column):\n",
    "        \"\"\"\n",
    "        Check if all values in the specified column are valid representations of months.\n",
    "\n",
    "        Parameters:\n",
    "        - df (pd.DataFrame): The pandas DataFrame to analyze.\n",
    "        - column (str): The name of the column to analyze.\n",
    "\n",
    "        Returns:\n",
    "        - dict: A dictionary indicating if there are invalid month values, with their indices and values.\n",
    "        \"\"\"\n",
    "        invalid_entries = []\n",
    "        for idx, val in df[column].items():\n",
    "            if not DataQualityIssues.normalize_month(str(val)):\n",
    "                invalid_entries.append((idx, val))\n",
    "\n",
    "        total_issues = len(invalid_entries)\n",
    "        if invalid_entries:\n",
    "            if total_issues > 20:\n",
    "                # Keep only the first 10 and last 10 items\n",
    "                first_10 = invalid_entries[:10]\n",
    "                last_10 = invalid_entries[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                message = f\"{total_issues} Invalid month value(s) at index(es): {display_list} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                message = f\"{total_issues} Invalid month value(s) at index(es): {invalid_entries}\"\n",
    "\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\": \"DQI #15 (Domain Violation - Accuracy)\"\n",
    "            }\n",
    "\n",
    "        return {\"issue\": False}\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def handle_invalid_weekdays(df, column):\n",
    "        \"\"\"\n",
    "        Check if all values in the specified column are valid representations of weekdays.\n",
    "\n",
    "        Parameters:\n",
    "        - df (pd.DataFrame): The pandas DataFrame to analyze.\n",
    "        - column (str): The name of the column to analyze.\n",
    "\n",
    "        Returns:\n",
    "        - dict: A dictionary indicating if there are invalid weekday values, with their indices and values.\n",
    "        \"\"\"\n",
    "        invalid_entries = []\n",
    "        weekdays = [\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\"]\n",
    "        weekday_abbr = [\"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\"]\n",
    "        weekday_abbr_short = [\"su\", \"mo\", \"tu\", \"we\", \"th\", \"fr\", \"sa\"]\n",
    "\n",
    "        for idx, val in df[column].items():\n",
    "            val_str = str(val).strip().lower()\n",
    "            # Normalize full names and abbreviations to full weekday name\n",
    "            if val_str.title() in weekdays or val_str.title() in weekday_abbr or val_str in weekday_abbr_short:\n",
    "                continue\n",
    "            elif val_str.isdigit() and 0 <= int(val_str) <= 7:\n",
    "                continue\n",
    "            else:\n",
    "                invalid_entries.append((idx, val))\n",
    "\n",
    "        total_issues = len(invalid_entries)\n",
    "        if invalid_entries:\n",
    "            if total_issues > 20:\n",
    "                # Keep only the first 10 and last 10 items\n",
    "                first_10 = invalid_entries[:10]\n",
    "                last_10 = invalid_entries[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                message = f\"{total_issues} Invalid weekday value(s) at index(es): {display_list} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                message = f\"{total_issues} Invalid weekday value(s) at index(es): {invalid_entries}\"\n",
    "\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\": \"DQI #15 (Domain Violation - Accuracy)\"\n",
    "            }\n",
    "\n",
    "        return {\"issue\": False}\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def handle_street_format(df, column):\n",
    "        \"\"\"\n",
    "        Check if the street names conform to a typical street format.\n",
    "        \"\"\"\n",
    "        invalid_format_indices = []\n",
    "        street_format_regex = re.compile(r'^[\\dA-Za-zÃ€-Ã–Ã˜-Ã¶Ã¸-Ã¿ .,\\-\\'#]+(?:\\s[A-Za-zÃ€-Ã–Ã˜-Ã¶Ã¸-Ã¿]+)*$')\n",
    "\n",
    "        for idx, val in df[column].items():\n",
    "            if not isinstance(val, str) or not street_format_regex.match(val):\n",
    "                invalid_format_indices.append((idx, val))\n",
    "\n",
    "        total_issues = len(invalid_format_indices)\n",
    "        if invalid_format_indices:\n",
    "            if total_issues > 20:\n",
    "                # Keep only the first 10 and last 10 items\n",
    "                first_10 = invalid_format_indices[:10]\n",
    "                last_10 = invalid_format_indices[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                issue_data = \", \".join([f\"({idx}, '{value}')\" for idx, value in display_list])\n",
    "                message = f\"{total_issues} Incorrect street format issue(s) at index(es): {issue_data} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                issue_data = \", \".join([f\"({idx}, '{value}')\" for idx, value in invalid_format_indices])\n",
    "                message = f\"{total_issues} Incorrect street format issue(s) at index(es): {issue_data}\"\n",
    "\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\": \"DQI #15 (Domain Violation - Accuracy)\"\n",
    "            }\n",
    "\n",
    "        return {\"issue\": False}\n",
    "\n",
    "    @staticmethod\n",
    "    def standardize_phone_number(phone_number):\n",
    "        \"\"\"\n",
    "        Standardize the phone number by removing common separators.\n",
    "        \"\"\"\n",
    "        return re.sub(r'[()\\-+ ]', '', str(phone_number))\n",
    "\n",
    "    @staticmethod\n",
    "    def handle_phone_number_format(df, column):\n",
    "        \"\"\"\n",
    "        Check if phone numbers in the specified column conform to expected formats.\n",
    "        \"\"\"\n",
    "        incorrect_format = []\n",
    "\n",
    "        for idx, phone_number in df[column].items():\n",
    "            if phone_number is None or isinstance(phone_number, str) and phone_number.strip() == '':\n",
    "                incorrect_format.append((idx, phone_number))\n",
    "                continue\n",
    "\n",
    "            cleaned_number = DataQualityIssues.standardize_phone_number(phone_number)\n",
    "            \n",
    "            if not (cleaned_number.isdigit() and 3 <= len(cleaned_number) <= 15):\n",
    "                incorrect_format.append((idx, phone_number))\n",
    "\n",
    "        total_issues = len(incorrect_format)\n",
    "        if incorrect_format:\n",
    "            if total_issues > 20:\n",
    "                # Keep only the first 10 and last 10 items\n",
    "                first_10 = incorrect_format[:10]\n",
    "                last_10 = incorrect_format[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                # Convert the list to a string while keeping the square brackets\n",
    "                issue_data = str(display_list)\n",
    "                message = f\"{total_issues} Incorrect telephone number format issue(s) at index(es): {issue_data} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                # Convert the list to a string while keeping the square brackets\n",
    "                issue_data = str(incorrect_format)\n",
    "                message = f\"{total_issues} Incorrect telephone number format issue(s) at index(es): {issue_data}\"\n",
    "\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\": \"DQI #15 (Domain Violation - Accuracy)\"\n",
    "            }\n",
    "\n",
    "        return {\"issue\": False}\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def handle_ip_format(df, column):\n",
    "        # Regular expression patterns for IPv4 and IPv6\n",
    "        ipv4_pattern = re.compile(r'^(?:[0-9]{1,3}\\.){3}[0-9]{1,3}$')\n",
    "        ipv6_pattern = re.compile(r'^([0-9a-fA-F]{1,4}:){7}[0-9a-fA-F]{1,4}$')\n",
    "\n",
    "        incorrect_indices_and_values = []\n",
    "\n",
    "        for idx, ip in df[column].items():\n",
    "            ip_str = str(ip).strip()  # Convert to string and strip whitespace\n",
    "            if not (ipv4_pattern.match(ip_str) or ipv6_pattern.match(ip_str)):\n",
    "                incorrect_indices_and_values.append((idx, ip))\n",
    "\n",
    "        total_issues = len(incorrect_indices_and_values)\n",
    "        if incorrect_indices_and_values:\n",
    "            if total_issues > 20:\n",
    "                # Keep only the first 10 and last 10 items\n",
    "                first_10 = incorrect_indices_and_values[:10]\n",
    "                last_10 = incorrect_indices_and_values[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                # Convert the list to a string while keeping the square brackets\n",
    "                issue_data = str(display_list)\n",
    "                message = f\"{total_issues} Invalid IP format issue(s) at index(es): {issue_data} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                # Convert the list to a string while keeping the square brackets\n",
    "                issue_data = str(incorrect_indices_and_values)\n",
    "                message = f\"{total_issues} Invalid IP format issue(s) at index(es): {issue_data}\"\n",
    "\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\": \"DQI #15 (Domain Violation - Accuracy)\"\n",
    "            }\n",
    "\n",
    "        return {\"issue\": False}\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def handle_url_format(df, column):\n",
    "        url_pattern = re.compile(\n",
    "            r'^(https?:\\/\\/)?'  # protocol\n",
    "            r'((([a-zA-Z0-9-]+\\.)+[a-zA-Z]{2,})|'  # domain name\n",
    "            r'localhost|'  # localhost\n",
    "            r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})'  # or ip\n",
    "            r'(?::\\d+)?(\\/[-a-zA-Z0-9@:%_\\+.~#?&//=]*)?$'  # port and path\n",
    "        )\n",
    "        incorrect_indices_and_values = []\n",
    "\n",
    "        for idx, url in df[column].items():\n",
    "            url_str = str(url)  # Convert to string\n",
    "            if not url_pattern.match(url_str):\n",
    "                incorrect_indices_and_values.append((idx, url))\n",
    "\n",
    "        total_issues = len(incorrect_indices_and_values)\n",
    "        if incorrect_indices_and_values:\n",
    "            if total_issues > 20:\n",
    "                # Keep only the first 10 and last 10 items\n",
    "                first_10 = incorrect_indices_and_values[:10]\n",
    "                last_10 = incorrect_indices_and_values[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                # Convert the list to a string while keeping the square brackets\n",
    "                issue_data = str(display_list)\n",
    "                message = f\"{total_issues} Invalid URL format issue(s) at index(es): {issue_data} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                # Convert the list to a string while keeping the square brackets\n",
    "                issue_data = str(incorrect_indices_and_values)\n",
    "                message = f\"{total_issues} Invalid URL format issue(s) at index(es): {issue_data}\"\n",
    "\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\": \"DQI #15 (Domain Violation - Accuracy)\"\n",
    "            }\n",
    "\n",
    "        return {\"issue\": False}\n",
    "\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def is_valid_email(email):\n",
    "        email_str = str(email)  # Convert to string\n",
    "\n",
    "        # Check the length of the email\n",
    "        if len(email_str) > 254:\n",
    "            return False\n",
    "\n",
    "        # Check if '@' is present\n",
    "        if '@' not in email_str:\n",
    "            return False\n",
    "\n",
    "        # Enhanced email regex pattern\n",
    "        email_pattern = re.compile(\n",
    "            r'^[a-zA-Z0-9.!#$%&\\'*+/=?^_`{|}~-]+@'  # local part\n",
    "            r'(?:[a-zA-Z0-9-]+)'  # domain name part (subdomains allowed)\n",
    "            r'(?:\\.[a-zA-Z0-9-]+)*'  # additional subdomains\n",
    "            r'\\.[a-zA-Z]{2,}$'  # TLD part\n",
    "        )\n",
    "\n",
    "        # Check for consecutive dots in local part\n",
    "        if '..' in email.split('@')[0]:\n",
    "            return False\n",
    "\n",
    "        # Extract and check the domain part\n",
    "        domain_part = email.split('@')[1]\n",
    "        if '--' in domain_part or domain_part.startswith('-') or domain_part.endswith('-'):\n",
    "            return False\n",
    "\n",
    "        return email_pattern.match(email) is not None\n",
    "\n",
    "    @staticmethod\n",
    "    def handle_email_format(df, column):\n",
    "        incorrect_indices_and_values = []\n",
    "        \n",
    "        for idx, email in df[column].items():\n",
    "            if not DataQualityIssues.is_valid_email(email):\n",
    "                incorrect_indices_and_values.append((idx, email))\n",
    "\n",
    "        total_issues = len(incorrect_indices_and_values)\n",
    "        if incorrect_indices_and_values:\n",
    "            if total_issues > 20:\n",
    "                # Keep only the first 10 and last 10 items\n",
    "                first_10 = incorrect_indices_and_values[:10]\n",
    "                last_10 = incorrect_indices_and_values[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                # Convert the list to a string while keeping the square brackets\n",
    "                issue_data = str(display_list)\n",
    "                message = f\"{total_issues} Invalid email format issue(s) at index(es): {issue_data} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                # Convert the list to a string while keeping the square brackets\n",
    "                issue_data = str(incorrect_indices_and_values)\n",
    "                message = f\"{total_issues} Invalid email format issue(s) at index(es): {issue_data}\"\n",
    "\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\": \"DQI #15 (Domain Violation - Accuracy)\"\n",
    "            }\n",
    "\n",
    "        return {\"issue\": False}\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def handle_binary_values(df, column):\n",
    "        \"\"\"\n",
    "        Check if the values in the specified column are valid binary values or if the column has exactly two unique values.\n",
    "\n",
    "        Parameters:\n",
    "        - df (pd.DataFrame): The pandas DataFrame to analyze.\n",
    "        - column (str): The name of the column to check.\n",
    "\n",
    "        Returns:\n",
    "        - dict: A dictionary indicating if there are non-binary values, with their indices and values, and the associated DQI.\n",
    "        \"\"\"\n",
    "        # Calculate unique non-null values\n",
    "        unique_values = df[column].dropna().unique()\n",
    "        \n",
    "        # If there are exactly two unique values, consider it binary and return no issue\n",
    "        if len(unique_values) == 2:\n",
    "            return {\"issue\": False}\n",
    "        \n",
    "        # Define acceptable binary values (including lowercase)\n",
    "        true_values = ['1', 'True', 'Yes', 'T', 'Y']\n",
    "        false_values = ['0', 'False', 'No', 'F','N']\n",
    "\n",
    "        non_binary_indices = []\n",
    "\n",
    "        for idx, val in df[column].items():\n",
    "            str_val = str(val).strip().capitalize()  # Capitalize to match 'True', 'False', etc.\n",
    "            if str_val not in true_values + false_values:\n",
    "                non_binary_indices.append((idx, val))\n",
    "\n",
    "        total_issues = len(non_binary_indices)\n",
    "        if non_binary_indices:\n",
    "            if total_issues > 20:\n",
    "                # Keep only the first 10 and last 10 items\n",
    "                first_10 = non_binary_indices[:10]\n",
    "                last_10 = non_binary_indices[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                # Convert the list to a string while keeping the square brackets\n",
    "                issue_data = str(display_list)\n",
    "                message = f\"{total_issues} Non-binary value(s) at index(es): {issue_data} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                # Convert the list to a string while keeping the square brackets\n",
    "                issue_data = str(non_binary_indices)\n",
    "                message = f\"{total_issues} Non-binary value(s) at index(es): {issue_data}\"\n",
    "\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\": \"DQI #15 (Domain Violation - Accuracy)\"\n",
    "            }\n",
    "\n",
    "        return {\"issue\": False}\n",
    "\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def handle_non_numeric_values(df, column):\n",
    "        # Define a function to check if a value is non-numeric and not a special placeholder\n",
    "        def is_non_numeric_and_not_placeholder(x):\n",
    "            special_placeholders = [\"''\", '\"\"', \"``\"]  # List of special placeholder strings\n",
    "            x_str = str(x).strip()\n",
    "            if x_str in special_placeholders:\n",
    "                return False  # Do not flag special placeholders as non-numeric\n",
    "\n",
    "            try:\n",
    "                float(x_str)  # Attempt to convert to float\n",
    "                return False  # Conversion successful, value is numeric\n",
    "            except (ValueError, TypeError):\n",
    "                return True  # Conversion failed, value is non-numeric\n",
    "\n",
    "        # Get indices and values for non-numeric values\n",
    "        non_numeric_indices = df[df[column].apply(is_non_numeric_and_not_placeholder)].index\n",
    "        non_numeric_values = df.loc[non_numeric_indices, column].tolist()\n",
    "        issue_data = list(zip(non_numeric_indices, non_numeric_values))\n",
    "        total_issues = len(issue_data)\n",
    "\n",
    "        if issue_data:\n",
    "            if total_issues > 20:\n",
    "                first_10 = issue_data[:10]\n",
    "                last_10 = issue_data[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                message = f\"{total_issues} Non-numeric value(s) at index(es): {display_list} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                message = f\"{total_issues} Non-numeric value(s) at index(es): {issue_data}\"\n",
    "\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\": \"DQI #17 (Wrong Data Type - Consistency)\"\n",
    "            }\n",
    "        return {\"issue\": False}\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def handle_non_alphanumeric_values(df, column):\n",
    "        # Define a function to check for strictly non-alphanumeric characters (excluding spaces and hyphens)\n",
    "        def is_strictly_non_alphanumeric(x):\n",
    "            return any(char not in string.ascii_letters + string.digits + ' -' for char in x)\n",
    "\n",
    "        # Get indices and values that are strictly non-alphanumeric\n",
    "        non_alphanumeric_indices = df[df[column].apply(lambda x: isinstance(x, str) and is_strictly_non_alphanumeric(x))].index\n",
    "        non_alphanumeric_values = df.loc[non_alphanumeric_indices, column].tolist()\n",
    "        issue_data = list(zip(non_alphanumeric_indices, non_alphanumeric_values))\n",
    "\n",
    "        total_issues = len(issue_data)\n",
    "        if issue_data:\n",
    "            if total_issues > 20:\n",
    "                # Keep only the first 10 and last 10 items\n",
    "                first_10 = issue_data[:10]\n",
    "                last_10 = issue_data[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                # Convert the list to a string while keeping the square brackets\n",
    "                issue_data = str(display_list)\n",
    "                message = f\"{total_issues} Non-alphanumeric value(s) at index(es): {issue_data} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                # Convert the list to a string while keeping the square brackets\n",
    "                issue_data = str(issue_data)\n",
    "                message = f\"{total_issues} Non-alphanumeric value(s) at index(es): {issue_data}\"\n",
    "\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\": \"DQI #17 (Wrong Data Type - Consistency)\"\n",
    "            }\n",
    "\n",
    "        return {\"issue\": False}\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def handle_non_string_values(df, column):\n",
    "        \"\"\"\n",
    "        Flags values in a column that are not of string type.\n",
    "\n",
    "        Parameters:\n",
    "        - df (pd.DataFrame): The pandas DataFrame to analyze.\n",
    "        - column (str): The name of the column to check.\n",
    "\n",
    "        Returns:\n",
    "        - dict: A dictionary indicating if there are non-string values, with their indices and values, and the associated DQI.\n",
    "        \"\"\"\n",
    "        non_string_indices = df[df[column].apply(lambda x: not isinstance(x, str))].index\n",
    "        non_string_values = df.loc[non_string_indices, column].tolist()\n",
    "        issue_data = list(zip(non_string_indices, non_string_values))\n",
    "\n",
    "        total_issues = len(issue_data)\n",
    "        if issue_data:\n",
    "            if total_issues > 20:\n",
    "                # Keep only the first 10 and last 10 items\n",
    "                first_10 = issue_data[:10]\n",
    "                last_10 = issue_data[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                # Convert the list to a string while keeping the square brackets\n",
    "                issue_data = str(display_list)\n",
    "                message = f\"{total_issues} Non-string value(s) at index(es): {issue_data} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                # Convert the list to a string while keeping the square brackets\n",
    "                issue_data = str(issue_data)\n",
    "                message = f\"{total_issues} Non-string value(s) at index(es): {issue_data}\"\n",
    "\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\": \"DQI #17 (Non-String Data Type - Consistency)\"\n",
    "            }\n",
    "\n",
    "        return {\"issue\": False}\n",
    "\n",
    "\n",
    "        \n",
    "    @staticmethod\n",
    "    def handle_alphanumeric_consistency(df, column):\n",
    "        \"\"\"\n",
    "        Check if the values in the column are alphanumeric and of consistent length,\n",
    "        excluding negative and floating-point numbers.\n",
    "        \"\"\"\n",
    "        # Filter out negative and floating-point numbers\n",
    "        filtered_df = df[df[column].apply(lambda x: not isinstance(x, (float, int)) or x >= 0 and float(x).is_integer())]\n",
    "\n",
    "        # Sample the first 10 non-null, non-empty, non-negative, non-floating values\n",
    "        id_samples = filtered_df[column].dropna().astype(str).str.strip()\n",
    "        id_samples = id_samples[id_samples != ''].head(10)\n",
    "\n",
    "        length_set = {len(val) for val in id_samples if val.isalnum()}\n",
    "        alphanumeric_consistent = len(length_set) == 1\n",
    "\n",
    "        if not alphanumeric_consistent:\n",
    "            return {\"issue\": False}  # Return no issue if first 10 samples are not consistent\n",
    "\n",
    "        consistent_length = length_set.pop()\n",
    "        inconsistent_indices = []\n",
    "\n",
    "        for idx, val in filtered_df[column].items():\n",
    "            val_str = str(val).strip()\n",
    "            if len(val_str) != consistent_length or not val_str.isalnum():\n",
    "                inconsistent_indices.append((idx, val))\n",
    "\n",
    "        total_issues = len(inconsistent_indices)\n",
    "        if inconsistent_indices:\n",
    "            if total_issues > 20:\n",
    "                # Keep only the first 10 and last 10 items\n",
    "                first_10 = inconsistent_indices[:10]\n",
    "                last_10 = inconsistent_indices[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                # Convert the list to a string while keeping the square brackets\n",
    "                issue_data = str(display_list)\n",
    "                message = f\"{total_issues} Inconsistent length in alphanumeric value(s) at index(es): {issue_data} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                # Convert the list to a string while keeping the square brackets\n",
    "                issue_data = str(inconsistent_indices)\n",
    "                message = f\"{total_issues} Inconsistent length in alphanumeric value(s) at index(es): {issue_data}\"\n",
    "\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\": \"DQI #17 (Wrong Data Type - Consistency)\"\n",
    "            }\n",
    "\n",
    "        return {\"issue\": False}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def handle_uniqueness_violation(df, column):\n",
    "        unique_violation_indices = df[df[column].duplicated()].index\n",
    "        unique_violation_values = df.loc[unique_violation_indices, column].tolist()\n",
    "        issue_data = list(zip(unique_violation_indices, unique_violation_values))\n",
    "        total_issues = len(issue_data)\n",
    "\n",
    "        if issue_data:\n",
    "            if total_issues > 20:\n",
    "                first_10 = issue_data[:10]\n",
    "                last_10 = issue_data[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                message = f\"{total_issues} Uniqueness violation(s) at index(es): {display_list} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                message = f\"{total_issues} Uniqueness violation(s) at index(es): {issue_data}\"\n",
    "\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\": \"DQI #19 (Uniqueness Violation - Uniqueness)\"\n",
    "            }\n",
    "        return {\"issue\": False}\n",
    "\n",
    "    @staticmethod\n",
    "    def handle_special_characters(df, column):\n",
    "        \"\"\"\n",
    "        Check for special characters in the specified column.\n",
    "        Allow periods in names for titles (e.g., Mr., Mrs.).\n",
    "        \"\"\"\n",
    "        def has_special_chars(x):\n",
    "            return any(char in x for char in ['!', '@', '#', '$', '%', '^', '&', '*', '(', ')', '+', '=', '{', '}', '[', ']', '|', '\\\\', ':', ';', '\"', \"'\", '<', '>', ',', '/', '?']) and '.' not in x\n",
    "\n",
    "        # Get indices and values with special characters\n",
    "        special_chars_indices = df[df[column].apply(lambda x: isinstance(x, str) and has_special_chars(x))].index\n",
    "        special_chars_values = df.loc[special_chars_indices, column].tolist()\n",
    "        issue_data = list(zip(special_chars_indices, special_chars_values))\n",
    "\n",
    "        total_issues = len(issue_data)\n",
    "        if issue_data:\n",
    "            if total_issues > 20:\n",
    "                # Keep only the first 10 and last 10 items\n",
    "                first_10 = issue_data[:10]\n",
    "                last_10 = issue_data[-10:]\n",
    "                display_list = first_10 + [('...', '...')] + last_10\n",
    "                issue_data = \", \".join([f\"({idx}, '{value}')\" for idx, value in display_list])\n",
    "                message = f\"{total_issues} Special character(s) at index(es): {issue_data} (displaying only the first and last 10 items)\"\n",
    "            else:\n",
    "                issue_data = \", \".join([f\"({idx}, '{value}')\" for idx, value in issue_data])\n",
    "                message = f\"{total_issues} Special character(s) at index(es): {issue_data}\"\n",
    "\n",
    "            return {\n",
    "                \"issue\": True,\n",
    "                \"error_message\": message,\n",
    "                \"dq_issue\": \"DQI #21 (Use of Special Characters - Consistency)\"\n",
    "            }\n",
    "\n",
    "        return {\"issue\": False}\n",
    "\n",
    "from datetime import datetime\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "07 Check Numerical ge zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error(s) found: \n",
      "DQI #1 (Missing Data - Completeness):\n",
      " 7 Blank/Empty/Null/NaN value(s) at index(es): [(6, None), (8, ''), (9, ' '), (10, 'null'), (12, ''), (13, '    '), (14, \"''\")]\n",
      "\n",
      "DQI #15 (Domain Violation - Accuracy):\n",
      " 1 Negative value(s) at index(es): [(7, -1)]\n",
      "\n",
      "DQI #17 (Wrong Data Type - Consistency):\n",
      " 2 Non-numeric value(s) at index(es): [(3, 'a3'), (5, '?')]\n",
      "\n",
      "Range of values: (-1.0:5.67).\n",
      "Last run on: 2024-04-07 21:40:26\n"
     ]
    }
   ],
   "source": [
    "def check_numerical_ge_zero(df: pd.DataFrame, column: str) -> dict:\n",
    "    \"\"\"\n",
    "    Check if all the values in the specified numerical column are greater than or equal to zero,\n",
    "    and also flag any non-numeric values and blank/empty/null/NaN values.\n",
    "    Additionally, report the range of numeric values.\n",
    "    \"\"\"\n",
    "    # First, handle blank/empty/null/NaN values\n",
    "    result_blank = DataQualityIssues.handle_blank_empty_null_nan(df, column)\n",
    "\n",
    "    # Only proceed with other checks if the value is not blank/empty/null/NaN\n",
    "    df_filtered = df[~df[column].apply(lambda x: pd.isnull(x) or str(x).strip() == '' or str(x).lower() == 'null')]\n",
    "    \n",
    "    result_negative = DataQualityIssues.handle_negative_values(df_filtered, column)\n",
    "    result_non_numeric = DataQualityIssues.handle_non_numeric_values(df_filtered, column)\n",
    "\n",
    "    error_summary_parts = []\n",
    "\n",
    "    if result_blank['issue']:\n",
    "        error_summary_parts.append(result_blank['dq_issue'] + ':\\n ' + result_blank['error_message']+ '\\n')\n",
    "\n",
    "    if result_negative['issue']:\n",
    "        error_summary_parts.append(result_negative['dq_issue'] + ':\\n ' + result_negative['error_message']+ '\\n')\n",
    "\n",
    "    if result_non_numeric['issue']:\n",
    "        error_summary_parts.append(result_non_numeric['dq_issue'] + ':\\n ' + result_non_numeric['error_message']+ '\\n')\n",
    "\n",
    "   # Convert the column to numeric, ignoring errors to leave non-numeric as NaN\n",
    "    numeric_values = pd.to_numeric(df_filtered[column], errors='coerce')\n",
    "\n",
    "    # Filter out non-numeric values to avoid them in min/max calculations\n",
    "    numeric_values_filtered = numeric_values.dropna()\n",
    "\n",
    "    # Calculate the range (min and max) of the numeric values\n",
    "    min_value = numeric_values_filtered.min()\n",
    "    max_value = numeric_values_filtered.max()\n",
    "    \n",
    "    # Count correct values\n",
    "    correct_values_count = df_filtered[df_filtered[column].apply(lambda x: pd.to_numeric(x, errors='coerce') >= 0)].shape[0]\n",
    "\n",
    "    if not error_summary_parts:\n",
    "        range_summary = f\"in the range ({min_value}:{max_value})\" if not numeric_values_filtered.empty else \"No valid numerical values found.\"\n",
    "        return f\"All {correct_values_count} values are numerical and greater or equal to 0 {range_summary}.\"\n",
    "\n",
    "    error_summary = \"Error(s) found: \\n\" + \"\\n\".join(error_summary_parts)\n",
    "    range_summary = f\"Range of values: ({min_value}:{max_value}).\" if not numeric_values_filtered.empty else \"No valid numerical values found.\"\n",
    "\n",
    "\n",
    "    return f\"{error_summary}\\n{range_summary}\"\n",
    "\n",
    "# Test function\n",
    "df_test = pd.DataFrame({'hours-per-week': [1, 2, 3, 'a3', 5, '?', None, -1, '', \" \", 'null', 5.67, np.nan, '    ',  \"''\"]})\n",
    "#df_test = pd.DataFrame({'hours-per-week': [1, 2, 3]})\n",
    "result = check_numerical_ge_zero(df_test, 'hours-per-week')\n",
    "print(result)\n",
    "\n",
    "from datetime import datetime\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "08 Check Numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error(s) found: \n",
      "DQI #1 (Missing Data - Completeness):\n",
      " 5 Blank/Empty/Null/NaN value(s) at index(es): [(6, None), (8, ''), (9, ' '), (10, 'null'), (12, '  ')]\n",
      "\n",
      "DQI #17 (Wrong Data Type - Consistency):\n",
      " 2 Non-numeric value(s) at index(es): [(3, 'a3'), (5, '?')]\n",
      "\n",
      "Range of values: (-1.0:5.67).\n",
      "Last run on: 2024-04-07 21:40:26\n"
     ]
    }
   ],
   "source": [
    "def check_numerical(df: pd.DataFrame, column: str) -> str:\n",
    "    \"\"\"\n",
    "    Check if all the values in the specified numerical column are numerical.\n",
    "    Additionally, report the range of numeric values.\n",
    "    \"\"\"\n",
    "    # First, handle blank/empty/null/NaN values\n",
    "    result_blank = DataQualityIssues.handle_blank_empty_null_nan(df, column)\n",
    "\n",
    "    # Only proceed with non-numeric value checks if the value is not blank/empty/null/NaN\n",
    "    df_filtered = df[~df[column].apply(lambda x: pd.isnull(x) or str(x).strip() == '' or str(x).lower() == 'null')]\n",
    "    result_non_numeric = DataQualityIssues.handle_non_numeric_values(df_filtered, column)\n",
    "\n",
    "    error_summary_parts = []\n",
    "\n",
    "    if result_blank['issue']:\n",
    "        error_summary_parts.append(result_blank['dq_issue'] + ':\\n ' + result_blank['error_message']+ '\\n')\n",
    "\n",
    "    if result_non_numeric['issue']:\n",
    "        error_summary_parts.append(result_non_numeric['dq_issue'] + ':\\n ' + result_non_numeric['error_message'])\n",
    "\n",
    "    # Convert the column to numeric, ignoring errors to leave non-numeric as NaN\n",
    "    numeric_values = pd.to_numeric(df_filtered[column], errors='coerce')\n",
    "\n",
    "    # Filter out non-numeric values to avoid them in min/max calculations\n",
    "    numeric_values_filtered = numeric_values.dropna()\n",
    "\n",
    "    # Calculate the range (min and max) of the numeric values\n",
    "    min_value = numeric_values_filtered.min()\n",
    "    max_value = numeric_values_filtered.max()\n",
    "\n",
    "    # Count correct values\n",
    "    correct_values_count = numeric_values_filtered.notna().sum()\n",
    "\n",
    "    if not error_summary_parts:\n",
    "        range_summary = f\"in the range ({min_value}:{max_value})\" if not numeric_values_filtered.empty else \"No valid numeric values found.\"\n",
    "        return f\"All {correct_values_count} values are numerical {range_summary}.\"\n",
    "\n",
    "    error_summary = \"Error(s) found: \\n\" + \"\\n\".join(error_summary_parts)\n",
    "    range_summary = f\"Range of values: ({min_value}:{max_value}).\" if not numeric_values_filtered.empty else \"No valid numeric values found.\"\n",
    "\n",
    "    return f\"{error_summary}\\n\\n{range_summary}\"\n",
    "\n",
    "# Test function\n",
    "#df_test = pd.DataFrame({'hours': [1, 2, 3, -2, 4.777]})\n",
    "df_test = pd.DataFrame({'hours': [1, 2, 3, 'a3', 5, '?', None, -1, '', \" \", 'null', 5.67, '  ']})\n",
    "result = check_numerical(df_test, 'hours')\n",
    "print(result)\n",
    "\n",
    "from datetime import datetime\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "09 Check Numerical between"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error(s) found: \n",
      "DQI #1 (Missing Data - Completeness):\n",
      " 5 Blank/Empty/Null/NaN value(s) at index(es): [(6, None), (8, ''), (9, ''), (10, 'NULL'), (11, '  ')]\n",
      "\n",
      "DQI #15 (Domain Violation - Accuracy):\n",
      " 2 Value(s) outside range [0, 130] at index(es): [(2, 131), (7, -1)]\n",
      "\n",
      "DQI #17 (Wrong Data Type - Consistency):\n",
      " 2 Non-numeric value(s) at index(es): [(3, 'a3'), (5, '?')]\n",
      "\n",
      "Actual range of values: (-1.0 : 131.0)\n",
      "Last run on: 2024-04-07 21:40:26\n"
     ]
    }
   ],
   "source": [
    "def check_numerical_between(df: pd.DataFrame, column: str, min_value: float, max_value: float) -> str:\n",
    "    \"\"\"\n",
    "    Check if all values in the specified column are numerical and fall within the given range.\n",
    "    Additionally, report the actual range of numeric values.\n",
    "    \"\"\"\n",
    "    # First, handle blank/empty/null/NaN values\n",
    "    result_blank = DataQualityIssues.handle_blank_empty_null_nan(df, column)\n",
    "\n",
    "    # Only proceed with other checks if the value is not blank/empty/null/NaN\n",
    "    df_filtered = df[~df[column].apply(lambda x: pd.isnull(x) or str(x).strip() == '' or str(x).lower() == 'null')]\n",
    "\n",
    "    # Checks for non-numeric and out-of-range values\n",
    "    result_non_numeric = DataQualityIssues.handle_non_numeric_values(df_filtered, column)\n",
    "    result_outside_range = DataQualityIssues.handle_values_outside_range(df_filtered, column, min_value, max_value)\n",
    "\n",
    "    error_summary_parts = []\n",
    "\n",
    "    if result_blank['issue']:\n",
    "        error_summary_parts.append(result_blank['dq_issue'] + ':\\n ' + result_blank['error_message'] + '\\n')\n",
    "\n",
    "    if result_outside_range['issue']:\n",
    "        error_summary_parts.append(result_outside_range['dq_issue'] + ':\\n ' + result_outside_range['error_message'] + '\\n')\n",
    "\n",
    "    if result_non_numeric['issue']:\n",
    "        error_summary_parts.append(result_non_numeric['dq_issue'] + ':\\n ' + result_non_numeric['error_message'] + '\\n')\n",
    "\n",
    "    # Convert the column to numeric, ignoring errors to leave non-numeric as NaN\n",
    "    numeric_values = pd.to_numeric(df_filtered[column], errors='coerce')\n",
    "\n",
    "    # Filter out non-numeric values to avoid them in min/max calculations\n",
    "    numeric_values_filtered = numeric_values.dropna()\n",
    "\n",
    "    # Calculate the range (min and max) of the numeric values\n",
    "    actual_min_value = numeric_values_filtered.min()\n",
    "    actual_max_value = numeric_values_filtered.max()\n",
    "\n",
    "    range_summary = f\"Actual range of values: ({actual_min_value} : {actual_max_value})\" if not numeric_values_filtered.empty else \"No valid numeric values found.\"\n",
    "\n",
    "    if not error_summary_parts:\n",
    "        return f\"All {len(numeric_values_filtered)} values are numerical and valid in the range [{min_value}, {max_value}].\\n{range_summary}\"\n",
    "\n",
    "    error_summary = \"Error(s) found: \\n\" + \"\\n\".join(error_summary_parts)\n",
    "\n",
    "    return f\"{error_summary}\\n{range_summary}\"\n",
    "\n",
    "# Test function\n",
    "df_test = pd.DataFrame({'age': [1, 2, 131, 'a3', 5, '?', None, -1, '', \"\", 'NULL', '  ']})\n",
    "result = check_numerical_between(df_test, 'age', 0, 130)\n",
    "print(result)\n",
    "\n",
    "from datetime import datetime\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 Check if ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error(s) found: \n",
      "DQI #1 (Missing Data - Completeness):\n",
      " 4 Blank/Empty/Null/NaN value(s) at index(es): [(4, None), (7, ''), (8, ' '), (9, 'null')]\n",
      "\n",
      "DQI #9 (Duplicates - Uniqueness):\n",
      " 2 Duplicate value(s) at index(es): [[0, 1], [6, 1]]\n",
      "\n",
      "DQI #15 (Domain Violation - Accuracy):\n",
      " 1 Negative value(s) at index(es): [(5, -1)]\n",
      "\n",
      "DQI #15 (Domain Violation - Accuracy):\n",
      " 1 Floating-point number(s) at index(es): [(10, 5.67)]\n",
      "\n",
      "DQI #19 (Uniqueness Violation - Uniqueness):\n",
      " 1 Uniqueness violation(s) at index(es): [(6, 1)]\n",
      "\n",
      "Alphanumeric range of values: (-1 : 5.67)\n",
      "Last run on: 2024-04-07 21:40:26\n"
     ]
    }
   ],
   "source": [
    "def check_id_attributes(df: pd.DataFrame, column: str) -> str:\n",
    "    \"\"\"\n",
    "    Check if the values in the specified column are suitable for use as a Primary Key (PK).\n",
    "    This function checks for blank, non-numeric, negative values, duplicates, and uniqueness violations.\n",
    "    Additionally, report the range of alphanumeric ID values.\n",
    "    \"\"\"\n",
    "    # Calculate the total number of values in the column\n",
    "    total_values_count = df[column].size\n",
    "\n",
    "    # Handle blank/empty/null/NaN values\n",
    "    result_blank = DataQualityIssues.handle_blank_empty_null_nan(df, column)\n",
    "\n",
    "    # Filter out blank values for further checks\n",
    "    df_filtered = df[~df[column].apply(lambda x: pd.isnull(x) or str(x).strip() == '' or str(x).lower() == 'null')]\n",
    " \n",
    "    # Handle negative values in the filtered DataFrame\n",
    "    result_negative = DataQualityIssues.handle_negative_values(df_filtered, column)\n",
    "\n",
    "    # Check for floating-point numbers in the filtered DataFrame\n",
    "    result_floating_point = DataQualityIssues.handle_floating_point_values(df_filtered, column)\n",
    "\n",
    "    # Check if values are alphanumeric and consistent in length in the filtered DataFrame\n",
    "    result_alphanumeric_consistency = DataQualityIssues.handle_alphanumeric_consistency(df_filtered, column)\n",
    "  \n",
    "    # Handle duplicates and uniqueness violations in the original DataFrame\n",
    "    result_duplicates = DataQualityIssues.handle_duplicates(df, column)\n",
    "    result_uniqueness = DataQualityIssues.handle_uniqueness_violation(df, column)\n",
    "\n",
    "    error_summary_parts = []\n",
    "\n",
    "    # Append results to error summary\n",
    "    if result_blank['issue']:\n",
    "        error_summary_parts.append(result_blank['dq_issue'] + ':\\n ' + result_blank['error_message'] + '\\n')\n",
    "\n",
    "    if result_duplicates['issue']:\n",
    "        error_summary_parts.append(result_duplicates['dq_issue'] + ':\\n ' + result_duplicates['error_message'] + '\\n')\n",
    "\n",
    "    if result_negative['issue']:\n",
    "        error_summary_parts.append(result_negative['dq_issue'] + ':\\n ' + result_negative['error_message'] + '\\n')\n",
    "\n",
    "    if result_floating_point['issue']:\n",
    "        error_summary_parts.append(result_floating_point['dq_issue'] + ':\\n ' + result_floating_point['error_message'] + '\\n')\n",
    "\n",
    "    if result_alphanumeric_consistency['issue']:\n",
    "        error_summary_parts.append(result_alphanumeric_consistency['dq_issue'] + ':\\n ' + result_alphanumeric_consistency['error_message'] + '\\n')\n",
    "\n",
    "    if result_uniqueness['issue']:\n",
    "        error_summary_parts.append(result_uniqueness['dq_issue'] + ':\\n ' + result_uniqueness['error_message'] + '\\n')\n",
    "\n",
    "    # Calculate the alphanumeric range (min and max) of the values\n",
    "    alphanumeric_values = df_filtered[column].dropna().astype(str)\n",
    "    alphanumeric_min_value = alphanumeric_values.min()\n",
    "    alphanumeric_max_value = alphanumeric_values.max()\n",
    "\n",
    "    range_summary = f\"Alphanumeric range of values: ({alphanumeric_min_value} : {alphanumeric_max_value})\" if not alphanumeric_values.empty else \"No valid alphanumeric values found.\"\n",
    "\n",
    "    if not error_summary_parts:\n",
    "        return f\"All {total_values_count} ID values are unique and valid, and thus suitable for use as a Primary Key.\\n{range_summary}\"\n",
    "\n",
    "    error_summary = \"Error(s) found: \\n\" + \"\\n\".join(error_summary_parts)\n",
    "    return f\"{error_summary}\\n{range_summary}\"\n",
    "\n",
    "# Example usage\n",
    "df_test = pd.DataFrame({'id_column': [1, 2, 3, 5, None, -1, 1, '', \" \", 'null', 5.67]})\n",
    "\n",
    "'''df_test = pd.DataFrame({'id_column': [1, 2, 3, 5]})'''\n",
    "\n",
    "'''df_test = pd.DataFrame({\n",
    "    'id_column': ['AB123CD456', 'AB123CD457', 'AB123CD458', 'AB123CD459', 'AB123CD460', \n",
    "                  'AB123CD461', 'AB123CD462', 'AB123CD463', 'AB123CD464', 'AB123CD465', \n",
    "                  'AB123CD466', 'AB123CD467', 'AB123CD468', 'AB123CD469', 'AB123CD470', \n",
    "                  'AB123CD471', 'AB123CD472', 'AB123CD473', 'AB123CD474', 'AB123CD475', \n",
    "                  'Duplicate', 'Duplicate', 'WrongLength1', 'AB123CD456', 'AB123CD479', \n",
    "                  'AB123CD480', 'AB123CD481', 'AB123CD482', 'AB123CD483', 'AB123CD484', \n",
    "                  'AB123CD48', 'AB123CD456', '?','--', 'Aaa']})'''\n",
    "\n",
    "'''df_test = pd.DataFrame({\n",
    "    'id_column': ['AB123CD456', 'AB123CD457', 'AB123CD458', 'AB123CD459', 'AB123CD460', \n",
    "                  'AB123CD461', 'AB123CD462', 'AB123CD463', 'AB123CD464', 'AB123CD465', \n",
    "                  'AB123CD466', 'AB123CD467', 'AB123CD468', 'AB123CD469', 'AB123CD470', \n",
    "                  'AB123CD471', 'AB123CD472', 'AB123CD473']})'''\n",
    "\n",
    "result = check_id_attributes(df_test, 'id_column')\n",
    "print(result)\n",
    "\n",
    "from datetime import datetime\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11 Check String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error(s) found: \n",
      "DQI #1 (Missing Data - Completeness):\n",
      " 4 Blank/Empty/Null/NaN value(s) at index(es): [(1, None), (2, ''), (3, ''), (4, '    ')]\n",
      "\n",
      "DQI #17 (Non-String Data Type - Consistency):\n",
      " 4 Non-string value(s) at index(es): [(5, 123), (6, 5.67), (7, True), (8, {'key': 'value'})]\n",
      "\n",
      "String range (lexicographical): (  Goodbye   : {'key': 'value'})\n",
      "Last run on: 2024-04-07 21:40:26\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def check_string_content(df: pd.DataFrame, column: str) -> str:\n",
    "    \"\"\"\n",
    "    Check if all the values in the specified column are non-empty, non-null strings.\n",
    "    Additionally, report the range of string values based on lexicographical order.\n",
    "    \"\"\"\n",
    "    # Calculate the total number of values in the column\n",
    "    total_values_count = df[column].size\n",
    "\n",
    "    if column not in df.columns:\n",
    "        return f\"Column {column} does not exist in the DataFrame\"\n",
    "    \n",
    "    try:\n",
    "        # First, handle blank/empty/null/NaN values\n",
    "        result_blank = DataQualityIssues.handle_blank_empty_null_nan(df, column)\n",
    "\n",
    "        # Handling special characters and extraneous data\n",
    "        df_filtered = df[~df[column].apply(lambda x: pd.isnull(x) or str(x).strip() == '' or str(x).lower() == 'null')]\n",
    "        result_non_string = DataQualityIssues.handle_non_string_values(df_filtered, column)\n",
    "\n",
    "        error_summary_parts = []\n",
    "        \n",
    "        if result_blank['issue']:\n",
    "            error_summary_parts.append(result_blank['dq_issue'] + ':\\n ' + result_blank['error_message'] + '\\n')\n",
    "        \n",
    "        if result_non_string['issue']:\n",
    "            error_summary_parts.append(result_non_string['dq_issue'] + ':\\n ' + result_non_string['error_message'] + '\\n')\n",
    "\n",
    "        # Calculate the lexicographical range of the string values\n",
    "        string_values = df_filtered[column].dropna().astype(str)\n",
    "        lex_min_value = string_values.min()\n",
    "        lex_max_value = string_values.max()\n",
    "\n",
    "        range_summary = f\"String range (lexicographical): ({lex_min_value} : {lex_max_value})\" if not string_values.empty else \"No valid string values found.\"\n",
    "\n",
    "        if error_summary_parts:\n",
    "            return \"Error(s) found: \\n\" + \"\\n\".join(error_summary_parts) + '\\n' + range_summary\n",
    "        else:\n",
    "            return f\"All {total_values_count} string values are valid.\\n{range_summary}\"\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "# Example usage\n",
    "df_bad_string_data = pd.DataFrame({\n",
    "    'text_column': [\n",
    "        'Hello',          # Valid string\n",
    "        None,             # Null value\n",
    "        np.nan,           # NaN value (also treated as empty)\n",
    "        '',               # Empty string\n",
    "        '    ',           # String with only spaces\n",
    "        123,              # Integer (non-string data type)\n",
    "        5.67,             # Float (non-string data type)\n",
    "        True,             # Boolean (non-string data type)\n",
    "        {'key': 'value'}, # Dictionary (non-string data type)\n",
    "        '  Goodbye  '     # Whitespace-padded string\n",
    "    ]\n",
    "}, columns=['text_column'])\n",
    "\n",
    "'''df_bad_string_data = pd.DataFrame({\n",
    "    'text_column': [\n",
    "        'Hello',          # Valid string\n",
    "        '  Goodbye  '     # Whitespace-padded string\n",
    "    ]\n",
    "}, columns=['text_column'])'''\n",
    "\n",
    "\n",
    "result = check_string_content(df_bad_string_data, 'text_column')\n",
    "print(result)\n",
    "\n",
    "from datetime import datetime\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12 Check if Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error(s) found: \n",
      "DQI #1 (Missing Data - Completeness):\n",
      " 7 Blank/Empty/Null/NaN value(s) at index(es): [(3, ''), (4, 'Null'), (13, None), (14, ' \" \"'), (15, ''), (16, ''), (17, \"''\")]\n",
      "\n",
      "DQI #4 (Ambiguous Data - Accuracy, Consistency):\n",
      " 1 Unacceptable value(s) at index(es): [(12, '?')]\n",
      "Categorical format with 7 unique values:\n",
      "Category  Frequency\n",
      "    bird          2\n",
      "     cat          2\n",
      "     dog          2\n",
      "    fish          2\n",
      "     100          1\n",
      "     200          1\n",
      "       ?          1\n",
      "Last run on: 2024-04-07 21:40:26\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def check_if_categorical(df, column, threshold=100):\n",
    "\n",
    "    # Calculate the total number of values in the column\n",
    "    total_values_count = df[column].size\n",
    "\n",
    "    # Handle blank/empty/null/NaN values\n",
    "    result_blank = DataQualityIssues.handle_blank_empty_null_nan(df, column)\n",
    "\n",
    "    # Extract indices from the result\n",
    "    blank_indices = []\n",
    "    if result_blank[\"issue\"]:\n",
    "        # Assuming the format of the error message remains consistent\n",
    "        blank_indices = re.findall(r\"\\((\\d+),\", result_blank[\"error_message\"])\n",
    "\n",
    "    # Convert indices to integers\n",
    "    blank_indices = [int(idx) for idx in blank_indices]\n",
    "\n",
    "    # Filter out blank/empty/null/NaN values for further analysis\n",
    "    df_filtered = df[~df.index.isin(blank_indices)]\n",
    "\n",
    "    # Exclude the indices/values identified in DQI #1 when checking for DQI #4\n",
    "    df_filtered_for_dqi4 = df_filtered\n",
    "\n",
    "    # Handle predefined unacceptable values\n",
    "    result_unacceptable_values = DataQualityIssues.handle_predefined_unacceptable_values(df_filtered_for_dqi4, column)\n",
    "\n",
    "    # Check for excessive distinct values\n",
    "    result_excessive_values = DataQualityIssues.handle_excessive_distinct_values(df_filtered, column, threshold)\n",
    "\n",
    "    error_summary_parts = []\n",
    "\n",
    "    if result_blank['issue']:\n",
    "        error_summary_parts.append(result_blank['dq_issue'] + ':\\n ' + result_blank['error_message'] + '\\n')\n",
    "\n",
    "    if result_unacceptable_values['issue']:\n",
    "        error_summary_parts.append(result_unacceptable_values['dq_issue'] + ':\\n ' + result_unacceptable_values['error_message'] + '\\n')\n",
    "\n",
    "    if result_excessive_values['issue']:\n",
    "        error_summary_parts.append(result_excessive_values['dq_issue'] + ':\\n ' + result_excessive_values['error_message'] + '\\n')\n",
    "\n",
    "    # Display frequency distribution with a maximum of 20 entries 10 from the beginning and 10 from the end)\n",
    "    frequency_table = df_filtered[column].value_counts(dropna=True).reset_index()\n",
    "    frequency_table.columns = ['Category', 'Frequency']\n",
    "    # Sort first by Frequency (descending) then by Category (alphabetically)\n",
    "    frequency_table = frequency_table.sort_values(by=['Frequency', 'Category'], ascending=[False, True]).reset_index(drop=True)\n",
    "\n",
    "    # Select the first 10 and last 10 rows\n",
    "    top_rows = frequency_table.head(10)\n",
    "    bottom_rows = frequency_table.tail(10)\n",
    "    if len(frequency_table) > 20:\n",
    "        ellipsis_row = pd.DataFrame([(\"...\", \"...\")], columns=['Category', 'Frequency'])\n",
    "        display_table = pd.concat([top_rows, ellipsis_row, bottom_rows], ignore_index=True)\n",
    "    else:\n",
    "        display_table = frequency_table\n",
    "\n",
    "    frequency_distribution_str = f\"Categorical format with {df_filtered[column].nunique(dropna=False)} unique values:\\n{display_table.to_string(index=False)}\"\n",
    "\n",
    "    # If no errors are found, return a message stating that all values are correct along with frequency distribution\n",
    "    if not error_summary_parts:\n",
    "        return f\"All {total_values_count} values are correctly categorical.\\n\\n{frequency_distribution_str}\"\n",
    "\n",
    "    # Combine frequency distribution with error summary (if any)\n",
    "    error_summary = \"Error(s) found: \\n\" + \"\\n\".join(error_summary_parts) if error_summary_parts else \"\"\n",
    "    return error_summary + frequency_distribution_str\n",
    "\n",
    "# Example usage\n",
    "#df_test = pd.DataFrame({'categorical_column': ['cat', 'dog', 'bird']})\n",
    "df_test = pd.DataFrame({'categorical_column': ['cat', 'dog', 'bird', '', 'Null', 100, 200, 'cat', 'dog', 'fish', 'fish', 'bird', '?', None,' \" \"','', \"\", \"''\"]})\n",
    "result = check_if_categorical(df_test, 'categorical_column')\n",
    "print(result)\n",
    "\n",
    "from datetime import datetime\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13 Check Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error(s) found: \n",
      "DQI #1 (Missing Data - Completeness):\n",
      " 1 Blank/Empty/Null/NaN value(s) at index(es): [(11, None)]\n",
      "\n",
      "DQI #15 (Domain Violation - Accuracy):\n",
      " 4 Invalid month value(s) at index(es): [(3, '0'), (4, '13'), (10, 'not a month'), (12, 'mn')]\n",
      "\n",
      "Frequency Distribution:\n",
      "   Month  Frequency\n",
      " January          3\n",
      "February          2\n",
      "December          2\n",
      "   March          1\n",
      "November          1\n",
      "Last run on: 2024-04-07 21:40:26\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def check_month(df, column):\n",
    "    error_summary_parts = []\n",
    "    month_counts = {}\n",
    "\n",
    "    # Calculate the total number of values in the column\n",
    "    total_values_count = df[column].size\n",
    "    \n",
    "    # First, handle blank/empty/null/NaN values\n",
    "    result_blank = DataQualityIssues.handle_blank_empty_null_nan(df, column)\n",
    "\n",
    "    if result_blank['issue']:\n",
    "        error_summary_parts.append(result_blank['dq_issue'] + ':\\n ' + result_blank['error_message'] + '\\n')\n",
    "\n",
    "    # Handling invalid months\n",
    "    df_filtered = df[~df[column].apply(lambda x: pd.isnull(x) or str(x).strip() == '' or str(x).lower() == 'null')]\n",
    "    result_invalid_months = DataQualityIssues.handle_invalid_months(df_filtered, column)\n",
    "\n",
    "    if result_invalid_months['issue']:\n",
    "        error_summary_parts.append(result_invalid_months['dq_issue'] + ':\\n ' + result_invalid_months['error_message'] + '\\n')\n",
    "\n",
    "    # Create frequency distribution for valid months\n",
    "    for val in df_filtered[column].dropna():\n",
    "        normalized_month = DataQualityIssues.normalize_month(str(val))\n",
    "        if normalized_month:\n",
    "            month_counts[normalized_month] = month_counts.get(normalized_month, 0) + 1\n",
    "\n",
    "    # Compile frequency distribution\n",
    "    frequency_table = pd.DataFrame([\n",
    "        (month, month_to_number(month), count) \n",
    "        for month, count in month_counts.items()\n",
    "    ], columns=['Month', 'MonthNum', 'Frequency'])\n",
    "    frequency_table = frequency_table.sort_values(by=['Frequency', 'MonthNum'], ascending=[False, True]).reset_index(drop=True)\n",
    "    frequency_distribution = f\"\\nFrequency Distribution:\\n{frequency_table[['Month', 'Frequency']].to_string(index=False)}\"\n",
    "\n",
    "    # If no errors are found, return a message stating that all values are valid along with frequency distribution\n",
    "    if not error_summary_parts:\n",
    "        return f\"All {total_values_count} month values are valid.\\n{frequency_distribution}\"\n",
    "\n",
    "    # Compile the final result message\n",
    "    error_summary = \"Error(s) found: \\n\" + \"\\n\".join(error_summary_parts) if error_summary_parts else \"\"\n",
    "    return error_summary + frequency_distribution\n",
    "\n",
    "def month_to_number(month_name):\n",
    "    \"\"\"\n",
    "    Convert a month name to its corresponding numeric value.\n",
    "\n",
    "    Parameters:\n",
    "    - month_name (str): The full name of the month.\n",
    "\n",
    "    Returns:\n",
    "    - int: Numeric representation of the month.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return datetime.strptime(month_name, '%B').month\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "#df_test = pd.DataFrame({'month_column': [1, '3', 12, 'Jan', 'January', 'feb', 'NOV', 'Dec','FEBRUARY']})\n",
    "df_test = pd.DataFrame({'month_column': [1, '3', 12, '0', '13', 'Jan', 'January', 'feb', 'NOV', 'Dec', 'not a month', None, 'mn', 'FEBRUARY']})\n",
    "result = check_month(df_test, 'month_column')\n",
    "print(result)\n",
    "\n",
    "from datetime import datetime\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14 Check Weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error(s) found: \n",
      "DQI #1 (Missing Data - Completeness):\n",
      " 1 Blank/Empty/Null/NaN value(s) at index(es): [(8, None)]\n",
      "\n",
      "DQI #15 (Domain Violation - Accuracy):\n",
      " 3 Invalid weekday value(s) at index(es): [(7, 'not a weekday'), (12, -1), (13, 'Mn')]\n",
      "\n",
      "Frequency Distribution:\n",
      "  Weekday  Frequency\n",
      "   Monday          3\n",
      "Wednesday          2\n",
      "   Sunday          2\n",
      "  Tuesday          1\n",
      " Saturday          1\n",
      "Last run on: 2024-04-07 21:40:26\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def normalize_weekday(weekday_str):\n",
    "    \"\"\"\n",
    "    Normalize the given weekday string to its full weekday name if valid.\n",
    "\n",
    "    Parameters:\n",
    "    - weekday_str (str): The weekday string to normalize.\n",
    "\n",
    "    Returns:\n",
    "    - str/None: Normalized weekday name if valid, None otherwise.\n",
    "    \"\"\"\n",
    "    weekdays = [\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\"]\n",
    "    weekday_abbr = [\"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\"]\n",
    "    weekday_abbr_short = [\"su\", \"mo\", \"tu\", \"we\", \"th\", \"fr\", \"sa\"]\n",
    "\n",
    "    weekday_str_norm = weekday_str.lower()\n",
    "\n",
    "    # Normalize full names and abbreviations to full weekday name\n",
    "    if weekday_str_norm.title() in weekdays:\n",
    "        return weekday_str_norm.title()\n",
    "    elif weekday_str_norm.title() in weekday_abbr:\n",
    "        return weekdays[weekday_abbr.index(weekday_str_norm.title())]\n",
    "    elif weekday_str_norm in weekday_abbr_short:\n",
    "        return weekdays[weekday_abbr_short.index(weekday_str_norm)]\n",
    "    elif weekday_str_norm.isdigit():\n",
    "        # Convert numeric representation to weekday name\n",
    "        weekday_num = int(weekday_str_norm)\n",
    "        if 1 <= weekday_num <= 7:\n",
    "            return weekdays[weekday_num - 1]  # Adjusting for 1-indexed weekdays\n",
    "    return None\n",
    "\n",
    "def check_weekday(df, column):\n",
    "    error_summary_parts = []\n",
    "    weekday_counts = {}\n",
    "\n",
    "    # Calculate the total number of values in the column\n",
    "    total_values_count = df[column].size\n",
    "\n",
    "    # First, handle blank/empty/null/NaN values\n",
    "    result_blank = DataQualityIssues.handle_blank_empty_null_nan(df, column)\n",
    "\n",
    "    if result_blank['issue']:\n",
    "        error_summary_parts.append(result_blank['dq_issue'] + ':\\n ' + result_blank['error_message'] + '\\n')\n",
    "\n",
    "    # Handling invalid weekdays\n",
    "    df_filtered = df[~df[column].apply(lambda x: pd.isnull(x) or str(x).strip() == '' or str(x).lower() == 'null')]\n",
    "    result_invalid_weekdays = DataQualityIssues.handle_invalid_weekdays(df_filtered, column)\n",
    "\n",
    "    if result_invalid_weekdays['issue']:\n",
    "        error_summary_parts.append(result_invalid_weekdays['dq_issue'] + ':\\n ' + result_invalid_weekdays['error_message'] + '\\n')\n",
    "\n",
    "    # Create frequency distribution for valid weekdays\n",
    "    for val in df_filtered[column].dropna():\n",
    "        normalized_weekday = normalize_weekday(str(val))\n",
    "        if normalized_weekday:\n",
    "            weekday_counts[normalized_weekday] = weekday_counts.get(normalized_weekday, 0) + 1\n",
    "\n",
    "    # Compile frequency distribution\n",
    "    frequency_table = pd.DataFrame(weekday_counts.items(), columns=['Weekday', 'Frequency'])\n",
    "    frequency_table = frequency_table.sort_values(by='Frequency', ascending=False).reset_index(drop=True)\n",
    "    frequency_distribution = f\"\\nFrequency Distribution:\\n{frequency_table.to_string(index=False)}\"\n",
    "\n",
    "    # If no errors are found, return a message stating that all values are valid along with frequency distribution\n",
    "    if not error_summary_parts:\n",
    "        return f\"All {total_values_count} weekday values are valid.\\n{frequency_distribution}\"\n",
    "\n",
    "    # Compile the final result message\n",
    "    error_summary = \"Error(s) found: \\n\" + \"\\n\".join(error_summary_parts) if error_summary_parts else \"\"\n",
    "    return error_summary + frequency_distribution\n",
    "\n",
    "# Example usage\n",
    "df_test = pd.DataFrame({'weekday_column': ['Monday', 'Tue', 2, 'wed', 'Sunday', 'sun', 0, 'not a weekday', None, 'Mo', 'WED', 7, -1, 'Mn']})\n",
    "#df_test = pd.DataFrame({'weekday_column': ['Monday', 'Tue', 2, 'wed', 'Sunday', 'sun', 'Mo', 'WED', 7, 0]})\n",
    "\n",
    "result = check_weekday(df_test, 'weekday_column')\n",
    "print(result)\n",
    "\n",
    "from datetime import datetime\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15 Check Date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error(s) found: \n",
      "DQI #1 (Missing Data - Completeness):\n",
      " 5 Blank/Empty/Null/NaN value(s) at index(es): [(24, ''), (25, ' '), (27, ''), (28, None), (29, 'Null')]\n",
      "\n",
      "DQI #6 (Outdated Temporal Data - Timeliness):\n",
      " 5 Date value(s) not in [1800-2100] period at index(es): [(32, '3/4/2121'), (33, '14/5/2222'), (34, '01/01/1500'), (35, '31/12/2321'), (36, '2121/12/25')]\n",
      "\n",
      "DQI #13 (Temporal mismatch - Accuracy, Timeliness):\n",
      " 8 Invalid date value(s) at index(es): [(17, '32/01/2021'), (18, '29/02/2021'), (19, '31/11/2021'), (20, '00/01/2021'), (21, '01/00/2021'), (22, '2021/13/01'), (23, 'not a date'), (26, '2021-02-30')]\n",
      "\n",
      "DQI #14 (Different units/representations - Consistency):\n",
      " 6 Date value(s) without format 'DDMMYYYY' in [1800-2100] period at index(es): [(3, '12/31/2021'), (4, '2021/12/25'), (11, '2021/04/7'), (12, '2021/08/15'), (13, '2021/11/03'), (14, '12/30/2021')]\n",
      "\n",
      "Date range: 2021-01-01 to 2021-12-31\n",
      "Last run on: 2024-04-07 21:40:26\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "min_valid_year = 1800\n",
    "max_valid_year = 2100\n",
    "\n",
    "def extract_indices_from_error_message(error_message):\n",
    "    # Regular expression to find tuples in the format (index, 'date')\n",
    "    return [int(m.group(1)) for m in re.finditer(r\"\\((\\d+), '.*?'\\)\", error_message)]\n",
    "\n",
    "def check_date(df, column, sample_size=10):\n",
    "    # Calculate the total number of values in the column\n",
    "    total_values_count = df[column].size\n",
    "\n",
    "    error_summary_parts = []\n",
    "\n",
    "    # Handle blank/empty/null/NaN values\n",
    "    result_blank = DataQualityIssues.handle_blank_empty_null_nan(df, column)\n",
    "    if result_blank['issue']:\n",
    "        error_summary_parts.append(result_blank['dq_issue'] + ':\\n ' + result_blank['error_message'] + '\\n')\n",
    "        \n",
    "    # Filter out blank/empty/null/NaN values\n",
    "    df_filtered = df[~df[column].apply(lambda x: pd.isnull(x) or str(x).strip() == '' or str(x).lower() == 'null')]\n",
    "\n",
    "    # Deduce regional format\n",
    "    date_samples = df_filtered[column].astype(str).str.strip()\n",
    "    date_samples = date_samples[:sample_size]\n",
    "    deduced_date_format, _ = DataQualityIssues.deduce_regional_format(date_samples)\n",
    "    deduced_strftime_format = DataQualityIssues.convert_to_strftime_format(deduced_date_format)\n",
    "\n",
    "    # Handling outdated temporal data\n",
    "    result_outdated = DataQualityIssues.handle_outdated_temporal_data(df_filtered, column, min_valid_year, max_valid_year)\n",
    "    if result_outdated['issue']:\n",
    "        error_summary_parts.append(result_outdated['dq_issue'] + ':\\n ' + result_outdated['error_message'] + '\\n')\n",
    "        outdated_indices = extract_indices_from_error_message(result_outdated['error_message'])\n",
    "        df_filtered = df_filtered[~df_filtered.index.isin(outdated_indices)]\n",
    "\n",
    "    # Handling invalid dates\n",
    "    result_invalid_dates = DataQualityIssues.handle_invalid_dates(df_filtered, column)\n",
    "    if result_invalid_dates['issue']:\n",
    "        error_summary_parts.append(result_invalid_dates['dq_issue'] + ':\\n ' + result_invalid_dates['error_message'] + '\\n')\n",
    "        invalid_indices = extract_indices_from_error_message(result_invalid_dates['error_message'])\n",
    "        df_filtered = df_filtered[~df_filtered.index.isin(invalid_indices )]\n",
    "\n",
    "    # Handling format issues for valid dates\n",
    "    result_format_issues = DataQualityIssues.handle_dates_format(df_filtered, column, deduced_date_format)\n",
    "    if result_format_issues['issue']:\n",
    "        error_summary_parts.append(result_format_issues['dq_issue'] + ':\\n ' + result_format_issues['error_message'] + '\\n')\n",
    "    \n",
    "    # Find the earliest and latest dates without strictly relying on deduced_strftime_format\n",
    "    try:\n",
    "        valid_dates = pd.to_datetime(df_filtered[column], errors='coerce').dropna()\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting dates: {e}\")\n",
    "        valid_dates = pd.Series()\n",
    "\n",
    "    if not valid_dates.empty:\n",
    "        earliest_date = valid_dates.min()\n",
    "        latest_date = valid_dates.max()\n",
    "        # Convert earliest and latest date to a more flexible format for display\n",
    "        earliest_date_str = earliest_date.strftime('%Y-%m-%d')\n",
    "        latest_date_str = latest_date.strftime('%Y-%m-%d')\n",
    "        date_range_summary = f\"Date range: {earliest_date_str} to {latest_date_str}\"\n",
    "    else:\n",
    "        date_range_summary = \"No valid date values found.\"\n",
    "        \n",
    "    # Compile the final result message\n",
    "    if error_summary_parts:\n",
    "        error_summary = \"Error(s) found: \\n\" + \"\\n\".join(error_summary_parts) + '\\n' + date_range_summary\n",
    "    else:\n",
    "        error_summary = f\"All {total_values_count} date values are valid in the {deduced_date_format} format in the range {earliest_date_str} to {latest_date_str}.\"\n",
    "\n",
    "    return error_summary  # Add this line to return the summary\n",
    "\n",
    "\n",
    "# Example usage\n",
    "dates = [\n",
    "    '3/4/2021', '14/5/2021', '01/01/2021', '12/31/2021', '2021/12/25', # Valid dates\n",
    "    '14 January 2021', '1 Feb 2021', '28 Mar 2021', '4 Apr 2021', '15 Oct 2021', '23 Nov 2021', # Textual months\n",
    "    '2021/04/7', '2021/08/15', '2021/11/03', '12/30/2021', '07/04/2021', '11/11/2021', # Various formats\n",
    "    '32/01/2021', '29/02/2021', '31/11/2021', '00/01/2021', '01/00/2021', '2021/13/01', # Invalid dates\n",
    "    'not a date', '', ' ', '2021-02-30', np.nan, None, 'Null', # Non-date and empty strings\n",
    "    '1 Feb 2021','1 February 2021',\n",
    "    '3/4/2121', '14/5/2222', '01/01/1500', '31/12/2321', '2121/12/25', # Dates before 1800 or greater than 2100\n",
    "]\n",
    "\n",
    "'''dates = [\n",
    "'2010-12-01','2010-12-02','2010-12-03','2010-12-14','2010-12-05',\n",
    "'2010-12-06','2010-12-07','2010-12-08','2010-12-09','2010-12-10',\n",
    "'2010-12-11','2010-12-12','2010-12-13','2010-12-14']'''\n",
    "\n",
    "'''dates = [\n",
    "    '01/01/2021', '02/01/2021', '03/01/2021', \n",
    "    '04/01/2021', '05/01/2021', '06/01/2021',\n",
    "    '07/01/2021', '08/01/2021', '09/01/2021', \n",
    "    '10/01/2021', '11/01/2021', '13/01/2021']'''\n",
    "   \n",
    "df_test_dates = pd.DataFrame({'date_column': dates})\n",
    "result = check_date(df_test_dates, 'date_column')\n",
    "print(result)\n",
    "\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16 Check DateTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error(s) found: \n",
      "DQI #1 (Missing Data - Completeness):\n",
      " 5 Blank/Empty/Null/NaN value(s) at index(es): [(27, ''), (28, None), (29, 'Null'), (30, ''), (31, '  ')]\n",
      "\n",
      "DQI #6 (Outdated Temporal Data - Timeliness):\n",
      " 5 Datetime value(s) not in [1800-2100] period at index(es): [(34, '3/4/2121 13:00'), (35, '14/5/2222 13:05'), (36, '01/01/1500 13:00:10'), (37, '31/12/2321 13:20'), (38, '2121/12/25 13:00:12')]\n",
      "\n",
      "DQI #14 (Different units/representations - Consistency):\n",
      " 4 Datetime value(s) without format 'DDMMYYYY' in [1800-2100] period at index(es): [(10, '2021-01-11 23:45'), (12, '2021/01/12 23:00'), (14, 'January 14, 2021 12:00'), (24, '2021/01/23')]\n",
      "\n",
      "DQI #13 (Temporal mismatch - Accuracy, Timeliness):\n",
      " 9 Invalid datetime value(s) at index(es): [(13, '13/01/2021 12:60'), (16, '2021/16/01 14:00'), (18, '18/01/2021 25:00'), (19, '2021-01-19T15:30'), (22, '21/01/2021 16:00:60'), (23, 'not a datetime'), (25, '24/01/2021 26:30'), (32, '29/02/2021 15:20'), (33, '2021-02-30 15:20:05')]\n",
      "\n",
      "Last run on: 2024-04-07 21:40:26\n"
     ]
    }
   ],
   "source": [
    "def extract_indices_from_error_message(error_message):\n",
    "    # Regular expression to find tuples in the format (index, 'value')\n",
    "    return [int(m.group(1)) for m in re.finditer(r\"\\((\\d+), '.*?'\\)\", error_message)]\n",
    "\n",
    "def update_format_counts(standardized_date, format_counts):\n",
    "    if len(standardized_date) == 8:\n",
    "        day, month, year = int(standardized_date[:2]), int(standardized_date[2:4]), int(standardized_date[4:])\n",
    "        if 1 <= day <= 31 and 1 <= month <= 12:\n",
    "            format_counts['DDMMYYYY'] += 1\n",
    "        if 1 <= month <= 12 and 1 <= day <= 31:\n",
    "            format_counts['MMDDYYYY'] += 1\n",
    "        year, month, day = int(standardized_date[:4]), int(standardized_date[4:6]), int(standardized_date[6:])\n",
    "        if min_valid_year <= year <= max_valid_year and 1 <= month <= 12 and 1 <= day <= 31:\n",
    "            format_counts['YYYYMMDD'] += 1\n",
    "    return format_counts\n",
    "\n",
    "def check_datetime(df, column, sample_size=10):\n",
    "    # Calculate the total number of values in the column\n",
    "    total_values_count = df[column].size\n",
    "    error_summary_parts = []\n",
    "\n",
    "    # Handle blank/empty/null/NaN values\n",
    "    result_blank = DataQualityIssues.handle_blank_empty_null_nan(df, column)\n",
    "    if result_blank['issue']:\n",
    "        error_summary_parts.append(result_blank['dq_issue'] + ':\\n ' + result_blank['error_message'] + '\\n')\n",
    "    \n",
    "    # Filter out blank/empty/null/NaN values\n",
    "    df_filtered = df[~df[column].apply(lambda x: pd.isnull(x) or str(x).strip() == '' or str(x).lower() == 'null')]\n",
    "\n",
    "    # Deduce regional format\n",
    "    datetime_samples = df_filtered[column].astype(str).str.strip()\n",
    "    date_samples = [sample.split(' ')[0] for sample in datetime_samples][:sample_size]\n",
    "    deduced_date_format, _ = DataQualityIssues.deduce_regional_format(date_samples)\n",
    "    deduced_strftime_format = DataQualityIssues.convert_to_strftime_format(deduced_date_format) + \" %H:%M\"\n",
    "    \n",
    "    # Handling outdated temporal datetimes\n",
    "    result_outdated = DataQualityIssues.handle_outdated_temporal_data_datetime(df_filtered, column, min_valid_year, max_valid_year)\n",
    "    if result_outdated['issue']:\n",
    "        error_summary_parts.append(result_outdated['dq_issue'] + ':\\n ' + result_outdated['error_message'] + '\\n')\n",
    "        outdated_indices = extract_indices_from_error_message(result_outdated['error_message'])\n",
    "        df_filtered = df_filtered[~df_filtered.index.isin(outdated_indices)]\n",
    "\n",
    "    # Handling format issues for valid dates\n",
    "    result_format_issues = DataQualityIssues.handle_datetimes_format(df_filtered, column, deduced_date_format)\n",
    "    if result_format_issues['issue']:\n",
    "        error_summary_parts.append(result_format_issues['dq_issue'] + ':\\n ' + result_format_issues['error_message'] + '\\n')\n",
    "        invalid_indices = extract_indices_from_error_message(result_format_issues['error_message'])\n",
    "        df_filtered = df_filtered[~df_filtered.index.isin(invalid_indices )]\n",
    "  \n",
    "    # Handle invalid datetime formats\n",
    "    result_invalid_formats = DataQualityIssues.handle_invalid_datetimes(df_filtered, column, deduced_date_format)\n",
    "    if result_invalid_formats['issue']:\n",
    "        error_summary_parts.append(result_invalid_formats['dq_issue'] + ':\\n ' + result_invalid_formats['error_message'] + '\\n')\n",
    "\n",
    "    # Find the earliest and latest datetime values\n",
    "    valid_datetimes = pd.to_datetime(df_filtered[column], errors='coerce', format=deduced_strftime_format).dropna()\n",
    "    earliest_datetime = valid_datetimes.min()\n",
    "    latest_datetime = valid_datetimes.max()\n",
    "\n",
    "    # Convert earliest and latest datetime to the deduced date format\n",
    "    datetime_range_summary = \"\"\n",
    "    if not valid_datetimes.empty:\n",
    "        earliest_datetime_str = earliest_datetime.strftime(deduced_strftime_format)\n",
    "        latest_datetime_str = latest_datetime.strftime(deduced_strftime_format)\n",
    "        datetime_range_summary = f\"Date range: {earliest_datetime_str} to {latest_datetime_str}\\n\"\n",
    "    else:\n",
    "        datetime_range_summary = \"No valid datetime values found.\\n\"\n",
    "\n",
    "   # Compile final result message\n",
    "    error_summary = \"Error(s) found: \\n\" + \"\\n\".join(error_summary_parts) if error_summary_parts else f\"All {total_values_count} datetime values are valid in the {deduced_date_format} format in the range {earliest_datetime_str} to {latest_datetime_str}.\"\n",
    "    return error_summary\n",
    "\n",
    "# Example usage\n",
    "timestamps = [\n",
    "    '01/01/2021 13:00', '02/01/2021 14:30', '03/01/2021 15:45', \n",
    "    '04/01/2021 16:00', '05/01/2021 17:15', '06/01/2021 18:30',\n",
    "    '07/01/2021 19:45', '08/01/2021 20:00', '09/01/2021 21:15', \n",
    "    '10/01/2021 22:30', '2021-01-11 23:45', '11-01-2021 10:00 PM', \n",
    "    '2021/01/12 23:00', '13/01/2021 12:60', 'January 14, 2021 12:00', \n",
    "    '15/01/2021', '2021/16/01 14:00', '17-01-2021', '18/01/2021 25:00', \n",
    "    '2021-01-19T15:30', '20th Jan 2021 16:00', '20 Jan 2021 16:00:10', '21/01/2021 16:00:60', \n",
    "    'not a datetime', '2021/01/23', '24/01/2021 26:30', '14 January 2021 12:00',\n",
    "    np.nan, None, 'Null', '', '  ', '29/02/2021 15:20','2021-02-30 15:20:05',\n",
    "    '3/4/2121 13:00', '14/5/2222 13:05', '01/01/1500 13:00:10', '31/12/2321 13:20', \n",
    "    '2121/12/25 13:00:12' # Dates before 1800 or greater than 2100\n",
    "]\n",
    "\n",
    "'''timestamps = [\n",
    "    '01/01/2021 13:00', '02/01/2021 14:30', '03/01/2021 15:45', \n",
    "    '04/01/2021 16:00', '05/01/2021 17:15', '06/01/2021 18:30',\n",
    "    '07/01/2021 19:45', '08/01/2021 20:00', '09/01/2021 21:15', \n",
    "    '10/01/2021 22:30']'''\n",
    "\n",
    "'''timestamps = ['2010-12-01 08:26:00','2010-12-02 08:26:00','2010-12-03 08:28:00','2010-12-01 08:26:00','2010-12-01 08:26:45','2010-12-01 08:26:50',\n",
    "              '2010-12-01 08:27:00','2010-12-01 08:27:00','2010-12-01 08:26:15','2010-12-01 08:26:00','2010-12-01 08:26:05','2010-12-01 08:26:10',\n",
    "              '2010-12-01 08:26:20','2010-12-01 08:26:06']'''\n",
    "\n",
    "df_test_timestamps = pd.DataFrame({'timestamp_column': timestamps})\n",
    "result = check_datetime(df_test_timestamps, 'timestamp_column')\n",
    "print(result)\n",
    "\n",
    "from datetime import datetime\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17 Check Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error(s) found: \n",
      "DQI #1 (Missing Data - Completeness):\n",
      " 2 Blank/Empty/Null/NaN value(s) at index(es): [(6, ''), (7, None)]\n",
      "\n",
      "DQI #13 (Temporal mismatch - Accuracy, Timeliness):\n",
      " 4 Invalid time value(s) at index(es): [(1, '13:61'), (5, 'invalid'), (8, '02:30 PN'), (9, '25:03')]\n",
      "\n",
      "Time range: (03:05:00 to 14:30:15)\n",
      "\n",
      "Last run on: 2024-04-07 21:40:26\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def parse_time(time_str):\n",
    "    # If the input is already a time object, return it directly\n",
    "    if isinstance(time_str, time):\n",
    "        return time_str\n",
    "\n",
    "    for fmt in ('%H:%M:%S', '%H:%M', '%I:%M %p'):\n",
    "        try:\n",
    "            return datetime.strptime(time_str, fmt).time()\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "        \n",
    "def check_time(df, column):\n",
    "    error_summary_parts = []\n",
    "    \n",
    "    # Calculate the total number of values in the column\n",
    "    total_values_count = df[column].size\n",
    "    \n",
    "    # Handle blank/empty/null/NaN values\n",
    "    result_blank = DataQualityIssues.handle_blank_empty_null_nan(df, column)\n",
    "    if result_blank['issue']:\n",
    "        error_summary_parts.append(result_blank['dq_issue'] + ':\\n ' + result_blank['error_message'] + '\\n')\n",
    "    \n",
    "    # Filter out blank/empty/null/NaN values and create a copy for safe modification\n",
    "    df_filtered = df[~df[column].apply(lambda x: pd.isnull(x) or str(x).strip() == '' or str(x).lower() == 'null')].copy()\n",
    "\n",
    "    # Handle invalid time formats\n",
    "    result_invalid_times = DataQualityIssues.handle_invalid_times(df_filtered, column)\n",
    "    if result_invalid_times['issue']:\n",
    "        error_summary_parts.append(result_invalid_times['dq_issue'] + ':\\n ' + result_invalid_times['error_message'] + '\\n')\n",
    "\n",
    "    # Convert time strings to datetime.time objects\n",
    "    df_filtered['time_converted'] = df_filtered[column].apply(parse_time)\n",
    "\n",
    "    valid_times = df_filtered['time_converted'].dropna()\n",
    "    earliest_time = min(valid_times, default=None)\n",
    "    latest_time = max(valid_times, default=None)\n",
    "\n",
    "    time_range_summary = \"\"\n",
    "    if earliest_time and latest_time:\n",
    "        time_range_summary = f\"\\nTime range: ({earliest_time} to {latest_time})\\n\"\n",
    "    else:\n",
    "        time_range_summary = \"No valid time values found.\\n\"\n",
    "\n",
    "    final_summary = \"\"\n",
    "    if error_summary_parts:\n",
    "        final_summary = \"Error(s) found: \\n\" + \"\\n\".join(error_summary_parts) + time_range_summary\n",
    "    else:\n",
    "        final_summary = f\"All {total_values_count} time values are valid in the range {earliest_time} to {latest_time}.\"\n",
    "\n",
    "    return final_summary\n",
    "\n",
    "# Example usage\n",
    "#df_test = pd.DataFrame({'time_column': ['12:30', '02:30 PM', '14:30:15', '03:05 AM']})\n",
    "df_test = pd.DataFrame({'time_column': ['12:30', '13:61', '02:30 PM', '14:30:15', '03:05 AM', 'invalid', '', None, '02:30 PN', '25:03']})\n",
    "\n",
    "result = check_time(df_test, 'time_column')\n",
    "print(result)\n",
    "\n",
    "from datetime import datetime\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18 Check Model Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error(s) found: \n",
      "DQI #1 (Missing Data - Completeness):\n",
      " 2 Blank/Empty/Null/NaN value(s) at index(es): [(3, ''), (4, '  ')]\n",
      "\n",
      "Frequency Distribution:\n",
      "model_name_column  Frequency\n",
      "                           1\n",
      "                           1\n",
      "               11          1\n",
      "            32/60          1\n",
      "           470v/7          1\n",
      "   50-850-iidn420          1\n",
      "         580-5840          1\n",
      "    90/80-model-3          1\n",
      "                ?          1\n",
      "           vs-100          1\n",
      "\n",
      "Range of Values: ( to vs-100)\n",
      "Last run on: 2024-04-07 21:40:26\n"
     ]
    }
   ],
   "source": [
    "def check_model_name(df, column):\n",
    "    error_summary_parts = []\n",
    "    \n",
    "    total_values_count = df[column].size\n",
    "    result_blank = DataQualityIssues.handle_blank_empty_null_nan(df, column)\n",
    "\n",
    "    df_filtered = df[~df[column].apply(lambda x: pd.isnull(x) or str(x).strip() == '' or str(x).lower() == 'null')]\n",
    "    if result_blank['issue']:\n",
    "        error_summary_parts.append(result_blank['dq_issue'] + ':\\n ' + result_blank['error_message']+ '\\n')\n",
    "\n",
    "    error_summary = \"Error(s) found: \\n\" + \"\\n\".join(error_summary_parts) if error_summary_parts else \"\"\n",
    "\n",
    "    # Frequency distribution calculation\n",
    "    model_counts = df[column].value_counts().to_dict()\n",
    "    frequency_table = pd.DataFrame(model_counts.items(), columns=[column, 'Frequency'])\n",
    "    frequency_table = frequency_table.sort_values(by=['Frequency', column], ascending=[False, True]).reset_index(drop=True)\n",
    "\n",
    "    if len(frequency_table) > 20:\n",
    "        top_rows = frequency_table.head(10)\n",
    "        bottom_rows = frequency_table.tail(10)\n",
    "        ellipsis_row = pd.DataFrame([(\"...\", \"...\")], columns=[column, 'Frequency'])\n",
    "        display_table = pd.concat([top_rows, ellipsis_row, bottom_rows], ignore_index=True)\n",
    "    else:\n",
    "        display_table = frequency_table\n",
    "\n",
    "    frequency_distribution = f\"\\nFrequency Distribution:\\n{display_table.to_string(index=False)}\\n\"\n",
    "\n",
    "    # Get the range of values (smallest and largest)\n",
    "    sorted_df = df.sort_values(by=column)\n",
    "    smallest_model = sorted_df.iloc[0][column]\n",
    "    biggest_model = sorted_df.iloc[-1][column]\n",
    "    range_of_values = f\"\\nRange of Values: ({smallest_model} to {biggest_model})\"\n",
    "\n",
    "    return error_summary + frequency_distribution + range_of_values if error_summary_parts else f\"All {total_values_count} {column} values are valid.\\n{frequency_distribution}{range_of_values}\"\n",
    "\n",
    "# Test the function with your dataframe\n",
    "df_test = pd.DataFrame({\n",
    "    'model_name_column': [\n",
    "        '32/60', '470v/7', 'vs-100', '', '  ', '?',\n",
    "        '90/80-model-3', '11',  '50-850-ii'\n",
    "        'dn420', '580-5840'\n",
    "    ]\n",
    "})\n",
    "result = check_model_name(df_test, 'model_name_column')\n",
    "print(result)\n",
    "\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18.5 Check Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error(s) found: \n",
      "DQI #1 (Missing Data - Completeness):\n",
      " 2 Blank/Empty/Null/NaN value(s) at index(es): [(4, ''), (5, '  ')]\n",
      "\n",
      "DQI #5 (Extraneous Data - Consistency, Uniqueness):\n",
      " 4 Extraneous data value(s) at index(es): [(6, '?'), (7, 'John3 Doe'), (8, 'Emily!'), (9, '11')]\n",
      "\n",
      "DQI #15 (Domain Violation - Accuracy):\n",
      " 1 Capitalization/Format issue(s) at index(es): [(1, 'jane doe')]\n",
      "\n",
      "Frequency Distribution:\n",
      "                              name_column  Frequency\n",
      "                                                   1\n",
      "                                                   1\n",
      "                                       11          1\n",
      "                                        ?          1\n",
      "                               Anne-Marie          1\n",
      "                                   Emily!          1\n",
      "                    Jean Paul Gautier, Jr          1\n",
      "                                 John Doe          1\n",
      "                     John F. Kennedy, Phd          1\n",
      "                         John Newman, PhD          1\n",
      "                         John Williams II          1\n",
      "                                John3 Doe          1\n",
      "JosÃ© Augusto NapoleÃ£o Ferreira dos Santos          1\n",
      "         JoÃ£o Paulo Pereira e Souza Filho          1\n",
      "                                  Madonna          1\n",
      "                              Mary Joe MD          1\n",
      "                                Mr. Smith          1\n",
      "                                 jane doe          1\n",
      "\n",
      "Range of Values: ( to jane doe)\n",
      "Last run on: 2024-04-07 21:40:26\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def check_name(df, column):\n",
    "    error_summary_parts = []\n",
    "    linking_words = {'the', 'and', 'of', 'do', 'da', 'de', 'del', 'dos', 'e', 'md', 'ii', 'iii', 'iv', 'v', 'jr', 'sr', 'phd'}  # Set of lowercase linking words and suffixes\n",
    "        \n",
    "    # Calculate the total number of values in the column\n",
    "    total_values_count = df[column].size\n",
    "\n",
    "    # First, handle blank/empty/null/NaN values\n",
    "    result_blank = DataQualityIssues.handle_blank_empty_null_nan(df, column)\n",
    "\n",
    "    # Check if first 10 values are all lowercase\n",
    "    first_10_lowercase = df[column].head(10).str.islower().all()\n",
    "\n",
    "    # Handling special characters and extraneous data\n",
    "    df_filtered = df[~df[column].apply(lambda x: pd.isnull(x) or str(x).strip() == '' or str(x).lower() == 'null')]\n",
    "    result_extraneous = DataQualityIssues.handle_extraneous_data(df_filtered, column)\n",
    "\n",
    "    if result_blank['issue']:\n",
    "        error_summary_parts.append(result_blank['dq_issue'] + ':\\n ' + result_blank['error_message'] + '\\n')\n",
    "\n",
    "    indices_to_exclude = set()\n",
    "\n",
    "    if result_extraneous['issue']:\n",
    "        # Convert indices to integers (if necessary)\n",
    "        indices_to_exclude.update(result_extraneous.get('indices', []))\n",
    "        #print(\"indices_to_exclude\",indices_to_exclude)\n",
    "        error_summary_parts.append(result_extraneous['dq_issue'] + ':\\n ' + result_extraneous['error_message'] + '\\n')\n",
    "\n",
    "    if indices_to_exclude:\n",
    "        #print(f\"Indices to be excluded: {sorted(indices_to_exclude)}\")    \n",
    "        # Filter out rows with extraneous data\n",
    "        df_filtered = df_filtered.loc[~df_filtered.index.isin(indices_to_exclude)]    \n",
    " \n",
    "    # Perform capitalization format check only if not all first 10 names are lowercase\n",
    "    if not first_10_lowercase:\n",
    "        result_capitalization_format = DataQualityIssues.handle_capitalization_format(df_filtered, column, linking_words)\n",
    "        if result_capitalization_format['issue']:\n",
    "            error_summary_parts.append(result_capitalization_format['dq_issue'] + ':\\n ' + result_capitalization_format['error_message']+ '\\n')\n",
    "    \n",
    "    # Compile the final result message\n",
    "    error_summary = \"Error(s) found: \\n\" + \"\\n\".join(error_summary_parts) if error_summary_parts else \"\"\n",
    "\n",
    "   # Frequency distribution calculation\n",
    "    name_counts = df[column].value_counts().to_dict()\n",
    "    frequency_table = pd.DataFrame(name_counts.items(), columns=[column, 'Frequency'])\n",
    "    frequency_table = frequency_table.sort_values(by=['Frequency', column], ascending=[False, True]).reset_index(drop=True)\n",
    "\n",
    "    if len(frequency_table) > 20:\n",
    "        top_rows = frequency_table.head(10)\n",
    "        bottom_rows = frequency_table.tail(10)\n",
    "        ellipsis_row = pd.DataFrame([(\"...\", \"...\")], columns=[column, 'Frequency'])\n",
    "        display_table = pd.concat([top_rows, ellipsis_row, bottom_rows], ignore_index=True)\n",
    "    else:\n",
    "        display_table = frequency_table\n",
    "\n",
    "    error_summary = \"Error(s) found: \\n\" + \"\\n\".join(error_summary_parts) if error_summary_parts else \"\"\n",
    "    frequency_distribution = f\"\\nFrequency Distribution:\\n{display_table.to_string(index=False)}\\n\"\n",
    "\n",
    "    # Get the range of values (smallest and largest)\n",
    "    sorted_df = df.sort_values(by=column)\n",
    "    smallest_name = sorted_df.iloc[0][column]\n",
    "    biggest_name = sorted_df.iloc[-1][column]\n",
    "    range_of_values = f\"\\nRange of Values: ({smallest_name} to {biggest_name})\"\n",
    "\n",
    "    return error_summary + frequency_distribution + range_of_values if error_summary_parts else f\"All {total_values_count} {column} values are valid.\\n{frequency_distribution}{range_of_values}\"\n",
    "\n",
    "# Test the function\n",
    "df_test = pd.DataFrame({\n",
    "    'name_column': [\n",
    "        'John Doe', 'jane doe', 'Mr. Smith', 'Anne-Marie', '', '  ', '?',\n",
    "        'John3 Doe', 'Emily!', '11', 'Mary Joe MD', 'John Williams II', 'Madonna',\n",
    "        'Jean Paul Gautier, Jr', 'JoÃ£o Paulo Pereira e Souza Filho', 'JosÃ© Augusto NapoleÃ£o Ferreira dos Santos', \n",
    "        'John F. Kennedy, Phd', 'John Newman, PhD'\n",
    "    ]\n",
    "})\n",
    "\n",
    "'''df_test = pd.DataFrame({\n",
    "    'name_column': [\n",
    "        'John Doe','Mr. Smith', 'Anne-Marie', 'Madonna',\n",
    "        'Jean Paul Gautier, Jr', 'JoÃ£o Paulo Pereira e Souza Filho', 'JosÃ© Augusto NapoleÃ£o Ferreira dos Santos', \n",
    "        'John F. Kennedy, Phd', 'John Newman, PhD'\n",
    "    ]\n",
    "})'''\n",
    "result = check_name(df_test, 'name_column')\n",
    "print(result)\n",
    "\n",
    "from datetime import datetime\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "19 Check Street"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error(s) found: \n",
      "DQI #1 (Missing Data - Completeness):\n",
      " 4 Blank/Empty/Null/NaN value(s) at index(es): [(20, None), (21, ''), (22, '  '), (28, 'Null')]\n",
      "\n",
      "DQI #5 (Extraneous Data - Consistency, Uniqueness):\n",
      " 3 Extraneous street data value(s) at index(es): [(23, '?'), (25, 'Emily!'), (26, '11')]\n",
      "\n",
      "DQI #15 (Domain Violation - Accuracy):\n",
      " 2 Capitalization/Format issue(s) at index(es): [(5, 'InvalidStreet'), (15, 'Sunset boulevard')]\n",
      "\n",
      "Last run on: 2024-04-07 21:40:26\n"
     ]
    }
   ],
   "source": [
    "def check_street(df, column):\n",
    "    \"\"\"\n",
    "    Check if street names in the specified column conform to expected standards.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The pandas DataFrame to analyze.\n",
    "    - column (str): The name of the column with street names.\n",
    "\n",
    "    Returns:\n",
    "    - str: A message indicating the result of the street name checks.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the total number of values in the column\n",
    "    total_values_count = df[column].size\n",
    "\n",
    "    error_summary_parts = []\n",
    "    linking_words = {'the', 'and', 'of', 'do', 'da', 'de', 'del', 'e', 'th','rd'}  # Set of lowercase linking words\n",
    "\n",
    "    # First, handle blank/empty/null/NaN values\n",
    "    result_blank = DataQualityIssues.handle_blank_empty_null_nan(df, column)\n",
    "\n",
    "    # Handling special characters and extraneous data\n",
    "    df_filtered = df[~df[column].apply(lambda x: pd.isnull(x) or str(x).strip() == '' or str(x).lower() == 'null')]\n",
    "    result_street_extraneous = DataQualityIssues.handle_street_extraneous_data(df_filtered, column)\n",
    "    result_capitalization_format = DataQualityIssues.handle_capitalization_format(df_filtered, column, linking_words)\n",
    "\n",
    "    if result_blank['issue']:\n",
    "        error_summary_parts.append(result_blank['dq_issue'] + ':\\n ' + result_blank['error_message'] + '\\n')\n",
    "\n",
    "    if result_street_extraneous['issue']:\n",
    "        error_summary_parts.append(result_street_extraneous['dq_issue'] + ':\\n ' + result_street_extraneous['error_message'] + '\\n')\n",
    "\n",
    "    if result_capitalization_format['issue']:\n",
    "        error_summary_parts.append(result_capitalization_format['dq_issue'] + ':\\n ' + result_capitalization_format['error_message']+ '\\n')\n",
    "    \n",
    "    # Compile the final result message\n",
    "    error_summary = \"Error(s) found: \\n\" + \"\\n\".join(error_summary_parts)\n",
    "    \n",
    "    # Sort the DataFrame by the name column\n",
    "    sorted_df = df.sort_values(by=column)\n",
    "\n",
    "    # Get the first and last name after sorting\n",
    "    smallest_name = sorted_df.iloc[0][column]\n",
    "    biggest_name = sorted_df.iloc[-1][column]\n",
    " \n",
    "    return error_summary if error_summary_parts else f\"All {total_values_count} street values are valid in the range ({smallest_name} to {biggest_name}).\\n\"\n",
    "\n",
    "# Test the function with sample data\n",
    "# Sample data for street checks\n",
    "streets = [\n",
    "    '123 Main St', '45 Oxford Road', 'Broadway Ave', '5th Avenue', \n",
    "    'Mt. Everest Street', 'InvalidStreet', '12, Elm Street', '77 Sunset Strip', \n",
    "    '221B Baker Street', 'Elm St.', 'Ocean Drive', 'Park Ave', 'Sesame St', \n",
    "    'Main Street 123', 'Pennsylvania Avenue NW', 'Sunset boulevard', \n",
    "    'Abbey Road', 'Fleet Street', 'Diagon Alley', '15/250 Beaufort St',\n",
    "    None, '', '  ', '?', 'John3 Doe', 'Emily!', '11', 'R. Prof Paulo Roberto Martins, 2', 'Null'\n",
    "]\n",
    "\n",
    "'''streets = [\n",
    "    '123 Main St', '45 Oxford Road', 'Broadway Ave', '5th Avenue', \n",
    "    'Mt. Everest Street', '12, Elm Street', '77 Sunset Strip', \n",
    "    '221B Baker Street', 'Elm St.', 'Ocean Drive', 'Park Ave', 'Sesame St', \n",
    "    'Main Street 123', 'Pennsylvania Avenue NW', \n",
    "    'Abbey Road', 'Fleet Street', 'Diagon Alley', '15/250 Beaufort St',\n",
    "    'R. Prof Paulo Roberto Martins, 2'\n",
    "]'''\n",
    "\n",
    "df_streets = pd.DataFrame({'street': streets})\n",
    "result = check_street(df_streets, 'street')\n",
    "print(result)\n",
    "\n",
    "from datetime import datetime\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20 Check City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error(s) found: \n",
      "DQI #1 (Missing Data - Completeness):\n",
      " 3 Blank/Empty/Null/NaN value(s) at index(es): [(22, None), (23, ''), (24, '  ')]\n",
      "\n",
      "DQI #5 (Extraneous Data - Consistency, Uniqueness):\n",
      " 3 Extraneous data value(s) at index(es): [(25, '?'), (26, 'Dubai!'), (27, 11)]\n",
      "\n",
      "DQI #15 (Domain Violation - Accuracy):\n",
      " 3 Capitalization/Format issue(s) at index(es): [(5, 'los angeles'), (9, 'new delhi'), (11, 'San francisco')]\n",
      "\n",
      "Frequency Distribution (showing top and bottom 10 of 24 categories):\n",
      "               City Frequency\n",
      "              Tokyo         2\n",
      "                 11         1\n",
      "                  ?         1\n",
      "          Amsterdam         1\n",
      "            Beijing         1\n",
      "             Berlin         1\n",
      "             Boston         1\n",
      "              Cairo         1\n",
      "            Chicago         1\n",
      "              Dubai         1\n",
      "                ...       ...\n",
      "           New York         1\n",
      "              Paris         1\n",
      "      San Francisco         1\n",
      "      San francisco         1\n",
      "          Singapore         1\n",
      "             Sydney         1\n",
      "SÃ£o JosÃ© dos Campos         1\n",
      "          SÃ£o Paulo         1\n",
      "        los angeles         1\n",
      "          new delhi         1\n",
      "\n",
      "Last run on: 2024-04-07 21:40:26\n"
     ]
    }
   ],
   "source": [
    "def check_city(df, column):\n",
    "    \"\"\"\n",
    "    Check if city names in the specified column conform to expected standards and provide a frequency distribution.\n",
    "    This includes checks for proper capitalization and invalid characters.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The pandas DataFrame to analyze.\n",
    "    - column (str): The name of the column with city names.\n",
    "\n",
    "    Returns:\n",
    "    - str: A message indicating the result of the city name checks and their frequency distribution.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the total number of values in the column\n",
    "    total_values_count = df[column].size\n",
    "\n",
    "    city_counts = {}\n",
    "    linking_words = {'the', 'and', 'of', 'do', 'da', 'de', 'del', 'e','dos'}  # Set of lowercase linking words\n",
    "\n",
    "    # First, handle blank/empty/null/NaN values\n",
    "    result_blank = DataQualityIssues.handle_blank_empty_null_nan(df, column)\n",
    "\n",
    "    # Handling special characters and extraneous data\n",
    "    df_filtered = df[~df[column].apply(lambda x: pd.isnull(x) or str(x).strip() == '' or str(x).lower() == 'null')]\n",
    "    result_extraneous = DataQualityIssues.handle_extraneous_data(df_filtered, column)\n",
    "    result_capitalization_format = DataQualityIssues.handle_capitalization_format(df_filtered, column, linking_words)\n",
    "\n",
    "    error_summary_parts = []\n",
    "\n",
    "    if result_blank['issue']:\n",
    "        error_summary_parts.append(result_blank['dq_issue'] + ':\\n ' + result_blank['error_message']+ '\\n')\n",
    "\n",
    "    if result_extraneous['issue']:\n",
    "        error_summary_parts.append(result_extraneous['dq_issue'] + ':\\n ' + result_extraneous['error_message']+ '\\n')\n",
    "\n",
    "    if result_capitalization_format['issue']:\n",
    "        error_summary_parts.append(result_capitalization_format['dq_issue'] + ':\\n ' + result_capitalization_format['error_message']+ '\\n')\n",
    "\n",
    "    # Frequency distribution calculation\n",
    "    for idx, city in df_filtered.iterrows():\n",
    "        city_str = str(city[column]).strip()\n",
    "\n",
    "        if city_str not in [item[1] for item in error_summary_parts]:\n",
    "            # Counting occurrences of each city\n",
    "            city_counts[city_str] = city_counts.get(city_str, 0) + 1\n",
    "\n",
    "    # Creating a frequency table sorted first by frequency and then alphabetically\n",
    "    if city_counts:\n",
    "        frequency_table = pd.DataFrame(city_counts.items(), columns=['City', 'Frequency'])\n",
    "        frequency_table = frequency_table.sort_values(by=['Frequency', 'City'], ascending=[False, True]).reset_index(drop=True)\n",
    "        # Select the first 10 and last 10 rows if more than 20 distinct items\n",
    "        top_rows = frequency_table.head(10)\n",
    "        bottom_rows = frequency_table.tail(10)\n",
    "        if len(frequency_table) > 20:\n",
    "            ellipsis_row = pd.DataFrame([(\"...\", \"...\")], columns=['City', 'Frequency'])\n",
    "            display_table = pd.concat([top_rows, ellipsis_row, bottom_rows], ignore_index=True)\n",
    "        else:\n",
    "            display_table = frequency_table\n",
    "\n",
    "        result_str = f\"Frequency Distribution (showing top and bottom 10 of {len(frequency_table)} categories):\\n{display_table.to_string(index=False)}\\n\"\n",
    "\n",
    "        # Compile the final result message\n",
    "        error_summary = ''\n",
    "        if error_summary_parts:\n",
    "            error_summary = \"Error(s) found: \\n\" + \"\\n\".join(error_summary_parts) + \"\\n\"\n",
    "\n",
    "        final_message = error_summary + result_str\n",
    "\n",
    "        # If no errors are found, return a message stating that all values are valid along with frequency distribution\n",
    "        if not error_summary_parts:\n",
    "            final_message = f\"All {total_values_count} city values are valid.\\n{result_str}\"\n",
    "\n",
    "        return final_message\n",
    "\n",
    "# Sample data for city checks\n",
    "cities = [\n",
    "    'New York', 'London', 'Paris', 'Tokyo', 'Tokyo',\n",
    "    'los angeles', 'Sydney', 'Beijing', 'Cairo', 'new delhi', 'San Francisco', \n",
    "    'San francisco', 'Chicago', 'Boston', 'Berlin', 'Amsterdam', \n",
    "    'Hong Kong', 'Singapore', 'Dubai', 'Moscow', 'SÃ£o Paulo','SÃ£o JosÃ© dos Campos',\n",
    "    None, '', '  ', '?', 'Dubai!',  11\n",
    "]\n",
    "\n",
    "'''cities = [\n",
    "    'New York', 'London', 'Paris', 'Tokyo', 'Tokyo',\n",
    "    'Sydney', 'Beijing', 'Cairo', 'San Francisco', \n",
    "    'Chicago', 'Boston', 'Berlin', 'Amsterdam', \n",
    "    'Hong Kong', 'Singapore', 'Dubai', 'Moscow', 'SÃ£o Paulo','SÃ£o JosÃ© dos Campos',\n",
    "    'NY', 'Los Angeles', 'Cairo', 'Brasilia'\n",
    "]'''\n",
    "df_cities = pd.DataFrame({'city': cities})\n",
    "result = check_city(df_cities, 'city')\n",
    "print(result)\n",
    "\n",
    "from datetime import datetime\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21 Check State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error(s) found: \n",
      "DQI #1 (Missing Data - Completeness):\n",
      " 3 Blank/Empty/Null/NaN value(s) at index(es): [(22, None), (23, ''), (24, '  ')]\n",
      "\n",
      "DQI #5 (Extraneous Data - Consistency, Uniqueness):\n",
      " 4 Extraneous data value(s) at index(es): [(0, 'CA2'), (25, '?'), (26, 'California!'), (27, '11')]\n",
      "\n",
      "DQI #15 (Domain Violation - Accuracy):\n",
      " 4 Capitalization/Format issue(s) at index(es): [(10, 'New york'), (21, 'new south wales'), (29, 'new Jersey'), (30, 'n york')]\n",
      "\n",
      "\n",
      "Frequency Distribution (showing top and bottom 10 of 30 categories):\n",
      "            State Frequency\n",
      "            Texas         2\n",
      "               11         1\n",
      "                ?         1\n",
      "           Alaska         1\n",
      "          Arizona         1\n",
      "          Bavaria         1\n",
      "              CA2         1\n",
      "      California!         1\n",
      "         Colorado         1\n",
      "            Delhi         1\n",
      "              ...       ...\n",
      "          Ontario         1\n",
      "           Punjab         1\n",
      "       Queensland         1\n",
      "Rio Grande do Sul         1\n",
      "        SÃ£o Paulo         1\n",
      "         Victoria         1\n",
      "               WA         1\n",
      "           n york         1\n",
      "       new Jersey         1\n",
      "  new south wales         1\n",
      "\n",
      "Last run on: 2024-04-07 21:40:27\n"
     ]
    }
   ],
   "source": [
    "def check_state(df, column):\n",
    "    \"\"\"\n",
    "    Check if state names in the specified column conform to expected standards of capitalization\n",
    "    and provide a frequency distribution, allowing certain lowercase words and abbreviations.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The pandas DataFrame to analyze.\n",
    "    - column (str): The name of the column with state names.\n",
    "\n",
    "    Returns:\n",
    "    - str: A message indicating the result of the state name checks and their frequency distribution.\n",
    "    \"\"\"\n",
    "    incorrect_indices_and_values = []\n",
    "    state_counts = {}\n",
    "    lowercase_exceptions = {\"e\", \"do\", \"dos\", \"da\", \"das\", \"de\",\"dos\"}  # Lowercase exceptions\n",
    "    \n",
    "    # Calculate the total number of values in the column\n",
    "    total_values_count = df[column].size\n",
    "\n",
    "    # First, handle blank/empty/null/NaN values\n",
    "    result_blank = DataQualityIssues.handle_blank_empty_null_nan(df, column)\n",
    "\n",
    "    # Handling special characters and extraneous data\n",
    "    df_filtered = df[~df[column].apply(lambda x: pd.isnull(x) or str(x).strip() == '' or str(x).lower() == 'null')]\n",
    "    result_extraneous = DataQualityIssues.handle_extraneous_data(df_filtered, column)\n",
    "    result_capitalization_format = DataQualityIssues.handle_capitalization_format(df_filtered, column, lowercase_exceptions)\n",
    "\n",
    "    error_summary_parts = []\n",
    "\n",
    "    if result_blank['issue']:\n",
    "        error_summary_parts.append(result_blank['dq_issue'] + ':\\n ' + result_blank['error_message']+ '\\n')\n",
    "\n",
    "    if result_extraneous['issue']:\n",
    "        error_summary_parts.append(result_extraneous['dq_issue'] + ':\\n ' + result_extraneous['error_message']+ '\\n')\n",
    "\n",
    "    if result_capitalization_format['issue']:\n",
    "        error_summary_parts.append(result_capitalization_format['dq_issue'] + ':\\n ' + result_capitalization_format['error_message']+ '\\n')\n",
    "\n",
    "    # Frequency distribution calculation\n",
    "    for idx, state in df.iterrows():\n",
    "        state_str = str(state[column]).strip()\n",
    "\n",
    "        # Skip blank/empty/null/NaN values and incorrect values for frequency calculation\n",
    "        if state_str and state_str not in [item[1] for item in incorrect_indices_and_values]:\n",
    "            # Counting occurrences of each state\n",
    "            state_counts[state_str] = state_counts.get(state_str, 0) + 1\n",
    "\n",
    "    # Creating a frequency table sorted first by frequency and then alphabetically\n",
    "    if state_counts:\n",
    "        frequency_table = pd.DataFrame(state_counts.items(), columns=['State', 'Frequency'])\n",
    "        frequency_table = frequency_table.sort_values(by=['Frequency', 'State'], ascending=[False, True]).reset_index(drop=True)\n",
    "\n",
    "        # Select the first 10 and last 10 rows if more than 20 distinct items\n",
    "        top_rows = frequency_table.head(10)\n",
    "        bottom_rows = frequency_table.tail(10)\n",
    "        if len(frequency_table) > 20:\n",
    "            ellipsis_row = pd.DataFrame([(\"...\", \"...\")], columns=['State', 'Frequency'])\n",
    "            display_table = pd.concat([top_rows, ellipsis_row, bottom_rows], ignore_index=True)\n",
    "        else:\n",
    "            display_table = frequency_table\n",
    "\n",
    "        result_str = f\"\\nFrequency Distribution (showing top and bottom 10 of {len(frequency_table)} categories):\\n{display_table.to_string(index=False)}\\n\"   \n",
    "\n",
    "        # Compile the final result message\n",
    "        error_summary = ''\n",
    "        if error_summary_parts:\n",
    "            error_summary = \"Error(s) found: \\n\" + \"\\n\".join(error_summary_parts) + \"\\n\"\n",
    "\n",
    "        final_message = error_summary + result_str\n",
    "\n",
    "        # If no errors are found, return a message stating that all values are valid along with frequency distribution\n",
    "        if not error_summary_parts:\n",
    "            final_message = f\"All {total_values_count} state values are valid.\\n{result_str}\"\n",
    "\n",
    "        return final_message\n",
    "\n",
    "# Sample data for state checks\n",
    "states = [\n",
    "     'CA2', 'New York', 'Texas', 'FL', 'Texas',\n",
    "    'Nevada', 'WA', 'Queensland', 'Bavaria', 'Delhi', 'New york',\n",
    "    'Illinois', 'Victoria', 'Ontario', 'Colorado', 'Arizona', \n",
    "    'NSW', 'Gauteng', 'Hawaii', 'Alaska', 'Punjab', 'new south wales',\n",
    "    None, '', '  ', '?', 'California!', '11', 'Rio Grande do Sul', 'new Jersey', 'n york', 'N. Dakota', 'SÃ£o Paulo'\n",
    "]\n",
    "\n",
    "'''states = [\n",
    "     'CA', 'New York', 'Texas', 'FL', 'Texas',\n",
    "    'Nevada', 'WA', 'Queensland', 'Bavaria', 'Delhi', 'New York',\n",
    "    'Illinois', 'Victoria', 'Ontario', 'Colorado', 'Arizona', \n",
    "    'NSW', 'Gauteng', 'Hawaii', 'Alaska', 'Punjab', \n",
    "    'Rio Grande do Sul', 'N. Dakota', 'SÃ£o Paulo'\n",
    "]'''\n",
    "\n",
    "df_states = pd.DataFrame({'state': states})\n",
    "result = check_state(df_states, 'state')\n",
    "print(result)\n",
    "\n",
    "from datetime import datetime\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "22 Check Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "Error(s) found: \n",
      "DQI #1 (Missing Data - Completeness):\n",
      " 4 Blank/Empty/Null/NaN value(s) at index(es): [(20, None), (21, ''), (22, '  '), (31, 'Null')]\n",
      "\n",
      "DQI #5 (Extraneous Data - Consistency, Uniqueness):\n",
      " 3 Extraneous data value(s) at index(es): [(23, '?'), (24, 'Canada!'), (25, '11')]\n",
      "\n",
      "DQI #15 (Domain Violation - Accuracy):\n",
      " 3 Capitalization/Format issue(s) at index(es): [(14, 'puerto rico'), (28, 'guatemala'), (29, 'papua New Guinea')]\n",
      "\n",
      "\n",
      "Frequency Distribution (showing top and bottom 10 of 30 categories):\n",
      "                         Country Frequency\n",
      "                       Australia         2\n",
      "                              11         1\n",
      "                               ?         1\n",
      "                              BR         1\n",
      "                          Brazil         1\n",
      "                          Canada         1\n",
      "                         Canada!         1\n",
      "                           China         1\n",
      "                           Egypt         1\n",
      "                          France         1\n",
      "                             ...       ...\n",
      "                             SWE         1\n",
      "Saint Vincent and the Grenadines         1\n",
      "                    South Africa         1\n",
      "    The United States of America         1\n",
      "                 US-Virgin-Isles         1\n",
      "                             USA         1\n",
      "                  United Kingdom         1\n",
      "                       guatemala         1\n",
      "                papua New Guinea         1\n",
      "                     puerto rico         1\n",
      "\n",
      "Last run on: 2024-04-07 21:40:27\n"
     ]
    }
   ],
   "source": [
    "def check_country(df, column):\n",
    "    \"\"\"\n",
    "    Check if country names in the specified column conform to expected standards of capitalization \n",
    "    and provide a frequency distribution.\n",
    "    \"\"\"\n",
    "    incorrect_indices_and_values = []\n",
    "    country_counts = {}\n",
    "    linking_words = {'the', 'and', 'of', 'do', 'da', 'de', 'del', 'e', 'dos','etc'}  # Set of lowercase linking words\n",
    "\n",
    "    # Calculate the total number of values in the column\n",
    "    total_values_count = df[column].size\n",
    "\n",
    "    # First, handle blank/empty/null/NaN values\n",
    "    result_blank = DataQualityIssues.handle_blank_empty_null_nan(df, column)\n",
    "\n",
    "    # Filtering df based on blank checks\n",
    "    indices_to_exclude_blank = [idx for idx, _ in result_blank.get('indices_and_values', [])]\n",
    "    print(indices_to_exclude_blank)\n",
    "    df_filtered = df.drop(indices_to_exclude_blank)\n",
    "\n",
    "    # Handling special characters and extraneous data\n",
    "    #df_filtered = df[~df[column].apply(lambda x: pd.isnull(x) or str(x).strip() == '' or str(x).lower() == 'null')]\n",
    "    result_extraneous = DataQualityIssues.handle_extraneous_data(df_filtered, column)\n",
    "\n",
    "    if result_extraneous['issue']:\n",
    "        indices_to_exclude_extraneous = result_extraneous['indices']  # Directly use the indices\n",
    "        df_filtered = df_filtered.drop(index=indices_to_exclude_extraneous)\n",
    "\n",
    "    result_capitalization_format = DataQualityIssues.handle_capitalization_format_country(df_filtered, column, linking_words)\n",
    "\n",
    "    error_summary_parts = []\n",
    "\n",
    "    if result_blank['issue']:\n",
    "        error_summary_parts.append(result_blank['dq_issue'] + ':\\n ' + result_blank['error_message']+ '\\n')\n",
    "\n",
    "    if result_extraneous['issue']:\n",
    "        error_summary_parts.append(result_extraneous['dq_issue'] + ':\\n ' + result_extraneous['error_message']+ '\\n')\n",
    "\n",
    "    if result_capitalization_format['issue']:\n",
    "        error_summary_parts.append(result_capitalization_format['dq_issue'] + ':\\n ' + result_capitalization_format['error_message']+ '\\n')\n",
    "\n",
    "    # Frequency distribution calculation\n",
    "    for idx, country in df.iterrows():\n",
    "        country_str = str(country[column]).strip()\n",
    "        \n",
    "        # Skip blank/empty/null/NaN values and incorrect values for frequency calculation\n",
    "        if country_str and country_str not in [item[1] for item in incorrect_indices_and_values]:\n",
    "            # Counting occurrences of each country\n",
    "            country_counts[country_str] = country_counts.get(country_str, 0) + 1\n",
    "\n",
    "    # Creating a frequency table sorted first by frequency and then alphabetically\n",
    "    if country_counts:\n",
    "        frequency_table = pd.DataFrame(country_counts.items(), columns=['Country', 'Frequency'])\n",
    "        frequency_table = frequency_table.sort_values(by=['Frequency', 'Country'], ascending=[False, True]).reset_index(drop=True)\n",
    "\n",
    "        # Select the first 10 and last 10 rows if more than 20 distinct items\n",
    "        top_rows = frequency_table.head(10)\n",
    "        bottom_rows = frequency_table.tail(10)\n",
    "        if len(frequency_table) > 20:\n",
    "            ellipsis_row = pd.DataFrame([(\"...\", \"...\")], columns=['Country', 'Frequency'])\n",
    "            display_table = pd.concat([top_rows, ellipsis_row, bottom_rows], ignore_index=True)\n",
    "            result_str = f\"\\nFrequency Distribution (showing top and bottom 10 of {len(frequency_table)} categories):\\n{display_table.to_string(index=False)}\\n\"\n",
    "        else:\n",
    "            result_str = f\"\\nFrequency Distribution:\\n{frequency_table.to_string(index=False)}\\n\"\n",
    "    else:\n",
    "        result_str = \"All country values are valid\"\n",
    "\n",
    "    # Compile the final result message\n",
    "    error_summary = ''\n",
    "    if error_summary_parts:\n",
    "        error_summary = \"Error(s) found: \\n\" + \"\\n\".join(error_summary_parts) + \"\\n\"\n",
    "\n",
    "    final_message = error_summary + result_str\n",
    "\n",
    "    # If no errors are found, return a message stating that all values are valid along with frequency distribution\n",
    "    if not error_summary_parts:\n",
    "        final_message = f\"All {total_values_count} country values are valid.\\n{result_str}\"\n",
    "\n",
    "    return final_message\n",
    "\n",
    "\n",
    "# Test function\n",
    "countries = [\n",
    "    'USA', 'United Kingdom', 'France', 'Japan', 'Australia', \n",
    "    'India', 'Australia', 'China', 'Egypt', 'Canada', \n",
    "    'Germany', 'Brazil', 'South Africa', 'Russia', 'puerto rico', \n",
    "    'ITA', 'SPA', 'BR', 'SWE', 'Papua New Guinea',\n",
    "    None, '', '  ', '?', 'Canada!', '11', 'The United States of America', 'Outlying-US (Guam-USVI-etc)',\n",
    "    'guatemala', 'papua New Guinea', 'Saint Vincent and the Grenadines', 'Null', 'US-Virgin-Isles'\n",
    "]\n",
    "\n",
    "'''\"AnalysedColumns 2202.xlsx\"countries = [\n",
    "    'USA', 'United Kingdom', 'France', 'Japan', 'Australia', \n",
    "    'India', 'Australia', 'China', 'Egypt', 'Canada', \n",
    "    'Germany', 'Brazil', 'South Africa', 'Russia', 'Puerto Rico', \n",
    "    'ITA', 'SPA', 'BR', 'SWE', 'Papua New Guinea',\n",
    "    'Canada', 'The United States of America',\n",
    "    'Guatemala', 'Papua New Guinea', 'Saint Vincent and the Grenadines', 'US-Virgin-Isles','?'\n",
    "]'''\n",
    "\n",
    "df_countries = pd.DataFrame({'country': countries})\n",
    "result = check_country(df_countries, 'country')\n",
    "print(result)\n",
    "\n",
    "from datetime import datetime\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "23 Check Postal Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 21 postal codes values are valid in the range (00184 to SW1A 1AA).\n",
      "\n",
      "Last run on: 2024-04-07 21:40:27\n"
     ]
    }
   ],
   "source": [
    "def check_postal_code(df: pd.DataFrame, column: str) -> dict:\n",
    "    \"\"\"\n",
    "    Check if postal code entries in the specified column are valid.\n",
    "    \"\"\"\n",
    "    # Calculate the total number of values in the column\n",
    "    total_values_count = df[column].size\n",
    "\n",
    "    # First, handle blank/empty/null/NaN values\n",
    "    result_blank = DataQualityIssues.handle_blank_empty_null_nan(df, column)\n",
    "\n",
    "    # Only proceed with other checks if the value is not blank/empty/null/NaN\n",
    "    df_filtered = df[~df[column].apply(lambda x: pd.isnull(x) or str(x).strip() == '' or str(x).lower() == 'null')]\n",
    "    \n",
    "    result_non_alphanumeric = DataQualityIssues.handle_non_alphanumeric_values(df_filtered, column)\n",
    "    result_short_length = DataQualityIssues.handle_short_length_values(df_filtered, column, 4)\n",
    "\n",
    "    error_summary_parts = []\n",
    "\n",
    "    if result_blank['issue']:\n",
    "        error_summary_parts.append(result_blank['dq_issue'] + ':\\n ' + result_blank['error_message']+ '\\n')\n",
    "\n",
    "    if result_short_length['issue']:\n",
    "        error_summary_parts.append(result_short_length['dq_issue'] + ':\\n ' + result_short_length['error_message']+ '\\n')\n",
    "        \n",
    "    if result_non_alphanumeric['issue']:\n",
    "        error_summary_parts.append(result_non_alphanumeric['dq_issue'] + ':\\n ' + result_non_alphanumeric['error_message']+ '\\n')\n",
    " \n",
    "    error_summary = \"Error(s) found: \\n\" + \"\\n\".join(error_summary_parts)  \n",
    "    # Sort the DataFrame by the name column\n",
    "    sorted_df = df.sort_values(by=column)\n",
    "\n",
    "    # Get the first and last name after sorting\n",
    "    smallest_name = sorted_df.iloc[0][column]\n",
    "    biggest_name = sorted_df.iloc[-1][column]\n",
    " \n",
    "    return error_summary if error_summary_parts else f\"All {total_values_count} postal codes values are valid in the range ({smallest_name} to {biggest_name}).\\n\"\n",
    "\n",
    "# Sample data for postal code checks, including edge cases\n",
    "'''postal_codes = [\n",
    "    '10001', 'SW1A 1AA', '75008', '100-0001', \n",
    "    '110001', '2000', '100000', '11511', 'M4W 1A8', \n",
    "    '10115', '01311', '2001', '101000', '06500', \n",
    "    '00184', '28013', '1012 WX', '111 20', '0101', \n",
    "    '71676-110', '6000', None, '', '  ', '?', '1000!', '11'\n",
    "]'''\n",
    "\n",
    "postal_codes = [\n",
    "    '10001', 'SW1A 1AA', '75008', '100-0001', \n",
    "    '110001', '2000', '100000', '11511', 'M4W 1A8', \n",
    "    '10115', '01311', '2001', '101000', '06500', \n",
    "    '00184', '28013', '1012 WX', '111 20', '0101', \n",
    "    '71676-110', '6000'\n",
    "]\n",
    "\n",
    "df_postal_codes = pd.DataFrame({'postal_code': postal_codes})\n",
    "\n",
    "# Test the function\n",
    "result = check_postal_code(df_postal_codes, 'postal_code')\n",
    "print(result)\n",
    "\n",
    "from datetime import datetime\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "24 Check Phone Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 17 telephone numbers are valid in the range ((123) 456-7890 to 4144494331).\n",
      "\n",
      "Last run on: 2024-04-07 21:40:27\n"
     ]
    }
   ],
   "source": [
    "def check_phone_numbers(df, column):\n",
    "    \"\"\"\n",
    "    Check if phone number entries in the specified column are valid.\n",
    "    \"\"\"\n",
    "    # Calculate the total number of values in the column\n",
    "    total_values_count = df[column].size\n",
    "\n",
    "    # First, handle blank/empty/null/NaN values\n",
    "    result_blank = DataQualityIssues.handle_blank_empty_null_nan(df, column)\n",
    "\n",
    "    # Only proceed with other checks if the value is not blank/empty/null/NaN\n",
    "    df_filtered = df[~df[column].apply(lambda x: pd.isnull(x) or str(x).strip() == '' or str(x).lower() == 'null')]\n",
    "    result_format = DataQualityIssues.handle_phone_number_format(df_filtered, column)\n",
    "\n",
    "    error_summary_parts = []\n",
    "\n",
    "    if result_blank['issue']:\n",
    "        error_summary_parts.append(result_blank['dq_issue'] + ':\\n ' + result_blank['error_message']+ '\\n')\n",
    "\n",
    "    if result_format['issue']:\n",
    "        error_summary_parts.append(result_format['dq_issue'] + ':\\n ' + result_format['error_message']+ '\\n')\n",
    "      \n",
    "    error_summary = \"Error(s) found: \\n\" + \"\\n\".join(error_summary_parts)\n",
    "\n",
    "    # Sort the DataFrame by the name column\n",
    "    sorted_df = df.sort_values(by=column)\n",
    "\n",
    "    # Get the first and last name after sorting\n",
    "    smallest_name = sorted_df.iloc[0][column]\n",
    "    biggest_name = sorted_df.iloc[-1][column]\n",
    " \n",
    "    return error_summary if error_summary_parts else f\"All {total_values_count} telephone numbers are valid in the range ({smallest_name} to {biggest_name}).\\n\"\n",
    "    \n",
    "# Test data\n",
    "'''df_test = pd.DataFrame({\n",
    "    'phone_numbers': [\n",
    "        '123-456-7890', '(123) 456-7890', '+1 123 456 7890', 'InvalidNumber', \n",
    "        '+55 21 11 3415 1515', '04148991268624', '+55 48 3224-4209', '+55 48 91268-624', \n",
    "        '000', '+61137425', '+1 414-690-7935', '04121993720444', '01188335944', \n",
    "        '4144494331', '+55 31 3414-2179', '+61 405 833 952', '0405 833 952',\n",
    "        None, '', '  ', '?', 'John Doe', '0405 833 952!', 11\n",
    "    ]\n",
    "})'''\n",
    "\n",
    "df_test = pd.DataFrame({\n",
    "    'phone_numbers': [\n",
    "        '123-456-7890', '(123) 456-7890', '+1 123 456 7890', \n",
    "        '+55 21 11 3415 1515', '04148991268624', '+55 48 3224-4209', '+55 48 91268-624', \n",
    "        '000', '+61137425', '+1 414-690-7935', '04121993720444', '01188335944', \n",
    "        '4144494331', '+55 31 3414-2179', '+61 405 833 952', '0405 833 952',\n",
    "        '0405 833 952'\n",
    "    ]\n",
    "})\n",
    "\n",
    "result = check_phone_numbers(df_test, 'phone_numbers')\n",
    "print(result)\n",
    "\n",
    "from datetime import datetime\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "25 Check IP format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 3 IP values are valid.\n",
      "Last run on: 2024-04-07 21:40:27\n"
     ]
    }
   ],
   "source": [
    "def check_ip_format(df, column):\n",
    "\n",
    "    # Calculate the total number of values in the column\n",
    "    total_values_count = df[column].size\n",
    "\n",
    "    # Handle blank/empty/null/NaN values first\n",
    "    result_blank = DataQualityIssues.handle_blank_empty_null_nan(df, column)\n",
    "    df_filtered = df[~df[column].apply(lambda x: pd.isnull(x) or str(x).strip() == '' or str(x).lower() == 'null')]\n",
    "\n",
    "    # Then check IP format\n",
    "    result_ip_format = DataQualityIssues.handle_ip_format(df_filtered, column)\n",
    "\n",
    "    error_summary_parts = []\n",
    "\n",
    "    if result_blank['issue']:\n",
    "        error_summary_parts.append(result_blank['dq_issue'] + ':\\n ' + result_blank['error_message'] + '\\n')\n",
    "\n",
    "    if result_ip_format['issue']:\n",
    "        error_summary_parts.append(result_ip_format['dq_issue'] + ':\\n ' + result_ip_format['error_message'] + '\\n')\n",
    "\n",
    "    return \"Error(s) found: \\n\" + \"\\n\".join(error_summary_parts) if error_summary_parts else f\"All {total_values_count} IP values are valid.\"\n",
    "\n",
    "# Example usage\n",
    "'''df_test = pd.DataFrame({\n",
    "    'ip_column': [\n",
    "        '192.168.1.1', '256.256.256.256', '127.0.0.1', '1.1', \n",
    "        '2001:0db8:85a3:0000:0000:8a2e:0370:7334', '::1', \n",
    "        '2001:db8::1234:5678', 'fe80::1ff:fe23:4567:890a', \n",
    "        None, '', '  ', '?', '0.0.0.0.0!', '11', 'incorrect:ipv6:address'\n",
    "    ]\n",
    "})'''\n",
    "\n",
    "df_test = pd.DataFrame({\n",
    "    'ip_column': [\n",
    "        '192.168.1.1', '256.256.256.256', '127.0.0.1'\n",
    "    ]\n",
    "})\n",
    "\n",
    "result = check_ip_format(df_test, 'ip_column')\n",
    "print(result)\n",
    "\n",
    "from datetime import datetime\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "26 Check URL format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 6 URL values are valid.\n",
      "Last run on: 2024-04-07 21:40:27\n"
     ]
    }
   ],
   "source": [
    "def check_url_format(df, column):\n",
    "    # Calculate the total number of values in the column\n",
    "    total_values_count = df[column].size\n",
    "    \n",
    "    # Handle blank/empty/null/NaN values first\n",
    "    result_blank = DataQualityIssues.handle_blank_empty_null_nan(df, column)\n",
    "    df_filtered = df[~df[column].apply(lambda x: pd.isnull(x) or str(x).strip() == '' or str(x).lower() == 'null')]\n",
    "\n",
    "    # Then check URL format\n",
    "    result_url_format = DataQualityIssues.handle_url_format(df_filtered, column)\n",
    "\n",
    "    error_summary_parts = []\n",
    "\n",
    "    if result_blank['issue']:\n",
    "        error_summary_parts.append(result_blank['dq_issue'] + ':\\n ' + result_blank['error_message'] + '\\n')\n",
    "\n",
    "    if result_url_format['issue']:\n",
    "        error_summary_parts.append(result_url_format['dq_issue'] + ':\\n ' + result_url_format['error_message'] + '\\n')\n",
    "\n",
    "    return \"Error(s) found: \\n\" + \"\\n\".join(error_summary_parts) if error_summary_parts else f\"All {total_values_count} URL values are valid.\"\n",
    "\n",
    "'''test_urls = [\n",
    "    'https://www.example.com', 'http://example.org', 'http://192.168.1.1', 'http://localhost/test',\n",
    "    'https://www.example.com:8080/path/to/resource', 'ftp://example.com', 'http://exa_mple.com',\n",
    "    'http://999.999.999.999', 'https://', 'http://', 'http://exam!ple.com', 'https://www.example..com',\n",
    "    'http:// example .com', 'justsometext', '12345', '', '  ', None, 'null',\n",
    "    'https://chat.openai.com/c/9c317ba2-cefe-44b9-b9f4-7ef818744434',\n",
    "    'https:--www.uol.com.br', 'https://www.uol.com.br', 'https:///www.uol.com.br'\n",
    "]'''\n",
    "\n",
    "test_urls = [\n",
    "    'https://www.example.com', 'http://example.org', 'http://192.168.1.1', 'http://localhost/test',\n",
    "    'https://www.example.com:8080/path/to/resource', 'https://www.uol.com.br'\n",
    "]\n",
    "\n",
    "# Example usage\n",
    "df_test_urls = pd.DataFrame({'url_column': test_urls})\n",
    "result = check_url_format(df_test_urls, 'url_column')\n",
    "print(result)\n",
    "\n",
    "from datetime import datetime\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "27 Check Email Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 3 email values are valid.\n",
      "Last run on: 2024-04-07 21:40:27\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def check_email_format(df, column):\n",
    "    # Calculate the total number of values in the column\n",
    "    total_values_count = df[column].size\n",
    "\n",
    "    # Handle blank/empty/null/NaN values first\n",
    "    result_blank = DataQualityIssues.handle_blank_empty_null_nan(df, column)\n",
    "    df_filtered = df[~df[column].apply(lambda x: pd.isnull(x) or str(x).strip() == '' or str(x).lower() == 'null')]\n",
    "\n",
    "    # Then check email format\n",
    "    result_email_format = DataQualityIssues.handle_email_format(df_filtered, column)\n",
    "\n",
    "    error_summary_parts = []\n",
    "\n",
    "    if result_blank['issue']:\n",
    "        error_summary_parts.append(result_blank['dq_issue'] + ':\\n ' + result_blank['error_message'] + '\\n')\n",
    "\n",
    "    if result_email_format['issue']:\n",
    "        error_summary_parts.append(result_email_format['dq_issue'] + ':\\n ' + result_email_format['error_message'] + '\\n')\n",
    "\n",
    "    return \"Error(s) found: \\n\" + \"\\n\".join(error_summary_parts) if error_summary_parts else f\"All {total_values_count} email values are valid.\"\n",
    "\n",
    "# Example usage\n",
    "\n",
    "'''df_test = pd.DataFrame({\n",
    "    'email_column': [\n",
    "        'example.com', 'userexample.com', 'name.domain.com',  # Missing @ Symbol\n",
    "        'user@.com', 'name@',  # Missing Domain\n",
    "        'user name@example.com', 'user@ exam ple.com', 'user @example.com',  # Spaces in Email Address\n",
    "        'user!@example.com', 'name#domain.com', 'user*name@example.com',  # Special Characters\n",
    "        'user@@example.com', 'name@domain@domain.com',  # Multiple @ Symbols\n",
    "        '@example.com', '@domain.com',  # Missing Username\n",
    "        'user@example.c', 'name@domain.',  # Domain Extension Too Short or Missing\n",
    "        'user..name@example.com', 'user@domain..com',  # Consecutive Dots\n",
    "        'user@-example.com', 'user@domain--name.com',  # Dashes in Domain\n",
    "        'user@[192.168.0.1]', 'name@[123.123.123.123]',  # IP Address in Domain\n",
    "        'user[name]@example.com', 'name[123]@domain.com',  # Brackets in Local Part\n",
    "        'a'*255 + '@example.com',  # Too Long Email Address\n",
    "        '', '  ', None, 'null'\n",
    "    ]\n",
    "})'''\n",
    "\n",
    "df_test = pd.DataFrame({\n",
    "    'email_column': [\n",
    "        'marcelo.valentimsilva@postgrad.curtin.edu.au', 'marcelovalentimsilva@gmail.com', 'marcelo_valentim@uol.com.br'\n",
    "    ]\n",
    "})\n",
    "\n",
    "result = check_email_format(df_test, 'email_column')\n",
    "print(result)\n",
    "\n",
    "from datetime import datetime\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "28 Check Binary Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 12 binary values are valid.\n",
      "\n",
      "Frequency Distribution:\n",
      "Value  Frequency\n",
      "   no          2\n",
      "    0          1\n",
      "    0          1\n",
      "    1          1\n",
      "False          1\n",
      "    T          1\n",
      "    Y          1\n",
      "  YES          1\n",
      "  Yes          1\n",
      " true          1\n",
      "    y          1\n",
      "\n",
      "Last run on: 2024-04-07 21:40:27\n"
     ]
    }
   ],
   "source": [
    "def check_binary_values(df, column):\n",
    "    \"\"\"\n",
    "    Check if the values in the specified column conform to binary values and provide a frequency distribution.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The pandas DataFrame to analyze.\n",
    "    - column (str): The name of the column with binary values.\n",
    "\n",
    "    Returns:\n",
    "    - str: A message indicating the result of the binary value checks.\n",
    "    \"\"\"\n",
    "    # Calculate the total number of values in the column\n",
    "    total_values_count = df[column].size\n",
    "    \n",
    "    # Handle blank/empty/null/NaN values first\n",
    "    result_blank = DataQualityIssues.handle_blank_empty_null_nan(df, column)\n",
    "    df_filtered = df[~df[column].apply(lambda x: pd.isnull(x) or str(x).strip() == '' or str(x).lower() == 'null')]\n",
    "\n",
    "    # Then check binary format\n",
    "    result_binary_format = DataQualityIssues.handle_binary_values(df_filtered, column)\n",
    "\n",
    "    error_summary_parts = []\n",
    "\n",
    "    if result_blank['issue']:\n",
    "        error_summary_parts.append(result_blank['dq_issue'] + ':\\n ' + result_blank['error_message'] + '\\n')\n",
    "\n",
    "    if result_binary_format['issue']:\n",
    "        error_summary_parts.append(result_binary_format['dq_issue'] + ':\\n ' + result_binary_format['error_message'] + '\\n')\n",
    "\n",
    "    # Frequency distribution calculation\n",
    "    binary_counts = df_filtered[column].value_counts(dropna=False).reset_index()\n",
    "    binary_counts.columns = ['Value', 'Frequency']\n",
    "    # Sort by frequency and then alphabetically\n",
    "    binary_counts = binary_counts.sort_values(by=['Frequency', 'Value'], ascending=[False, True])\n",
    "    frequency_distribution_str = f\"\\nFrequency Distribution:\\n{binary_counts.to_string(index=False)}\\n\"\n",
    "\n",
    "    return (\"Error(s) found: \\n\" + \"\\n\".join(error_summary_parts) if error_summary_parts else f\"All {total_values_count} binary values are valid.\") + '\\n'+ frequency_distribution_str\n",
    "\n",
    "\n",
    "'''df_test = pd.DataFrame({\n",
    "    'binary_column': ['1', '0', 'Yes', 'YES', 'y', 'no', 'true', 'False', 'Invalid', '2', 'Y', 0, 3, None, '', '?','   ',\"T\", 0.1, '-2', 'no']\n",
    "})'''\n",
    "\n",
    "df_test = pd.DataFrame({\n",
    "    'binary_column': ['1', '0', 'Yes', 'YES', 'y', 'no', 'true', 'False', 'Y', 0, \"T\", 'no']\n",
    "})\n",
    "\n",
    "result = check_binary_values(df_test, 'binary_column')\n",
    "print(result)\n",
    "\n",
    "from datetime import datetime\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "29 Analyse Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of Analysis on: 2024-04-07 21:40:27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instant:\n",
      "  Numerical >=0 format: All 17379 values are numerical and greater or equal to 0 in the range (1:17379).\n",
      "\n",
      "dteday (date):\n",
      "  Date format: All 17379 date values are valid in the YYYYMMDD format in the range 2011-01-01 to 2012-12-31.\n",
      "\n",
      "season:\n",
      "  All 17379 values are correctly categorical.\n",
      "\n",
      "Categorical format with 4 unique values:\n",
      " Category  Frequency\n",
      "        3       4496\n",
      "        2       4409\n",
      "        1       4242\n",
      "        4       4232\n",
      "\n",
      "yr (nominal):\n",
      "  All 17379 values are correctly categorical.\n",
      "\n",
      "Categorical format with 2 unique values:\n",
      " Category  Frequency\n",
      "        1       8734\n",
      "        0       8645\n",
      "\n",
      "mnth (month):\n",
      "  Month format: All 17379 month values are valid.\n",
      "\n",
      "Frequency Distribution:\n",
      "    Month  Frequency\n",
      "      May       1488\n",
      "     July       1488\n",
      " December       1483\n",
      "   August       1475\n",
      "    March       1473\n",
      "  October       1451\n",
      "     June       1440\n",
      "    April       1437\n",
      "September       1437\n",
      " November       1437\n",
      "  January       1429\n",
      " February       1341\n",
      "\n",
      "hr (hour):\n",
      "  Numerical (between 0 and 24) format: All 17379 values are numerical and valid in the range [0, 24].\n",
      "Actual range of values: (0 : 23)\n",
      "\n",
      "holiday ( or not):\n",
      "  Binary format: All 17379 binary values are valid.\n",
      "\n",
      "Frequency Distribution:\n",
      " Value  Frequency\n",
      "     0      16879\n",
      "     1        500\n",
      "\n",
      "\n",
      "weekday:\n",
      "  Weekday format: All 17379 weekday values are valid.\n",
      "\n",
      "Frequency Distribution:\n",
      "  Weekday  Frequency\n",
      "   Friday       2512\n",
      " Thursday       2487\n",
      "   Sunday       2479\n",
      "  Tuesday       2475\n",
      "Wednesday       2471\n",
      "   Monday       2453\n",
      "\n",
      "workingday (neither):\n",
      "  Binary format: All 17379 binary values are valid.\n",
      "\n",
      "Frequency Distribution:\n",
      " Value  Frequency\n",
      "     1      11865\n",
      "     0       5514\n",
      "\n",
      "\n",
      "weathersit:\n",
      "  All 17379 values are correctly categorical.\n",
      "\n",
      "Categorical format with 4 unique values:\n",
      " Category  Frequency\n",
      "        1      11413\n",
      "        2       4544\n",
      "        3       1419\n",
      "        4          3\n",
      "\n",
      "temp (normalized):\n",
      "  Normalized format: All 17379 values are numerical and valid in the range [0, 1].\n",
      "Actual range of values: (0.02 : 1.0)\n",
      "\n",
      "atemp (normalized):\n",
      "  Normalized format: All 17379 values are numerical and valid in the range [0, 1].\n",
      "Actual range of values: (0.0 : 1.0)\n",
      "\n",
      "hum (normalized):\n",
      "  Normalized format: All 17379 values are numerical and valid in the range [0, 1].\n",
      "Actual range of values: (0.0 : 1.0)\n",
      "\n",
      "windspeed (normalized):\n",
      "  Normalized format: All 17379 values are numerical and valid in the range [0, 1].\n",
      "Actual range of values: (0.0 : 0.8507)\n",
      "\n",
      "casual (count):\n",
      "  Numerical >=0 format: All 17379 values are numerical and greater or equal to 0 in the range (0:367).\n",
      "\n",
      "registered (count):\n",
      "  Numerical >=0 format: All 17379 values are numerical and greater or equal to 0 in the range (0:886).\n",
      "\n",
      "cnt (count):\n",
      "  Numerical >=0 format: All 17379 values are numerical and greater or equal to 0 in the range (1:977).\n",
      "\n",
      "Last run on: 2024-04-07 21:40:32\n"
     ]
    }
   ],
   "source": [
    "def analyse_data_quality(df, analysed_columns_df, desired_dataset_index):\n",
    "\n",
    "    print(f\"Start of Analysis on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "    all_results_ordered = {}\n",
    "\n",
    "    # Define a range for valid years and weeks\n",
    "    min_valid_year = 1800\n",
    "    max_valid_year = 2100\n",
    "        \n",
    "    categorical_threshold = 100\n",
    "\n",
    "    column_order = analysed_columns_df[analysed_columns_df['index'] == desired_dataset_index].sort_values('ID')['Column'].tolist()\n",
    "\n",
    "    keyword = \"categorical\"  # Keyword to search for in the \"FinalFormat\"\n",
    "\n",
    "    # Find columns marked as \"categorical\" for the specified dataset index\n",
    "    categorical_columns = analysed_columns_df[\n",
    "        (analysed_columns_df['index'] == desired_dataset_index) & \n",
    "        (analysed_columns_df['FinalFormat'].str.contains(keyword, case=False, na=False))\n",
    "    ]['Column'].tolist()\n",
    "\n",
    "    # Iterate through the columns in the custom order and analyze them\n",
    "    for column in column_order:\n",
    "        # Ensure that the column exists in the DataFrame\n",
    "        if column in dataset_df.columns:\n",
    "            if column in categorical_columns:\n",
    "                all_results_ordered[column] = check_if_categorical(dataset_df, column, categorical_threshold)\n",
    "            else:\n",
    "                # Check the format specified in the \"AnalysedColumns\" sheet\n",
    "                format_in_sheet = analysed_columns_df[\n",
    "                    (analysed_columns_df['index'] == desired_dataset_index) &\n",
    "                    (analysed_columns_df['Column'] == column)\n",
    "                ]['FinalFormat'].iloc[0]\n",
    "                \n",
    "                if pd.isna(format_in_sheet):\n",
    "                    all_results_ordered[column] = \"Target word not found, and Format not determined\"\n",
    "                else:\n",
    "                    if \"ID column\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"ID column format\": check_id_attributes(dataset_df, column),\n",
    "                        }\n",
    "                    elif \"numerical >= 0\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"Numerical >=0 format\": check_numerical_ge_zero(dataset_df, column),\n",
    "                        }\n",
    "                    elif \"percentage\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"Percentage format\": check_numerical_between(dataset_df, column, 0, 100),\n",
    "                        }\n",
    "                    elif \"age\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"Age format\": check_numerical_between(dataset_df, column, 0, 130),\n",
    "                        }\n",
    "                    elif \"numerical between 0 and 24\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"Numerical (between 0 and 24) format\": check_numerical_between(dataset_df, column, 0, 24),\n",
    "                        }\n",
    "                    elif \"numerical between 0 and 360\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"Numerical (between 0 and 360) format\": check_numerical_between(dataset_df, column, 0, 360),\n",
    "                        }\n",
    "                    elif \"numerical between 0 and 60\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"Numerical (between 0 and 60) format\": check_numerical_between(dataset_df, column, 0, 60),\n",
    "                        }\n",
    "                    elif \"numerical\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"Numerical format\": check_numerical(dataset_df, column),\n",
    "                        }\n",
    "                    elif \"string\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"String format\": check_string_content(dataset_df, column),\n",
    "                        }\n",
    "                    elif \"datetime\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"Datetime format\": check_datetime(dataset_df, column),\n",
    "                        }\n",
    "                    elif \"date\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"Date format\": check_date(dataset_df, column),\n",
    "                        }\n",
    "                    elif \"time\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"Time format\": check_time(dataset_df, column),\n",
    "                        }\n",
    "                    elif \"month\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"Month format\": check_month(dataset_df, column),\n",
    "                        }\n",
    "                    elif \"year\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"Year format\": check_numerical_between(dataset_df, column, min_valid_year, max_valid_year),\n",
    "                        }\n",
    "                    elif \"weekday\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"Weekday format\": check_weekday(dataset_df, column),\n",
    "                        } \n",
    "                    elif \"week\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"Week format\": check_numerical_between(dataset_df, column, 1, 53),\n",
    "                        }\n",
    "                    elif \"day\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"Day format\" : check_numerical_between(dataset_df, column, 1, 366),\n",
    "                        }\n",
    "                    elif \"model name\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"Model Name format\": check_model_name(dataset_df, column),\n",
    "                        }\n",
    "                    elif \"name\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"Name format\": check_name(dataset_df, column),\n",
    "                        }\n",
    "                    elif \"street\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"Street format\": check_street(dataset_df, column),\n",
    "                        }\n",
    "                    elif \"city\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"City format\": check_city(dataset_df, column),\n",
    "                        }\n",
    "                    elif \"state\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"State format\": check_state(dataset_df, column),\n",
    "                        }\n",
    "                    elif \"country\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"Country format\": check_country(dataset_df, column),\n",
    "                        }\n",
    "                    elif \"postal code\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"Postal Code format\": check_postal_code(dataset_df, column),\n",
    "                        }  \n",
    "                    elif \"phone\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"Phone format\": check_phone_numbers(dataset_df, column),\n",
    "                        }\n",
    "                    elif \"ph\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"pH format\": check_numerical_between(dataset_df, column, 0, 14),\n",
    "                        }    \n",
    "                    elif \"latitude\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"Latitude format\": check_numerical_between(dataset_df, column, -90, 90),\n",
    "                        }\n",
    "                    elif \"longitude\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"Longitude format\": check_numerical_between(dataset_df, column, -180, 180),\n",
    "                        }\n",
    "                    elif \"normalized\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"Normalized format\": check_numerical_between(dataset_df, column, 0, 1),\n",
    "                        }\n",
    "                    elif \"IP format\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"IP Address format\": check_ip_format(df, column),\n",
    "                        }\n",
    "                    elif \"URL format\" in format_in_sheet:\n",
    "                        url_result = check_url_format(df, column)\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"URL format\": url_result\n",
    "                        }\n",
    "                    elif \"E-mail format\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"Email format\": check_email_format(df, column),\n",
    "                        }\n",
    "                    elif \"binary\" in format_in_sheet:\n",
    "                        all_results_ordered[column] = {\n",
    "                            \"Binary format\": check_binary_values(df, column),\n",
    "                        }\n",
    "                    else:\n",
    "                        print(f\"Column '{column}' not found in the dataset.\")\n",
    "        \n",
    "    # Loop to print the results in the desired format\n",
    "    for column, analysis_result in all_results_ordered.items():\n",
    "        # Fetch the 'SourceKeyword' for this column from 'analysed_columns_df'\n",
    "        SourceKeyword_value = analysed_columns_df.loc[(analysed_columns_df['index'] == desired_dataset_index) & (analysed_columns_df['Column'] == column), 'SourceKeyword'].iloc[0]\n",
    "        # Check if 'SourceKeyword' is not NaN before converting to lower case\n",
    "        if pd.notna(SourceKeyword_value) and SourceKeyword_value.lower() != column.lower():\n",
    "            print(f\"{column} ({SourceKeyword_value}):\")\n",
    "        else:\n",
    "            print(f\"{column}:\")\n",
    "            \n",
    "        if isinstance(analysis_result, pd.DataFrame):\n",
    "            print(analysis_result.to_string(index=False))\n",
    "        elif isinstance(analysis_result, dict):\n",
    "            for key, value in analysis_result.items():\n",
    "                print(f\"  {key}: {value}\")\n",
    "        elif isinstance(analysis_result, str):  # Added condition to handle string results\n",
    "            print(f\"  {analysis_result}\")\n",
    "        else:\n",
    "            print(\"No results available.\")\n",
    "        \n",
    "        print()  # Add an empty line for separation\n",
    "\n",
    "analyse_data_quality(dataset_df, analysed_columns_df, desired_dataset_index)\n",
    "\n",
    "from datetime import datetime\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXIT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\290099c\\AppData\\Local\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "raise SystemExit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "30 Load CHANGED dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last run on: 2024-03-12 19:34:33\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>%%%</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>-3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>AA</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>-8</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N</td>\n",
       "      <td>b</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>G</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>a3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>D</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>C</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>T</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>S</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16\n",
       "0        @   2       3   5   1   8  13   0   6   6  10   8   0   8   0   8\n",
       "1      %%%   5  12  -3   7   2  10   5   5   4  13   3   9   2   8   4  10\n",
       "2           AA  11   6  -8   6  10   6   2   6  10   3   7   3   7   3   9\n",
       "3        N   b  11   6   6   3   5   9   4   6   4   4  10   6  10   2   8\n",
       "4        G   2   1  a3   1   1   8   6   6   6   6   5   9   1   7   5  10\n",
       "...    ...  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..\n",
       "19995    D   2   2   3   3   2   7   7   7   6   6   6   4   2   8   3   7\n",
       "19996    C   7  10   8   8   4   4   8   6   9  12   9  13   2   9   3   7\n",
       "19997    T   6   9   6   7   5   6  11   3   7  11   9   5   2  12   2   4\n",
       "19998    S   2   3   4   2   1   8   7   2   6  10   6   8   1   9   5   8\n",
       "19999    A   4   9   6   6   2   9   5   3   1   8   1   8   2   7   2   8\n",
       "\n",
       "[20000 rows x 17 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the CHANGED dataset from changed_dataset_local_path \n",
    "#df = pd.read_csv('your_file.csv', na_values=['', ' ', 'null', 'none', 'None', 'Nan', 'NULL'], keep_default_na=False)\n",
    "\n",
    "#changed_df = pd.read_csv(changed_dataset_local_path, header=None, delimiter=',', na_values=['', ' ', 'null', 'none', 'None', 'Nan', 'NULL'], keep_default_na=False)\n",
    "#changed_df = pd.read_csv(changed_dataset_local_path, header=None, delimiter=',', na_values=[], keep_default_na=False)\n",
    "\n",
    "#changed_df = pd.read_csv(changed_dataset_local_path, delimiter=';', na_values=[], keep_default_na=False) #186, 222\n",
    "#changed_df = pd.read_excel(changed_dataset_local_path, header=header, parse_dates=parse_dates, na_values=[], keep_default_na=False) #602 350\n",
    "#changed_df.head(10)\n",
    "\n",
    "changed_df = pd.read_csv(changed_dataset_local_path, header=None, delimiter=',', na_values=[], keep_default_na=False)\n",
    "changed_df.head(10)\n",
    "\n",
    "from datetime import datetime\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "changed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Successfully assigned column names to the dataset 'Letter Recognition' for index 58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last run on: 2024-03-12 19:34:44\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lettr</th>\n",
       "      <th>x-box</th>\n",
       "      <th>y-box</th>\n",
       "      <th>width</th>\n",
       "      <th>high</th>\n",
       "      <th>onpix</th>\n",
       "      <th>x-bar</th>\n",
       "      <th>y-bar</th>\n",
       "      <th>x2bar</th>\n",
       "      <th>y2bar</th>\n",
       "      <th>xybar</th>\n",
       "      <th>x2ybr</th>\n",
       "      <th>xy2br</th>\n",
       "      <th>x-ege</th>\n",
       "      <th>xegvy</th>\n",
       "      <th>y-ege</th>\n",
       "      <th>yegvx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>%%%</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>-3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>AA</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>-8</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N</td>\n",
       "      <td>b</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>G</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>a3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>S</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>B</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>J</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>M</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lettr x-box y-box width  high  onpix  x-bar  y-bar  x2bar  y2bar  xybar  \\\n",
       "0     @     2           3     5      1      8     13      0      6      6   \n",
       "1   %%%     5    12    -3     7      2     10      5      5      4     13   \n",
       "2          AA    11     6    -8      6     10      6      2      6     10   \n",
       "3     N     b    11     6     6      3      5      9      4      6      4   \n",
       "4     G     2     1    a3     1      1      8      6      6      6      6   \n",
       "5     S     4    11     5     8      3      8      8      6      9      5   \n",
       "6     B     4     2     5     4      4      8      7      6      6      7   \n",
       "7     A     1     1     3     2      1      8      2      2      2      8   \n",
       "8     J     2     2     4     4      2     10      6      2      6     12   \n",
       "9     M    11    15    13     9      7     13      2      6      2     12   \n",
       "\n",
       "   x2ybr  xy2br  x-ege  xegvy  y-ege  yegvx  \n",
       "0     10      8      0      8      0      8  \n",
       "1      3      9      2      8      4     10  \n",
       "2      3      7      3      7      3      9  \n",
       "3      4     10      6     10      2      8  \n",
       "4      5      9      1      7      5     10  \n",
       "5      6      6      0      8      9      7  \n",
       "6      6      6      2      8      7     10  \n",
       "7      2      8      1      6      2      7  \n",
       "8      4      8      1      6      1      7  \n",
       "9      1      9      8      1      1      8  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df = changed_df\n",
    "\n",
    "\n",
    "# Call the function to assign the column names\n",
    "dataset_df = assign_column_names(analysed_columns_df, desired_dataset_index, dataset_df, dataset_name)\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "dataset_df.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "31 Run CHANGED dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of Analysis on: 2024-03-12 19:34:54\n",
      "lettr (letter):\n",
      "  String format: Error(s) found: \n",
      "DQI #1 (Missing Data - Completeness):\n",
      " 1 Blank/Empty/Null/NaN value(s) at index(es): [(2, '')]\n",
      "\n",
      "String range (lexicographical): (%%% : Z)\n",
      "\n",
      "x-box (x):\n",
      "  Numerical format: Error(s) found: \n",
      "DQI #17 (Wrong Data Type - Consistency):\n",
      " 2 Non-numeric value(s) at index(es): [(2, 'AA'), (3, 'b')]\n",
      "\n",
      "Range of values: (0.0:15.0).\n",
      "\n",
      "y-box (y):\n",
      "  Numerical format: Error(s) found: \n",
      "DQI #1 (Missing Data - Completeness):\n",
      " 1 Blank/Empty/Null/NaN value(s) at index(es): [(0, '')]\n",
      "\n",
      "\n",
      "Range of values: (0:15).\n",
      "\n",
      "width:\n",
      "  Numerical >=0 format: Error(s) found: \n",
      "DQI #15 (Domain Violation - Accuracy):\n",
      " 1 Negative value(s) at index(es): [(1, '-3')]\n",
      "\n",
      "DQI #17 (Wrong Data Type - Consistency):\n",
      " 1 Non-numeric value(s) at index(es): [(4, 'a3')]\n",
      "\n",
      "Range of values: (-3.0:15.0).\n",
      "\n",
      "high (height):\n",
      "  Numerical >=0 format: Error(s) found: \n",
      "DQI #15 (Domain Violation - Accuracy):\n",
      " 1 Negative value(s) at index(es): [(2, -8)]\n",
      "\n",
      "Range of values: (-8:15).\n",
      "\n",
      "onpix (integer):\n",
      "  Numerical format: All 20000 values are numerical in the range (0:15).\n",
      "\n",
      "x-bar (x):\n",
      "  Numerical format: All 20000 values are numerical in the range (0:15).\n",
      "\n",
      "y-bar (y):\n",
      "  Numerical format: All 20000 values are numerical in the range (0:15).\n",
      "\n",
      "x2bar (integer):\n",
      "  Numerical format: All 20000 values are numerical in the range (0:15).\n",
      "\n",
      "y2bar (integer):\n",
      "  Numerical format: All 20000 values are numerical in the range (0:15).\n",
      "\n",
      "xybar (integer):\n",
      "  Numerical format: All 20000 values are numerical in the range (0:15).\n",
      "\n",
      "x2ybr (integer):\n",
      "  Numerical format: All 20000 values are numerical in the range (0:15).\n",
      "\n",
      "xy2br (integer):\n",
      "  Numerical format: All 20000 values are numerical in the range (0:15).\n",
      "\n",
      "x-ege (x):\n",
      "  Numerical format: All 20000 values are numerical in the range (0:15).\n",
      "\n",
      "xegvy (integer):\n",
      "  Numerical format: All 20000 values are numerical in the range (0:15).\n",
      "\n",
      "y-ege (y):\n",
      "  Numerical format: All 20000 values are numerical in the range (0:15).\n",
      "\n",
      "yegvx (integer):\n",
      "  Numerical format: All 20000 values are numerical in the range (0:15).\n",
      "\n",
      "Last run on: 2024-03-12 19:34:56\n"
     ]
    }
   ],
   "source": [
    "#print(changed_df, analysed_columns_df, desired_dataset_index)\n",
    "\n",
    "analyse_data_quality(changed_df, analysed_columns_df, desired_dataset_index)\n",
    "\n",
    "from datetime import datetime\n",
    "print(f\"Last run on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
