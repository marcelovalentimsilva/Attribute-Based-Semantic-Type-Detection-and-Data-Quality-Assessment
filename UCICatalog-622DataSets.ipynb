{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 num_papers 29\n",
      "2 num_papers 51\n",
      "3 num_papers 6\n",
      "4 num_papers 4\n",
      "Index: 0\n",
      "Index: 1\n",
      "Index: 2\n",
      "Index: 3\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import csv\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, Tag\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Define the range of datasets to scrape\n",
    "start_index = 1\n",
    "end_index = 623\n",
    "\n",
    "# Helper function to remove <sup> tags from a given BeautifulSoup tag\n",
    "def remove_sup_tags(tag):\n",
    "    for sup in tag.find_all(\"sup\"):\n",
    "        sup.extract()\n",
    "    return tag\n",
    "\n",
    "\n",
    "def has_papers_that_cite_this_data_set(tag):\n",
    "    \"\"\"This function checks if a tag has papers that cite this dataset.\n",
    "\n",
    "    The function is checking for a paragraph tag with class \"normal\" and checks if the previous sibling tag\n",
    "    with class \"small-heading\" contains the text \"Papers That Cite This Data Set\". \n",
    "    \"\"\"\n",
    "    if not (tag.name == \"p\" and tag.get(\"class\") == [\"normal\"]):\n",
    "        return False\n",
    "    \n",
    "    previous_small_heading = tag.find_previous(\"p\", class_=\"small-heading\")\n",
    "    \n",
    "    if not previous_small_heading:\n",
    "        return False\n",
    "    \n",
    "    text = previous_small_heading.get_text(strip=True)\n",
    "    return \"Papers That Cite This Data Set\" in text\n",
    "\n",
    "\n",
    "def get_dataset_details(url):\n",
    "    \"\"\"Fetches and parses a webpage to extract specific dataset details.\n",
    "\n",
    "    The function sends a GET request to the provided URL and creates a BeautifulSoup object \n",
    "    for parsing the HTML content. It then searches for a table with a specific border attribute \n",
    "    and iterates over its rows to find dataset details. If no table is found, the function returns \n",
    "    a tuple of empty strings.\n",
    "\n",
    "    It also looks for specific information in paragraphs ('p' tags) with class \"small-heading\" and \n",
    "    text that matches certain criteria, such as \"Attribute Information:\", \"Source:\", \"Data Set Information:\", \n",
    "    \"Relevant Papers:\", and papers that cite the data set.\n",
    "    \"\"\"\n",
    "    # Send a GET request to the provided URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Decode the content of the response\n",
    "    content = response.content.decode(\"utf-8\", \"replace\")\n",
    "    \n",
    "    # Create a BeautifulSoup object for parsing the HTML content\n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "\n",
    "    # Find a table with a specific border attribute\n",
    "    table = soup.find(\"table\", {\"border\": \"1\"})\n",
    "\n",
    "    # If no table is found, return a tuple of empty strings\n",
    "    if table is None:\n",
    "        return \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"\n",
    "\n",
    "    # Find all rows in the table\n",
    "    rows = table.find_all(\"tr\")\n",
    "\n",
    "    # Initialize variables to hold the information we want to extract\n",
    "    area = \"\"\n",
    "    date_donated = \"\"\n",
    "    web_hits = \"\"\n",
    "    attribute_info = \"\"\n",
    "    source = \"\"\n",
    "    data_set_information = \"\"\n",
    "    relevant_papers = \"\"\n",
    "    papers_that_cite_this_data_set = \"\"\n",
    "    num_papers = 0\n",
    "\n",
    "    # Iterate over the rows of the table to extract the relevant information\n",
    "    for row in rows:\n",
    "        cols = row.find_all(\"td\")\n",
    "\n",
    "        # If the row has six columns, check if it contains information about the area, date donated or web hits\n",
    "        if len(cols) == 6:\n",
    "            if \"Area:\" in cols[4].get_text(strip=True):\n",
    "                area = cols[5].get_text(strip=True)\n",
    "            if \"Date Donated\" in cols[4].get_text(strip=True):\n",
    "                date_donated = cols[5].get_text(strip=True)   \n",
    "            if \"Number of Web Hits:\" in cols[4].get_text(strip=True):\n",
    "                web_hits = cols[5].get_text(strip=True)\n",
    "\n",
    "    # Find and extract the attribute information\n",
    "    attribute_info_tag = soup.find(\"p\", class_=\"small-heading\", text=\"Attribute Information:\")\n",
    "\n",
    "    if attribute_info_tag:\n",
    "        # Store each line of attribute information in a list\n",
    "        attribute_info_lines = []\n",
    "        sibling = attribute_info_tag.find_next_sibling()\n",
    "        \n",
    "        # Loop through the siblings (following 'p' tags) of the attribute_info_tag\n",
    "        while sibling and sibling.name == \"p\" and \"class\" in sibling.attrs and \"normal\" in sibling[\"class\"]:\n",
    "            lines = sibling.decode_contents().split(\"<br/>\")\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    " #               if not line.startswith(\"Given\") and not line.startswith(\"\") and not line.startswith(\"-\") and not line.startswith(\" \"):\n",
    "                attribute_info_lines.append(line)\n",
    "            sibling = sibling.find_next_sibling()\n",
    "\n",
    "        # Join the attribute information lines into a single string\n",
    "        attribute_info = \"\\n\".join(attribute_info_lines)\n",
    "    \n",
    "    # Similar procedures are followed for 'source', 'data_set_information', and 'relevant_papers'\n",
    "    # Each involves finding a specific tag, then traversing its siblings to extract information\n",
    "\n",
    "    # Find and extract the source information\n",
    "    source_tag = soup.find(\"p\", class_=\"small-heading\", text=\"Source:\")\n",
    "    if source_tag:\n",
    "        source_lines = []\n",
    "        sibling = source_tag.find_next_sibling()\n",
    "        while sibling and sibling.name == \"p\" and \"class\" in sibling.attrs and \"normal\" in sibling[\"class\"]:\n",
    "            lines = sibling.decode_contents().split(\"<br/>\")\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                source_lines.append(line)\n",
    "            sibling = sibling.find_next_sibling()\n",
    "\n",
    "        source = \"\\n\".join(source_lines)\n",
    "\n",
    "    # Find and extract the data set information\n",
    "    data_set_information_tag = soup.find(\"p\", class_=\"small-heading\", text=\"Data Set Information:\")\n",
    "    if data_set_information_tag:\n",
    "        data_set_information_lines = []\n",
    "        sibling = data_set_information_tag.find_next_sibling()\n",
    "        while sibling and sibling.name == \"p\" and \"class\" in sibling.attrs and \"normal\" in sibling[\"class\"]:\n",
    "            lines = sibling.decode_contents().split(\"<br/>\")\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                data_set_information_lines.append(line)\n",
    "            sibling = sibling.find_next_sibling()\n",
    "\n",
    "        data_set_information = \"\\n\".join(data_set_information_lines)\n",
    "\n",
    "    # Find and extract the relevant papers\n",
    "    relevant_papers_tag = soup.find(\"p\", class_=\"small-heading\", text=\"Relevant Papers:\")\n",
    "    if relevant_papers_tag:\n",
    "        relevant_papers_lines = []\n",
    "        sibling = relevant_papers_tag.find_next_sibling()\n",
    "        while sibling and sibling.name == \"p\" and \"class\" in sibling.attrs and \"normal\" in sibling[\"class\"]:\n",
    "            lines = sibling.decode_contents().split(\"<br/>\")\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                relevant_papers_lines.append(line)\n",
    "            sibling = sibling.find_next_sibling()\n",
    "\n",
    "        relevant_papers = \"\\n\".join(relevant_papers_lines) \n",
    "   \n",
    "    # Find and extract the papers that cite this data set\n",
    "    papers_that_cite_this_data_set_tag = soup.find(has_papers_that_cite_this_data_set)\n",
    "\n",
    "    if papers_that_cite_this_data_set_tag:\n",
    "        # Get the next sibling of the papers_that_cite_this_data_set_tag\n",
    "        sibling = papers_that_cite_this_data_set_tag\n",
    "\n",
    "       # Store each paper in a list\n",
    "        papers = []\n",
    "\n",
    "        # Initialize a counter for the number of papers\n",
    "        num_papers = 0\n",
    "\n",
    "        # Loop through the siblings (following 'p' tags) of the papers_that_cite_this_data_set_tag\n",
    "        while sibling and sibling.name == \"p\":\n",
    "            # Check if the sibling has the class \"normal\"\n",
    "            if \"class\" in sibling.attrs and \"normal\" in sibling[\"class\"]:\n",
    "                # Replace <br> tags with a placeholder\n",
    "                for br in sibling.find_all(\"br\"):\n",
    "                    br.replace_with(\"|||\")\n",
    "                \n",
    "                # Split the text of the sibling by the placeholder to get a list of papers\n",
    "                paper_list = sibling.text.split(\"|||\")\n",
    "\n",
    "                # Handle the case where papers are separated by <br><br> instead of just <br>\n",
    "                if len(paper_list) == 1 and \"<br>\" in sibling.decode_contents():\n",
    "                    paper_list = sibling.decode_contents().split(\"<br><br>\")\n",
    "\n",
    "                # Filter out empty strings from paper_list\n",
    "                paper_list = [paper.strip() for paper in paper_list if paper.strip()]\n",
    "\n",
    "                # Add each paper to the list of papers and increment the counter\n",
    "                for paper in paper_list:\n",
    "                    papers.append(paper)\n",
    "                    num_papers += 1\n",
    "  \n",
    "            # Get the next sibling of the current sibling\n",
    "            sibling = sibling.find_next_sibling(\"p\")\n",
    "\n",
    "        # Join the list of papers into a single string separated by semicolons\n",
    "        papers_that_cite_this_data_set = \"; \".join(papers)\n",
    "        \n",
    "    else:\n",
    "        # If no papers_that_cite_this_data_set_tag is found, set the papers_that_cite_this_data_set and num_papers to their default values\n",
    "        papers_that_cite_this_data_set = \"\"\n",
    "        num_papers = 0\n",
    "    \n",
    "    # Return the extracted information as a tuple\n",
    "    return area, date_donated, web_hits, attribute_info, source, data_set_information, relevant_papers, papers_that_cite_this_data_set, num_papers\n",
    "   \n",
    "def fetch_data_folder_url(url):\n",
    "    # Send a GET request to the provided URL.\n",
    "    response = requests.get(url)\n",
    "    # Decode the content of the response to utf-8, replacing any characters that can't be decoded.\n",
    "    content = response.content.decode(\"utf-8\", \"replace\")\n",
    "    # Parse the HTML content of the response with BeautifulSoup.\n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "\n",
    "    # Search for an 'a' tag whose 'href' attribute contains the string \"machine-learning-databases\".\n",
    "    # This is often where the link to the data folder is stored on many machine learning database websites.\n",
    "    data_folder_tag = soup.find(\"a\", href=lambda x: x and \"machine-learning-databases\" in x)\n",
    "\n",
    "    # If the data_folder_tag is found, combine the base URL with the href of the data_folder_tag\n",
    "    # to create the full URL of the data folder.\n",
    "    # The urljoin function is used to ensure that the URLs are combined correctly, even if the href is a relative URL.\n",
    "    if data_folder_tag is not None:\n",
    "        return urljoin(url, data_folder_tag[\"href\"])\n",
    "\n",
    "    # If no data_folder_tag is found, return None.\n",
    "    return None\n",
    "\n",
    "def fetch_dataset_file(url):\n",
    "    \"\"\"This function checks each hyperlink in a web page to find a dataset file. \n",
    "    It specifically looks for links ending with \".data\" or \".txt\", or others, which are common data file formats. \n",
    "    If it finds a data file, it returns the complete URL to this file and the file format. \n",
    "    If no data file is found, it returns an empty string and an empty file format.\"\"\"\n",
    "\n",
    "    # Send a GET request to the provided URL.\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Decode the content of the response to utf-8, replacing any characters that can't be decoded.\n",
    "    content = response.content.decode(\"utf-8\", \"replace\")\n",
    "\n",
    "    # Parse the HTML content of the response with BeautifulSoup.\n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "\n",
    "    # Find all the 'a' tags on the page, which commonly represent hyperlinks.\n",
    "    links = soup.find_all(\"a\")\n",
    "\n",
    "    # Initialize an empty string to hold the file format of the data file.\n",
    "    file_format = \"\"\n",
    "\n",
    "    # Iterate over all the links found on the page.\n",
    "    for link in links:\n",
    "        # If the text of the link ends with '.data', this is likely a link to a data file.\n",
    "        if \".data\" in link.text:\n",
    "            # Set the file format to 'data'.\n",
    "            file_format = \"data\"\n",
    "            # Return a tuple containing the full URL of the data file (created by joining the base URL with the href of the link)\n",
    "            # and the file format.\n",
    "            return urljoin(url, link[\"href\"]), file_format\n",
    "        elif \".txt\" in link.text:\n",
    "            file_format = \"txt\"\n",
    "            return urljoin(url, link[\"href\"]), file_format\n",
    "        elif \".csv\" in link.text:\n",
    "            file_format = \"csv\"\n",
    "            return urljoin(url, link[\"href\"]), file_format\n",
    "        elif \".xls\" in link.text:\n",
    "            file_format = \"xls\"\n",
    "            return urljoin(url, link[\"href\"]), file_format\n",
    "        elif \".xlsx\" in link.text:\n",
    "            file_format = \"xlsx\"\n",
    "            return urljoin(url, link[\"href\"]), file_format\n",
    "        elif \".rar\" in link.text:\n",
    "            file_format = \"rar\"\n",
    "            return urljoin(url, link[\"href\"]), file_format \n",
    "        elif \".arff\" in link.text:\n",
    "            file_format = \"arff\"\n",
    "            return urljoin(url, link[\"href\"]), file_format\n",
    "        elif \".mat\" in link.text:\n",
    "            file_format = \"mat\"\n",
    "            return urljoin(url, link[\"href\"]), file_format          \n",
    "        elif \".Z\" in link.text:\n",
    "            file_format = \"Z\"\n",
    "            return urljoin(url, link[\"href\"]), file_format\n",
    "        elif \".zip\" in link.text:\n",
    "            file_format = \"zip\"\n",
    "            return urljoin(url, link[\"href\"]), file_format\n",
    "        elif \".Tar\" in link.text:\n",
    "            file_format = \"Tar\"\n",
    "            return urljoin(url, link[\"href\"]), file_format\n",
    "        elif \".gz\" in link.text:\n",
    "            file_format = \"gz\"\n",
    "            return urljoin(url, link[\"href\"]), file_format\n",
    "        elif \".dat\" in link.text:\n",
    "            file_format = \"dat\"\n",
    "            return urljoin(url, link[\"href\"]), file_format\n",
    "        elif \".tar\" in link.text:\n",
    "            file_format = \"tar\"\n",
    "            return urljoin(url, link[\"href\"]), file_format \n",
    "        elif \".7z\" in link.text:\n",
    "            file_format = \"7z\"\n",
    "            return urljoin(url, link[\"href\"]), file_format\n",
    "        elif \".json\" in link.text:\n",
    "            file_format = \"json\"\n",
    "            return urljoin(url, link[\"href\"]), file_format  \n",
    "        elif \".tsv\" in link.text:\n",
    "            file_format = \"tsv\"\n",
    "            return urljoin(url, link[\"href\"]), file_format\n",
    "\n",
    "    # If no data file is found among the links, return a tuple containing an empty string and the file format (which will also be an empty string).\n",
    "    return \"\", file_format\n",
    "\n",
    "def fetch_names_file(url):\n",
    "    \"\"\"\n",
    "    This function checks each hyperlink in a web page to find a dataset names or info file. \n",
    "    It specifically looks for links ending with \".names\" or \".info\" or others. \n",
    "    If it finds a names or info file, it returns the complete URL to this file and the file format. \n",
    "    If no names or info file is found, it returns an empty string and an empty file format.\"\"\"\n",
    "    # Send a GET request to the provided URL.\n",
    "    response = requests.get(url)\n",
    "    # Decode the content of the response to utf-8, replacing any characters that can't be decoded.\n",
    "    content = response.content.decode(\"utf-8\", \"replace\")\n",
    "    # Parse the HTML content of the response with BeautifulSoup.\n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "\n",
    "    # Find all the 'a' tags on the page, which commonly represent hyperlinks.\n",
    "    links = soup.find_all(\"a\")\n",
    "    # Initialize an empty string to hold the file format of the data file.\n",
    "    file_format = \"\"\n",
    "\n",
    "    # Iterate over all the links found on the page.\n",
    "    for link in links:\n",
    "        # If the text of the link ends with '.names', this is likely a link to a names file.\n",
    "        if \".names\" in link.text:\n",
    "            # Set the file format to 'names'.\n",
    "            file_format = \"names\"\n",
    "            # Return a tuple containing the full URL of the names file (created by joining the base URL with the href of the link)\n",
    "            # and the file format.\n",
    "            return urljoin(url, link[\"href\"]), file_format\n",
    "        # If the text of the link ends with '.info', this is likely a link to an info file.\n",
    "        elif \".info\" in link.text:\n",
    "            # Set the file format to 'info'.\n",
    "            file_format = \"info\"\n",
    "            # Return a tuple containing the full URL of the info file (created by joining the base URL with the href of the link)\n",
    "            # and the file format.\n",
    "            return urljoin(url, link[\"href\"]), file_format\n",
    "        elif \".doc\" in link.text:\n",
    "            file_format = \"doc\"\n",
    "            return urljoin(url, link[\"href\"]), file_format\n",
    "        elif \".docx\" in link.text:\n",
    "            file_format = \"docx\"\n",
    "            return urljoin(url, link[\"href\"]), file_format\n",
    "    \n",
    "    # If no names or info file is found among the links, return a tuple containing an empty string and the file format (which will also be an empty string).\n",
    "    return \"\", file_format\n",
    "\n",
    "def get_uci_datasets(start_index, end_index):\n",
    "    \"\"\"\n",
    "    This function fetches dataset details from the UCI Machine Learning Repository.\n",
    "\n",
    "    It starts by sending a GET request to the UCI datasets page and parsing the response content. \n",
    "    Then it finds the main table that contains information about the datasets.\n",
    "\n",
    "    It loops over each row in the table, which corresponds to a dataset, and skips the datasets \n",
    "    which are not within the range [start_index, end_index].\n",
    "\n",
    "    For each dataset within the range, it extracts the details like name, URL, instances, attributes, \n",
    "    and year from the table columns. It also fetches additional details by calling the function \n",
    "    `get_dataset_details` with the dataset URL.\n",
    "\n",
    "    All the collected details are then stored in a dictionary and appended to a list. The function \n",
    "    finally returns this list of dictionaries, each containing details about a dataset.\n",
    "\n",
    "    If the main table is not found on the page, the function prints an error message and returns an \n",
    "    empty list.\n",
    "    \"\"\"\n",
    "    # Define the URL for the UCI Machine Learning Repository\n",
    "    url = \"https://archive.ics.uci.edu/ml/datasets.php\"\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Decode the content of the response from bytes to UTF-8 string\n",
    "    content = response.content.decode(\"utf-8\", \"replace\")\n",
    "\n",
    "    # Use BeautifulSoup to parse the HTML content of the response\n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "\n",
    "    try:\n",
    "        # Try to find the table in the HTML that has a border attribute set to \"1\"\n",
    "        table = soup.find(\"table\", {\"border\": \"1\"})\n",
    "\n",
    "        # Find all row elements (tr) in the table\n",
    "        rows = table.find_all(\"tr\")\n",
    "    except AttributeError:\n",
    "        # If the table is not found, print an error message and return an empty list\n",
    "        print(\"Table not found\")\n",
    "        return []\n",
    "\n",
    "    # Initialize an empty list to store the datasets\n",
    "    dataset_list = []\n",
    "    \n",
    "    # Initialize a variable to keep track of how many datasets have been processed   \n",
    "    processed_datasets = 1\n",
    "\n",
    "    # Iterate over the rows in the table, skipping the first row (header row)\n",
    "    for row in rows[1:]:\n",
    "        # If the number of processed datasets is equal to or exceeds the end_index, stop processing\n",
    "        if processed_datasets >= end_index:\n",
    "            break\n",
    "\n",
    "        # Find all column elements (td) in the row\n",
    "        cols = row.find_all(\"td\")\n",
    "\n",
    "        # If the row has less than 9 columns, skip this row and continue with the next one\n",
    "        if len(cols) < 9:\n",
    "            continue\n",
    "\n",
    "        # If the number of processed datasets is less than the start_index, increment the counter and continue with the next row\n",
    "        if processed_datasets < start_index:\n",
    "            processed_datasets += 1\n",
    "            continue\n",
    "\n",
    "        # Get the dataset name, URL, number of instances, number of attributes, and year from the columns\n",
    "        dataset_name = cols[0].get_text(strip=True)\n",
    "        dataset_url = urljoin(url, cols[0].find(\"a\")[\"href\"])\n",
    "        instances = cols[6].get_text(strip=True)\n",
    "        attributes = cols[7].get_text(strip=True)\n",
    "        year = cols[8].get_text(strip=True)\n",
    "\n",
    "        # Use the get_dataset_details function to get detailed information about the dataset\n",
    "        area, date_donated, web_hits, attribute_info,source, data_set_information, relevant_papers,papers_that_cite_this_data_set, num_papers = get_dataset_details(dataset_url)\n",
    "\n",
    "        # Print the number of processed datasets and the number of papers that cite this dataset\n",
    "        print(processed_datasets, \"num_papers\", num_papers)\n",
    "        \n",
    "        # Append a dictionary with all the dataset information to the dataset_list\n",
    "        dataset_list.append({\n",
    "            \"index\": processed_datasets,\n",
    "            \"name\": dataset_name,\n",
    "            \"url\": dataset_url,\n",
    "            \"instances\": instances,\n",
    "            \"attributes\": attributes,\n",
    "            \"year\": year,\n",
    "            \"area\": area,\n",
    "            \"date_donated\" : date_donated,\n",
    "            \"web_hits\": web_hits,\n",
    "            \"attribute_info\": attribute_info,\n",
    "            \"source\": source,\n",
    "            \"data_set_information\": data_set_information,\n",
    "            \"relevant_papers\": relevant_papers,\n",
    "            \"papers_that_cite_this_data_set\": papers_that_cite_this_data_set,\n",
    "            \"num_papers\" : num_papers,\n",
    "            \"data_folder_url\": \"\",\n",
    "            \"dataset_file_url\": \"\",\n",
    "            \"dataset_file_format\": \"\",\n",
    "            \"names_file_url\": \"\",\n",
    "            \"names_file_format\": \"\",\n",
    "        })\n",
    "\n",
    "        # Increment the count of processed datasets\n",
    "        processed_datasets += 1\n",
    "\n",
    "    # After processing all rows, return the list of datasets\n",
    "    return dataset_list\n",
    "\n",
    "datasets = get_uci_datasets(start_index, end_index)\n",
    "\n",
    "# Save results to CSV file\n",
    "with open(\"uci_datasets.csv\", \"w\", newline=\"\") as csvfile:\n",
    "    fieldnames = [\"index\", \"name\", \"url\", \"instances\", \"attributes\", \"year\", \"area\", \"date_donated\", \"web_hits\",\"data_folder_url\", \"dataset_file_url\", \"dataset_file_format\", \"names_file_url\", \"names_file_format\", \"attribute_info\", \"source\",\"data_set_information\",\"relevant_papers\",\"papers_that_cite_this_data_set\",\"num_papers\"]\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        print(f\"Index: {i}\")\n",
    "\n",
    "        data_folder_url = fetch_data_folder_url(dataset[\"url\"])\n",
    "        dataset[\"data_folder_url\"] = data_folder_url\n",
    "\n",
    "        if data_folder_url is not None:\n",
    "            dataset_file_url, dataset_file_format = fetch_dataset_file(data_folder_url)\n",
    "            dataset[\"dataset_file_url\"] = dataset_file_url\n",
    "            dataset[\"dataset_file_format\"] = dataset_file_format\n",
    "\n",
    "            names_file_url, names_file_format = fetch_names_file(data_folder_url)\n",
    "            dataset[\"names_file_url\"] = names_file_url\n",
    "            dataset[\"names_file_format\"] = names_file_format\n",
    "        else:\n",
    "            dataset[\"dataset_file_url\"] = \"\"\n",
    "            dataset[\"names_file_url\"] = \"\"\n",
    "\n",
    "        writer.writerow({\n",
    "            \"index\": i+1,\n",
    "            \"name\": dataset[\"name\"],\n",
    "            \"url\": dataset[\"url\"],\n",
    "            \"instances\": dataset[\"instances\"],\n",
    "            \"attributes\": dataset[\"attributes\"],\n",
    "            \"year\": dataset[\"year\"],\n",
    "            \"area\": dataset[\"area\"],\n",
    "            \"date_donated\": dataset[\"date_donated\"],\n",
    "            \"web_hits\": dataset[\"web_hits\"],\n",
    "            \"data_folder_url\": dataset[\"data_folder_url\"],\n",
    "            \"dataset_file_url\": dataset[\"dataset_file_url\"],\n",
    "            \"dataset_file_format\": dataset[\"dataset_file_format\"],\n",
    "            \"names_file_url\": dataset[\"names_file_url\"],\n",
    "            \"names_file_format\": dataset[\"names_file_format\"],\n",
    "            \"attribute_info\": dataset[\"attribute_info\"],\n",
    "            \"source\": dataset[\"source\"],\n",
    "            \"data_set_information\": dataset[\"data_set_information\"],\n",
    "            \"relevant_papers\": dataset[\"relevant_papers\"],\n",
    "            \"papers_that_cite_this_data_set\": dataset[\"papers_that_cite_this_data_set\"],\n",
    "            \"num_papers\": dataset[\"num_papers\"]\n",
    "                        \n",
    "        })\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0c3df0fee6849ff3cb4f535ebfc704e5baac454a8021cec4e5c8cd4e80394129"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
